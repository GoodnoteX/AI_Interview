
> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本文介绍深度学习常见的概念——权重初始化、评估指标、梯度消失和梯度爆炸。
> 

![在这里插入图片描述](https://github.com/GoodnoteX/Ai_Interview/blob/main/深度学习笔记/image/3.png)

@[toc]
# 权重初始化

权重初始化是影响模型**训练速度和性能**的重要因素。合理的权重初始化可以帮助**加快收敛，减少不稳定性**。以下是几种常见的权重初始化方法及其解释：
### 零初始化 (Zero Initialization)
- **描述**：将**所有权重初始化为零**。
- **优点**：简单易实现。
- **缺点**：所有神经元的输出相同，导致**每个神经元在反向传播时更新相同的权重，无法有效学习**。此方法通常**不适用于深度学习**。
### 随机初始化 (Random Initialization)

- 均匀分布随机初始化：将权重初始化为一个在指定范围内的均匀分布随机值，常用的范围是[-1, 1]。
$$ W \sim U(-a, a) $$
- 正态分布随机初始化：将权重初始化为服从正态分布的小值。通常均值为0，标准差较小。
$$ W \sim N(0, \sigma^{2}) $$

- **描述**：将**权重随机初始化为小的随机值**（通常在**一个小范围内**）。
- **优点**：避免了神经元输出相同的问题，允许不同神经元学习不同的特征。
- **缺点**：选择不当的范围可能导致**梯度消失或梯度爆炸**问题。

### Xavier 初始化（Glorot 初始化）
- **描述**：适用于**Sigmoid或Tanh**激活函数。权重**根据输入和输出的数量**进行初始化，通常使用正态分布或均匀分布。
  - 计算公式：
$$\text{Var}(W) = \frac{2}{n_{\text{in}} + n_{\text{out}}}$$


- **优点**：**通过考虑输入和输出的规模，减少了层与层之间的激活值的方差**，有助于更稳定的训练。

> 
> **描述** Xavier初始化是由Glorot和Bengio在2010年提出的一种权重初始化方法，旨在解决深度神经网络中激活值方差的问题。它主要用于具有Sigmoid或Tanh激活函数的神经网络。
> 
> **计算公式**
> - 权重初始化为均匀分布或正态分布：
>   - 对于均匀分布： $$W \sim U\left(-\frac{\sqrt{6}}{\sqrt{n_{\text{in}}+n_{\text{out}}}},
> \frac{\sqrt{6}}{\sqrt{n_{\text{in}}+n_{\text{out}}}}\right)$$
>   - 对于正态分布： $$W \sim N\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)$$
> 其中，
$n_{\text{in}}$ 是输入层的神经元数量，
$n_{\text{out}}$ 是输出层的神经元数量。
> 
> **优点**
> - **保持激活方差**：Xavier初始化通过考虑输入和输出神经元的数量，能够保持激活值在网络每一层的方差相对稳定。这样，在前向传播和反向传播过程中，信息可以更有效地传播。
> - **减少梯度消失**：在深度网络中，随着层数增加，梯度消失的问题可能会加剧。Xavier初始化通过合理的权重分布，降低了这种情况的发生概率。
> 
> **适用情况**
> - 适用于使用Sigmoid或Tanh激活函数的网络。对于ReLU激活函数，由于其特性（输出为0的概率较高），Xavier初始化可能会导致“死亡神经元”现象，因此通常不推荐。

### He 初始化
- **描述**：专门为**ReLU**激活函数设计的初始化方法，权重**根据输入的数量**进行初始化。
  - 计算公式：
$$\text{Var}(W)=\frac{2}{n_{\text{in}}}$$
- **优点**：**减少**了在ReLU激活函数中由于零输入造成的 “**死亡神经元**”问题 ，适用于深度神经网络。




> 详细全文请移步公主号：Goodnote。  
参考：[欢迎来到好评笔记（Goodnote）！](https://mp.weixin.qq.com/s/lCcceUHTrM7wOjnxkfrFsQ)


