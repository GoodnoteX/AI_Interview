
> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本文介绍深度学习常见的概念——权重初始化、评估指标、梯度消失和梯度爆炸。
> 

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/d8c15357e25e404a9598e67ab601a928.png#pic_center)

@[toc]
# 权重初始化

权重初始化是影响模型**训练速度和性能**的重要因素。合理的权重初始化可以帮助**加快收敛，减少不稳定性**。以下是几种常见的权重初始化方法及其解释：
### 零初始化 (Zero Initialization)
- **描述**：将**所有权重初始化为零**。
- **优点**：简单易实现。
- **缺点**：所有神经元的输出相同，导致**每个神经元在反向传播时更新相同的权重，无法有效学习**。此方法通常**不适用于深度学习**。
### 随机初始化 (Random Initialization)

- 均匀分布随机初始化：将权重初始化为一个在指定范围内的均匀分布随机值，常用的范围是[-1, 1]。
$$ W \sim U(-a, a) $$
- 正态分布随机初始化：将权重初始化为服从正态分布的小值。通常均值为0，标准差较小。
$$ W \sim N(0, \sigma^{2}) $$

- **描述**：将**权重随机初始化为小的随机值**（通常在**一个小范围内**）。
- **优点**：避免了神经元输出相同的问题，允许不同神经元学习不同的特征。
- **缺点**：选择不当的范围可能导致**梯度消失或梯度爆炸**问题。

### Xavier 初始化（Glorot 初始化）
- **描述**：适用于**Sigmoid或Tanh**激活函数。权重**根据输入和输出的数量**进行初始化，通常使用正态分布或均匀分布。
  - 计算公式：
$$ \text{Var}(W) = \frac{2}{n_{\text{in}} + n_{\text{out}}} $$


- **优点**：**通过考虑输入和输出的规模，减少了层与层之间的激活值的方差**，有助于更稳定的训练。

> 
> **描述** Xavier初始化是由Glorot和Bengio在2010年提出的一种权重初始化方法，旨在解决深度神经网络中激活值方差的问题。它主要用于具有Sigmoid或Tanh激活函数的神经网络。
> 
> **计算公式**
> - 权重初始化为均匀分布或正态分布：
>   - 对于均匀分布： $$ W \sim U\left(-\frac{\sqrt{6}}{\sqrt{n_{\text{in}}+n_{\text{out}}}},
> \frac{\sqrt{6}}{\sqrt{n_{\text{in}}+n_{\text{out}}}}\right) $$
>   - 对于正态分布： $$ W \sim N\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right) $$
> 其中，$n_{\text{in}}$是输入层的神经元数量，$n_{\text{out}}$是输出层的神经元数量。
> 
> **优点**
> - **保持激活方差**：Xavier初始化通过考虑输入和输出神经元的数量，能够保持激活值在网络每一层的方差相对稳定。这样，在前向传播和反向传播过程中，信息可以更有效地传播。
> - **减少梯度消失**：在深度网络中，随着层数增加，梯度消失的问题可能会加剧。Xavier初始化通过合理的权重分布，降低了这种情况的发生概率。
> 
> **适用情况**
> - 适用于使用Sigmoid或Tanh激活函数的网络。对于ReLU激活函数，由于其特性（输出为0的概率较高），Xavier初始化可能会导致“死亡神经元”现象，因此通常不推荐。

### He 初始化
- **描述**：专门为**ReLU**激活函数设计的初始化方法，权重**根据输入的数量**进行初始化。
  - 计算公式：
$$ \text{Var}(W)=\frac{2}{n_{\text{in}}} $$
- **优点**：**减少**了在ReLU激活函数中由于零输入造成的 “**死亡神经元**”问题 ，适用于深度神经网络。

> 
> 
> **描述** He初始化是由Kaiming He等人在2015年提出的，专为ReLU（Rectified Linear Unit）激活函数设计的一种权重初始化方法。该方法旨在减少深度神经网络训练中的梯度消失和梯度爆炸问题。
> 
> **计算公式**
> - 权重初始化为正态分布： $$ W \sim N\left(0, \frac{2}{n_{\text{in}}} \right) $$ 其中，$n_{\text{in}}$是输入层的神经元数量。
> 
> **优点**
> - 适应ReLU特性：由于ReLU激活函数的特性（即一半的输入值为0），He初始化通过将标准差设为$\sqrt{\frac{2}{n_{\text{in}}}}$，有效地避免了大部分神经元在初始阶段的输出为0的问题，从而减少了“死亡神经元”现象的发生。
> - 保持方差：在前向传播中，He初始化通过确保激活值的方差不会过快下降，帮助模型在训练初期保持较好的学习能力，促进更快的收敛。
> 
> **适用情况**
> - 专门为使用ReLU及其变种（如Leaky ReLU、Parametric ReLU等）激活函数的深度学习模型设计，能有效提高这些模型的性能和训练稳定性。


### 正交初始化（Orthogonal Initialization）

**描述**：
生成的权重矩阵 W 是**正交矩阵**，即满足$W^T W = I$（单位矩阵）。正交矩阵具有特性：它的**特征值的模为 1**，这意味着在**正交矩阵上的操作不会导致梯度的膨胀或收缩**，因此适合深层网络或循环神经网络（RNN），有助于稳定梯度传播。

**初始化过程**：
- 首先生成一个随机矩阵，然后对该矩阵进行正交计算。
- 对于维数不相等的矩阵，进行截断，使矩阵能够符合输入和输出的维度。
正交计算获得正交矩阵方法：
1. **奇异值分解（SVD）**，将其分解为 3 个矩阵，其中包含一个正交矩阵。用该正交矩阵初始化权重。

> 
> 
> 在奇异值分解（SVD，Singular Value Decomposition）中，一个矩阵$A$可以分解为3个矩阵： $$ A =
> U\Sigma V^{T} $$ 其中：
> - $U$是一个**左奇异矩阵**，其列向量是$A$的左奇异向量。
> - $\Sigma$是一个对角矩阵，包含了$A$的奇异值。
> - $V^{T}$是一个**右奇异矩阵**的转置，$V$的列向量是$A$的右奇异向量。
> - U 和 V 都是正交矩阵，**通常**选择 U 矩阵作为正交矩阵来初始化权重。

2. **QR 分解**：对矩阵 W  进行 QR 分解，将其分解为一个正交矩阵  Q 和上三角矩阵 R。
> $$ W = Q \cdot R $$ 
> 其中：
> - $Q$是所需的正交矩阵。

**优点**：
- **梯度稳定性**：它的特征值的模为 1，权重矩阵的特征值稳定，防止了梯度消失或梯度爆炸的情况，确保在前向传播和反向传播中信息能够更平稳地传递。，特别适用于深层网络和 RNN 等时间序列模型。

**缺点**：
- **计算复杂度较高**：生成正交矩阵需要使用奇异值分解（SVD）【PCA用到】，这在高维度情况下计算开销较大，尤其在大规模神经网络中会导致初始化阶段的计算较慢。
- **适用范围有限**：主要用于循环神经网络（RNN）等需要保持梯度稳定的结构，对于其他模型（如卷积神经网络 CNN），正交初始化的优势不明显。

**适用场景**：
- 主要用于 **循环神经网络（RNN）**、**长短时记忆网络（LSTM）** 和 **门控循环单元（GRU）**，这些网络需要在时间维度上进行长时间的梯度传播，因此正交初始化能够帮助缓解梯度消失或梯度爆炸的问题。
- 也可用于深层网络模型，如 **深度前馈神经网络（DNN）**，特别是在训练过程不稳定的情况下，正交初始化可以帮助保持信号传播稳定。

### 预训练模型初始化
- **描述**：使用在大规模数据集上预训练的模型的权重进行初始化，然后再进行微调。
- **优点**：可以显著**提高**模型在**小数据集上的表现**，**加快收敛速度**。

## 权重初始化方法总结

| **初始化方法**          | **描述**                                                                                              | **优点**                                                                                       | **缺点**                                                                                               | **适用场景**                                      |
|-------------------------|-------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|---------------------------------------------------|
| **零初始化**            | 将所有**权重初始化为 0**。                                                                                 | 简单易实现。                                                                                    | 导致所有神经元输出相同，无法有效学习特征，反向传播时无法更新不同的权重。                                 | 不适用于深度学习模型                              |
| **随机初始化**          | 将**权重随机初始化为小的随机值**。                                                                         | 避免神经元输出相同，允许不同神经元学习不同的特征。                                              | 不适当的范围可能导致**梯度消失**或**梯度爆炸**问题。                                                        | 适用于大多数模型的简单初始化                      |
| **Xavier 初始化**       | 基于**输入输出规模**初始化权重，适合 **Sigmoid/Tanh** 激活函数。                                               | 平衡输入输出，**减少激活值的方差变化**，促进稳定训练。                                               | 对 ReLU 等非线性激活函数效果不理想。                                                                 | 适用于 Sigmoid 或 Tanh 激活函数的网络              |
| **He 初始化**           | 为 **ReLU** 设计，考虑激活函数的非线性特性，权重按**输入规模**初始化。                                         | **减少 ReLU 激活中“死亡神经元”问题**，适合深层网络。                                                 | 主要针对 ReLU，其他激活函数可能效果不佳。                                                             | 适用于 ReLU 或 Leaky ReLU 激活函数的网络           |
| **正交初始化**          | 保证**权重矩阵正交**，特征值稳定，常用于循环神经网络（RNN）。                                             | 保持梯度稳定，**防止梯度消失或爆炸**，适合处理长序列。                                               | 需要奇异值分解（**SVD**），计算开销较大。                                                                | 适用于 RNN、LSTM、GRU 等长序列处理的模型           |
| **预训练模型初始化**    | 使用**预训练模型的权重**进行初始化，适用于**迁移学习**。                                                      | 在小数据集上表现优秀，快速收敛，加速训练。                                                       | 需要预训练权重，并且可能不适用于所有数据集或任务。                                                    | 适用于迁移学习和微调任务，如图像分类、NLP 等       |

## 总结
- **零初始化**：简单但**不能用于深度学习**。
- **随机初始化**：常见的简单方法，但**需合理选择范围**以防梯度问题。
- **Xavier 初始化**：适合 **Sigmoid 和 Tanh** 激活函数，能平衡输入输出的方差。
- **He 初始化**：适合 **ReLU** 激活函数，解决 ReLU 的“死亡神经元”问题。
- **正交初始化**：对**循环神经网络（RNN）**有效，确保梯度稳定，适合长序列处理。
- **预训练初始化**：**迁移学习**的常用方法，适合在大数据集预训练的权重上进行微调。

# 评估指标
评估指标是用于**量化模型性能**的重要工具，帮助我们理解模型在处理数据时的准确性和有效性。以下是对常用评估指标的解释：
## 1. 分类问题
对于分类问题，评估指标主要用于衡量模型对不同**类别的预测准确性**。**混淆矩阵**（Confusion Matrix）是评估分类模型表现的**常用工具**。
### 混淆矩阵

|                | 预测正类 (Predicted Positive) | 预测负类 (Predicted Negative) |
|----------------|-------------------------------|-------------------------------|
| **真实正类 (Actual Positive)**   | **TP** (True Positive)      | **FN** (False Negative)      |
| **真实负类 (Actual Negative)**   | **FP** (False Positive)     | **TN** (True Negative)       |

- **TP（True Positive，真正例）**：模型正确地将实际为正类的样本预测为正类。
- **FP（False Positive，假正例）**：模型错误地将实际为负类的样本预测为正类（也称为 Type I 错误）。
- **TN（True Negative，真负例）**：模型正确地将实际为负类的样本预测为负类。
- **FN（False Negative，假负例）**：模型错误地将实际为正类的样本预测为负类（也称为 Type II 错误）。
### 二分类问题的评估指标
- **准确率(Accuracy)**:
  - **计算公式**: $Accuracy=\frac{正确预测的样本数}{总样本数}$
  - **描述**: 表示模型正确分类的样本占总样本的比例。适用于类别分布均衡的情况。
- **精确度(Precision)**:
  - **计算公式**: $Precision = \frac{真正例}{真正例 + 假正例}$
  - **描述**: 表示被预测为正类的样本中，实际为正类的比例。适用于关注假阳性（错误地将负类预测为正类）的场景。
- **召回率(Recall)**:
  - **计算公式**: $Recall=\frac{真正例}{真正例 + 假负例}$
  - **描述**: 表示实际为正类的样本中，被正确预测为正类的比例。适用于关注假阴性（未能识别的正类样本）的场景。
- **F1 - score**:
  - **计算公式**: $F1 - score=2\times\frac{Precision\times Recall}{Precision + Recall}$
  - **描述**: 精确率和召回率的调和平均，提供了一个综合的性能评估，适用于需要平衡精确率和召回率的场景。

### 多分类问题的评估指标
- **准确率**：
  - 同二分类
#### 宏平均 (Macro Average)
  - 计算方式：对**每个类别的指标进行单独计算，然后取平均值**。
  - 适用场景：**适合类别分布均衡**的情况，因为它不会受每个类别样本数量的影响，而是**对每个类别的贡献一视同仁**。
- **公式**:
  - **宏平均精确度**:
$$Macro Precision=\frac{1}{N}\sum_{i = 1}^{N}Precision_{i}$$
  - **宏平均召回率**:
$$Macro Recall=\frac{1}{N}\sum_{i = 1}^{N}Recall_{i}$$
  - **宏平均F1值**:
$$Macro F1=\frac{1}{N}\sum_{i = 1}^{N}F1_{i}$$
其中，$N$是类别的总数，$Precision_{i}$、$Recall_{i}$、$F1_{i}$是第$i$个类别的精确度、召回率和F1值。
#### 微平均 (Micro Average)
  - 计算方式：**先累积每个类别的 TP、FP、TN、FN，然后计算评估指标。**。
  - 适用场景：**类别分布不均衡**的情况，因为它更倾向于以样本为中心，将所有类别的样本总数作为计算基础，因此更能反映数据集中主要类别的表现。

## 2. 回归问题
回归问题评估指标用于衡量**模型预测值与实际值之间的差异**。

- **均方误差(MSE)**:
  - **计算公式**: $MSE=\frac{1}{n}\sum_{i = 1}^{n}(y_{i}-\hat{y}_{i})^{2}$
  - **描述**: 预测值与实际值之间差异的平方的平均值，强调较大的误差。
- **均方根误差(RMSE)**:
  - **计算公式**: $RMSE=\sqrt{MSE}$
  - **描述**: 与均方误差相似，但通过平方根转换回原始单位，使得结果更直观。
- **平均绝对误差(MAE)**:
  - **计算公式**: $MAE=\frac{1}{n}\sum_{i = 1}^{n}|y_{i}-\hat{y}_{i}|$
  - **描述**: 预测值与实际值之间绝对差异的平均值，对异常值不敏感。
- **决定系数($R^{2}$)**:
  - **计算公式**: $R^{2}=1-\frac{SS_{res}}{SS_{tot}}$
  - **描述**: 表示模型解释变异的比例，值介于0到1之间，1表示完美拟合。



> 决定系数详细解释
> 决定系数($R^{2}$)是一种用于评估回归模型拟合优度的指标，表示自变量对因变量变异的解释程度。$R^{2}$的值介于0和1之间，值越高，表示模型的拟合效果越好。以下是$R^{2}$计算公式的详细解析：
>$$R^{2}=1-\frac{SS_{res}}{SS_{tot}}$$
> 
> 1. **$SS_{res}$（残差平方和，Sum of Squares of Residuals）**:
>    - **定义**: 表示模型预测值与实际观察值之间的差异平方和。
>    - **计算公式**: $SS_{res}=\sum_{i}(y_{i}-\hat{y}_{i})^{2}$
>      其中$y_{i}$是实际值，$\hat{y}_{i}$是预测值。
>    - **意义**: 反映了模型未能解释的变异，数值越小，说明模型的预测越接近实际值。
> 2. **$SS_{tot}$（总平方和，Total Sum of Squares）**:
>    - **定义**: 表示实际值与实际值均值之间的差异平方和。
>    - **计算公式**: $SS_{tot}=\sum_{i}(y_{i}-\bar{y})^{2}$
>      其中$\bar{y}$是实际值的均值。
>    - **意义**: 表示实际值的总变异。它衡量了因变量的总方差，即在没有任何模型的情况下，数据本身的变动程度。
> 3. **$R^{2}$的计算**:
>    - **计算思路**:
>      - $1-\frac{SS_{res}}{SS_{tot}}$: 反映了模型解释的变异比例。$R^{2}$的值越接近于1，表示模型越好地解释了数据的变异；越接近于0，表示模型的解释能力较差。
>      - 如果$R^{2}=1$，意味着模型完美拟合所有数据点；如果$R^{2}=0$，表示模型没有解释能力，其预测与实际均值相同。

## 总结
选择合适的评估指标是评估模型性能的关键步骤。分类问题的指标关注于**类别预测的准确性**，而回归问题的指标则关注于**预测值与实际值的差异**。根据具体任务和需求选择合适的指标，可以更好地评估和优化模型性能。

---
# 梯度消失和梯度爆炸
在深度学习中，梯度消失和梯度爆炸是训练深度神经网络时常见的问题，尤其是在涉及多个隐藏层的情况下。这两个问题直接影响模型的学习能力和训练效果。以下是对这两种现象的详细解释：
## 梯度消失 (Vanishing Gradient)
### 描述
梯度消失是指在**反向传播**过程中，**梯度值逐渐减小**，**接近于零**。（在反向传播时，靠近输出层的梯度会较大，但随着层数的增加，梯度在传递过程中会**不断乘以小于1的数**，最终导致靠近输入层的梯度趋近于零，影响网络的训练效果。）这导致**在深层网络中，前面的层几乎不更新权重**，模型难以学习。

### 产生原因
- **激活函数**：使用**Sigmoid或Tanh**等激活函数时，当输入值非常大或非常小时，这些函数的**导数会接近于零**。因为这些激活函数的**输出在极端值时平坦**，导致**梯度几乎消失**。
- **层数增加**：随着**层数的增加**，梯度通过多个层传递，每层的梯度可能都变得非常小，从而导致前面的层几乎没有更新。

### 影响
- **训练速度变慢**：网络难以学习到有效的特征，导致收敛速度变慢。
- **模型性能下降**：网络在面对复杂任务时表现不佳，无法达到预期效果。

### 解决方法
- **使用ReLU激活函数**：使用 ReLU（Rectified Linear Unit）等非饱和激活函数，避免梯度趋近 0。ReLU 的梯度在正区间为常数 1，能够有效缓解梯度消失。
- **权重初始化方法**：使用 Xavier 初始化、He 初始化等方法，使得权重初始值较好地分布在合理范围内，避免过大或过小的权重值引起梯度消失。
- **残差网络（ResNet）**：通过引入残差连接（skip connections），使得梯度可以直接从后面层传递到前面层，从而缓解梯度消失问题。
- **归一化技术**：使用批归一化（Batch Normalization）来保证每层的输入数据分布稳定，从而减小梯度消失的风险。


## 梯度爆炸 (Exploding Gradient)
### 描述
梯度爆炸是指在**反向传播**过程中，随着**网络层数的增加**，**梯度值不断增大，最终导致权重更新过大，模型无法收敛**。链式法则的累积效应，如果某些层的激活函数导数大于1，梯度在传递过程中会**不断乘以大于1的数**，那么这些梯度在层与层之间的传递过程中会被不断放大。

### 产生原因
- **权重初始化不当**：如果权重初始化得过大，可能导致梯度在反向传播时迅速增大。
- **激活函数**：某些激活函数（ReLU ）的导数可能在某些条件下变得非常大，尤其是在深层网络中，连续的乘法可能导致梯度指数级增长。
- **梯度累积效应/网络深度增加**：反向传播中，每层的梯度通过链式法则相乘，如果每层的梯度大于 1，乘积效应会导致梯度迅速变大，尤其是深层网络。
### 影响
- **训练不稳定**：模型的损失函数可能发散，导致无法有效训练。
- **权重溢出**：过大的权重更新可能导致数值溢出，产生NaN（非数）值，导致训练中断。

### 解决方法
- **梯度裁剪（Gradient Clipping）**：在更新权重之前，对梯度进行裁剪，**限制其最大值**，从而避免梯度爆炸。
- **合理的权重初始化**：使用适当的初始化方法，**如Xavier或He初始化**，防止初始化阶段的梯度过大。
- **选择合适的激活函数**：避免使用会导致激活值过大的激活函数，如Sigmoid和Tanh，尤其是在深层网络中。
- **正则化**：通过**正则化（如 L2 正则化）**，可以对权重进行约束，防止其增长过大，从而避免梯度爆炸。
- **优化算法改进**：一些优化算法如 Adam、RMSProp 能够根据梯度历史信息自适应调整学习率，这有助于缓解梯度爆炸。
---

## 总结
梯度消失和梯度爆炸是深度学习训练中的重要问题，影响模型的收敛性和性能。理解这两个现象及其产生原因，有助于采取相应的解决方案，提高深度学习模型的训练效果和稳定性。通过使用合适的激活函数、改进网络结构和优化训练过程，可以有效缓解这些问题。

## Transformer中的优化方案
在 Transformer 模型中，梯度消失和梯度爆炸问题的解决方案涵盖了模型的多个层面，确保模型能够在深度学习训练中保持稳定的梯度传播。以下是主要的技术方法及其作用：

1. **残差连接**：通过为**每层引入直接连接**，确保梯度能够有效传播，缓解深层网络中的梯度消失问题。
   
2. **层归一化**：在每层的输入上进行归一化处理，避免输入分布波动过大，同时解决梯度消失和梯度爆炸问题。

3. **缩放点积注意力**：通过对点积结果进行**缩放**，防止点积值过大，**避免在 softmax 计算时引发梯度爆炸**。

4. **权重初始化**：通过 Xavier 或 He 初始化等策略，确保**初始梯度处于合理范围**，避免梯度在训练初期消失或爆炸。

5. **学习率调度器**：采用 warmup 和 decay 策略，通过动态调整学习率，防止初期梯度过小导致梯度消失，后期避免梯度过大引发梯度爆炸。

6. **梯度裁剪**：在梯度过大时，限制梯度的范数，确保训练过程中不会出现梯度爆炸。

7. **正则化技术**：通过 **L2 正则化**限制权重增长和 **Dropout** 防止过拟合，间接防止梯度爆炸。

这些技术共同作用，使得 Transformer 能够在大规模训练和深层结构中有效应对梯度消失和梯度爆炸问题，从而确保其在自然语言处理、机器翻译等任务中的出色表现。

--- 


# 历史文章

## 机器学习

[机器学习笔记——损失函数、代价函数和KL散度](https://blog.csdn.net/haopinglianlian/article/details/143831958?)
[机器学习笔记——特征工程、正则化、强化学习](https://blog.csdn.net/haopinglianlian/article/details/143832118?)
[机器学习笔记——30种常见机器学习算法简要汇总](https://blog.csdn.net/haopinglianlian/article/details/143832321)
[机器学习笔记——感知机、多层感知机(MLP)、支持向量机(SVM)](https://blog.csdn.net/haopinglianlian/article/details/143832552)
[机器学习笔记——KNN（K-Nearest Neighbors，K 近邻算法）](https://blog.csdn.net/haopinglianlian/article/details/143832692)
[机器学习笔记——朴素贝叶斯算法](https://blog.csdn.net/haopinglianlian/article/details/143832781?)
[机器学习笔记——决策树](https://blog.csdn.net/haopinglianlian/article/details/143834363)
[机器学习笔记——集成学习、Bagging（随机森林）、Boosting（AdaBoost、GBDT、XGBoost、LightGBM）、Stacking](https://blog.csdn.net/haopinglianlian/article/details/143834494?)
[机器学习笔记——Boosting中常用算法（GBDT、XGBoost、LightGBM）迭代路径](https://blog.csdn.net/haopinglianlian/article/details/143834628)
[机器学习笔记——聚类算法（Kmeans、GMM-使用EM优化）](https://blog.csdn.net/haopinglianlian/article/details/143834707)
[机器学习笔记——降维](https://blog.csdn.net/haopinglianlian/article/details/143834847)

## 深度学习
[深度学习笔记——优化算法、激活函数](https://blog.csdn.net/haopinglianlian/article/details/143835137)
[深度学习笔记——归一化、正则化](https://blog.csdn.net/haopinglianlian/article/details/143835273)

