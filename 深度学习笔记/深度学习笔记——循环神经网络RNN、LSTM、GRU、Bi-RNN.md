> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本文详细介绍面试过程中可能遇到的循环神经网络RNN、LSTM、GRU、Bi-RNN知识点。
> 
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/a5e1866da2d34cce8e434dc7df4801cb.png#pic_center)


---
> @[toc]
> 大家好，这里是Goodnote(好评笔记)。本文详细介绍面试过程中可能遇到的循环神经网络RNN、LSTM、GRU、Bi-RNN知识点。
> 


# 文本特征提取的方法
## 1. 基础方法
### 1.1 词袋模型（Bag of Words, BOW）
**词袋模型**最简单的方法。它将文本表示为一个**词频向量**，**不考虑词语的顺序或上下文关系**，只统计每个词在文本中出现的频率。
#### 工作原理
1. **构建词汇表**：对整个语料库中的所有词汇建立一个词汇表（也称为词典）。每个文档中的每个词都与词汇表中的一个位置对应。
2. **生成词频向量**：对于每个文本（文档），生成一个**与词汇表长度相同的向量**。向量中每个元素表示该词在文档中出现的次数（或者是否出现，用二进制表示）。
#### 举例
假设有两个句子：
- 句子 1：`猫 喜欢 鱼`
- 句子 2：`狗 不 喜欢 鱼`

词汇表 = ["猫", "狗", "喜欢", "不", "鱼"]

- 句子 1 的词袋向量表示为：[1, 0, 1, 0, 1]
- 句子 2 的词袋向量表示为：[0, 1, 1, 1, 1]
#### 优点
- 简单直观，易于实现，有效地表示词频信息。
#### 缺点
- **忽略词序**：词袋模型无法捕捉词语的顺序，因此在语义表达上有局限。
- **高维稀疏**：对于大词汇表，词袋模型会生成非常长的特征向量，大多数元素为 0，容易导致**稀疏矩阵，影响计算效率**。
- **受到常见词的影响**：常见词（如 "the"、"and" 等）可能在各类文档中频繁出现，但对语义贡献较少，词袋模型会受到这些高频词的影响，降低模型的效果。

### 1.2 TF-IDF（Term Frequency-Inverse Document Frequency）
**TF-IDF** 是对词袋模型的改进，它**为词语赋予不同的权重**，来**衡量每个词在文档中的重要性**。与词袋模型相比，TF-IDF 不仅考虑词频，还考虑词的普遍性，**以避免常见词（如"the"、"and"）的影响**。
#### 工作原理


1. **TF（词频）**：计算某个词在文档中出现的频率。
  $$ TF(t,d)=\frac{词t在文档d中的出现次数}{文档d的总词数} $$
2. **IDF（逆文档频率）**：衡量词在整个语料库中的普遍性，出现频率越低的词权重越高。
    $$IDF(t)=\log\left(\frac{N}{1 + DF(t)}\right) $$
     - 其中$N$是文档总数，$DF(t)$是包含词$t$的文档数。
3. **TF - IDF**：将$TF$和$IDF$相乘，得到词在特定文档中的权重：
  $$ TF - IDF(t,d)=TF(t,d)\times IDF(t) $$



#### 举例
对于句子“猫 喜欢 鱼”和“狗 不 喜欢 鱼”，假设 "喜欢" 出现在所有文档中，IDF 会给它较低的权重，而像 "猫"、"狗" 这样的词会有较高的 IDF 权重，因为它们只出现在一部分文档中。
#### 优点
- **更准确地反映词的重要性，避免了词袋模型中常见词占主导地位的情况**。尤其适用于文本分类任务。
#### 缺点
- **稀疏矩阵**：虽然词频的权重经过调整，但**词汇表的大小仍然很大，容易产生稀疏矩阵问题**。
- **忽略词序**：仍然**无法捕捉词语之间的顺序和上下文关系**。
### 1.3 TF-IDF的改进——BM25
BM25**对TF和IDF进行加权**，同时考虑文档长度对相关性的影响，使得**对较短和较长文档的评分更加合理**。

BM25 的计算公式如下：


$$ BM25(q,d)=\sum_{t\in q}IDF(t)\cdot\frac{TF(t,d)\cdot(k_1 + 1)}{TF(t,d)+k_1\cdot(1 - b + b\cdot\frac{|d|}{avgdl})} $$
其中：
- $q$ 是查询，$d$ 是文档，$t$ 是查询中的词。
- $IDF$是与$TF - IDF$相似的逆文档频率。
- $TF$是词频。
- $k_1$ 是调节词频饱和度的参数，通常取值范围为$[1.2,2.0]$。
- $b$ 是调节文档长度的参数，通常取值范围为$[0.0,1.0]$，$b = 0.75$是一个常用的设置。
- $|d|$是文档的长度（词数），$avgdl$是语料库中文档的平均长度。


TF-IDF 中的 IDF（逆文档频率）使用$\log\frac{N}{df(t)}$来衡量词的普遍性。然而这种计算方式可能会导致在某些极端情况下（如 df(t) = 0 ）出现不合理的结果。
BM25 对 IDF 进行了小改进，以提高在极端情况下的稳定性：

$$ IDF(t)=\log\frac{N - df(t)+ 0.5}{df(t)+ 0.5} $$

这种改进的 IDF 计算在文档数量较少或者某个词的出现频率极高时，能提供更合理的 IDF 值，增加了 BM25 的稳定性。
#### 优化
相比于 TF-IDF，BM25 主要做了以下改进：
- **非线性词频**缩放：**通过$k_1$ 控制词频TF饱和** ，避免 TF 值无限增大导致的偏差。
- **文档长度归一化**：使用**参数$b$调整文档长度对评分的影响**，防止长文档得分偏高。
- **改进的 IDF 计算**：**使用平滑**后的 IDF 计算，保证在**极端情况下的稳定性**。
- **查询词频考虑**：在评分中更合理地衡量查询中词频的影响，提高了对复杂查询的检索效果。
### 1.4 N-Gram 模型
**N-Gram 模型**是一种**基于词袋模型**的**扩展**方法，它通过**将词组作为特征，来捕捉词语的顺序信息**。
#### 工作原理
- **N-Gram** 是指在文本中**提取连续的 n 个词组成的词组作为特征**。当 n=1 时，即为 unigram（单词级别特征）；当 n=2 时，即为 bigram（双词组特征）；当 n=3 时，即为 trigram（词三元组特征）。
  
- 在提取 N-Gram 时，模型不仅关注单个词，还捕捉到词与词之间的顺序和依赖关系。例如，2-Gram 模型会将句子分解为相邻的两词组合。

#### 举例
对于句子“猫 喜欢 吃 鱼”，2-Gram 模型会提取出以下特征：
- ["猫 喜欢", "喜欢 吃", "吃 鱼"]
#### 优点
- 能**捕捉到顺序和依赖关系**，比单词级别的特征表达更丰富。n 越大，模型捕捉的上下文信息越多。
#### 缺点
- **维度膨胀**：n 值越大，特征向量的维度会急剧增加，容易导致稀疏矩阵和计算复杂度升高。
- 对长文本，N-Gram 模型可能会生成非常多的组合，**计算资源消耗较大**。
## 2. 词向量（Word Embeddings）
**词向量**是现代 NLP 中的关键特征提取方法，能够捕捉词语的语义信息。常见的词向量方法包括 Word2Vec、GloVe、和 FastText。词向量的核心思想是**将每个词表示为一个低维的、密集的向量，词向量之间的相似性能够反映词语的语义相似性**。
### 2.1 Word2Vec
**Word2Vec** 是一种**使用浅层神经网络学习词向量的模型**，由 Google 在 2013 年提出。它有两种模型架构：CBOW 和 Skip-gram。
#### 工作原理
- **CBOW（Continuous Bag of Words）**：**根据上下文中的词语来预测中心词**。模型输入是上下文词语，输出是预测的中心词。
- **Skip-gram**：与 CBOW 相反，它是**根据中心词来预测上下文中的词语**。
#### 举例
对于一个句子 "猫 喜欢 吃 鱼"，CBOW 会使用上下文 ["猫", "吃", "鱼"] 来预测 "喜欢"，而 Skip-gram 则会使用 "喜欢" 来预测上下文。
#### 优点
- **语义相似性**：Word2Vec 生成的词向量**能够捕捉词语之间的语义相似性**。例如，“king” 和 “queen” 的词向量会非常相近。
- **稠密向量**：与词袋模型和 TF-IDF 生成的高维稀疏向量不同，Word2Vec 生成的词向量是**低维的密集向量**（如 100 维或 300 维），**更加高效**。
#### 缺点
- **无法处理 OOV（未登录词）**：如果测试集中出现了训练集中未见过的词，Word2Vec 无法为其生成词向量。
- **上下文无关**：Word2Vec 生成的**词向量是固定的，无法根据上下文变化来调整词向量**。
### 2.2 FastText
**FastText** 是 Facebook 提出的词向量方法，它是 Word2Vec 的改进版。FastText 通过**将词分解为n-gram**字符级别的子词，捕捉**词的形态信息**。
#### 工作原理
- FastText 将词分解为多个字符 n-gram，然后对每个 n-gram 生成词向量。通过这种方式，FastText 可以**捕捉到词语内部的形态信息**，尤其对拼写错误或**未登录词**有较好的处理能力。
#### 优点
- **处理 OOV（未登录词）**：因为 FastText 基于子词生成词向量，它**能够为未见过的词生成向量表示**。
- **考虑词形信息**：能够捕捉词的形态变化，例如词根、前缀、后缀等。
#### 缺点
- **计算复杂度较高**：相比 Word2Vec，FastText 需要对每个词生成多个 n-gram，因此计算量更大。

## 3. 预训练模型：BERT（Bidirectional Encoder Representations from Transformers）
**BERT** 是一种基于 Transformer 架构的预训练语言模型，由 Google 于 2018 年提出。与传统的词向量方法不同，BERT 通过双向的 Transformer 网络，能够生成上下文相关的动态词向量。
#### 工作原理
- **双向Transformer**：BERT 同时从词语的前后上下文学习词的表示，而不像传统的模型只从前向或后向学习。这样，BERT 能够捕捉到更丰富的语义信息。
- **预训练任务**：
  1. **遮蔽语言模型（Masked Language Model, MLM）**：在训练时，BERT 会随机遮蔽部分词语，并要求模型预测这些词，从而让模型学到上下文的双向依赖关系。
  2. **下一句预测（Next Sentence Prediction, NSP）**：训练时，BERT 要预测两句话是否是连续的句子对，这让模型能够学习句子级别的关系。
#### 优点
- **上下文相关词向量**：BERT 生成的词向量是上下文相关的。例如，"bank" 在句子 "I went to the bank" 和 "The river bank" 中会有不同的向量表示。
- **强大的语义理解能力**：BERT 在问答、阅读理解、文本分类等任务中表现非常好，能够捕捉到复杂的语义关系。
#### 缺点
- **计算资源需求大**：BERT 是一个深层的 Transformer 模型，预训练和微调都需要大量的计算资源，训练时间较长。
- **较慢的推理速度**：由于模型较大，在实际应用中推理速度较慢，尤其在实时任务中。

> BERT详细参考历史/后续文章：[深度学习笔记——GPT、BERT、T5]

## 总结
| **方法**             | **工作原理**                                                                                                                                                     | **优点**                                                                                                                                                       | **缺点**                                                                                                           | **适用场景**                                      |
|----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|---------------------------------------------------|
| **词袋模型（BOW）**  | 将文本表示为**词频向量**，**不考虑词序和上下文**。                                                                                                                      | 简单直观，易实现，能够有效表示词频信息。                                                                                                                       | 忽略词序，生成高维稀疏向量。                                                                                      | 文本分类、信息检索                                |
| **TF-IDF**           | **基于词袋模型**，**考虑词**在文档中的频率以及整个语料库中的**普遍性**，赋予不同词**权重**。                                                                                     | 反映词的重要性，避免常见词主导影响，适用于文本分类。                                                                                                           | 生成稀疏矩阵，**无法捕捉词序和上下文关系**。                                                                        | 文本分类、关键词提取                              |
| **BM25**             | 基于 TF-IDF 的改进，考虑词频、文档长度、词重要性等因素，以计算每个词对文档匹配的相关性得分。     **非线性词频**缩放、 **文档长度归一化**、 **改进的 IDF 计算**                                                                    | 更好地反映词在文档中的相关性，更适合信息检索，适用于长文档，计算匹配更准确。                                                                                   | 对参数敏感，适用性依赖于超参数调优，不能捕捉上下文关系。                                                           | 信息检索、文档排名                                 |
| **N-Gram**           | 捕捉连续 **n 个词作为特征**，**考虑词序信息**。                                                                                                                          | 能捕捉词语的顺序和依赖关系，n 越大捕捉的上下文信息越多。                                                                                                       | 维度膨胀，计算资源消耗大。                                                                                      | 语言模型、短文本分类                              |
| **Word2Vec**         | 使用**浅层神经网**络学习词向量，有 **CBOW** 和 **Skip-gram** 两种架构。                                                                                                      | 词向量能**捕捉语义相似性**，生成低维稠密向量，效率高。                                                                                                             | **无法处理未登录词**（OOV），词向量上下文无关。                                                                     | 词嵌入、相似度计算、文本分类                      |
| **FastText**         | 将**词分解为字符 n-gram**，生成词向量，捕捉词的形态信息。                                                                                                            | **能处理未登录词**，捕捉词形信息，适合拼写错误和变形词。                                                                                                            | 计算复杂度高于 Word2Vec。                                                                                        | 词嵌入、拼写纠错、文本分类                        |
| **BERT**             | 基于双向 Transformer，通过预训练生成上下文相关的词向量，支持 **Masked Language Model 和 Next Sentence Prediction**。                                                  | 生成上下文相关词向量，语义理解强，适用于复杂 NLP 任务。                                                                                                         | 需要大量计算资源，训练和推理时间长。                                                                            | 问答系统、文本分类、阅读理解                      |

- **传统方法**：如词袋模型、TF-IDF 和 N-Gram 易于实现，但无法捕捉语义和上下文信息。
- **词向量方法**：如 Word2Vec 和 FastText 通过词嵌入表示词语的语义关系，适合语义相似度计算、文本分类等任务。FastText 能够处理未登录词。
- **预训练模型**：如 BERT，能够生成上下文相关的动态词向量，适用于更复杂的自然语言处理任务，但对计算资源的要求更高。

---
# RNN
循环神经网络（RNN，Recurrent Neural Network）是一种用于**处理序列数据等具有顺序关系的数据**的神经网络。与传统的前馈神经网络不同，RNN **具有循环连接** ，允许**信息通过隐藏状态在序列的不同时间步之间传播**。这种结构使得RNN非常适合处理时间序列、文本数据、语音信号等**具有顺序关系的数据**。
## RNN 参数


|参数|维度|作用|
|---|---|---|
|输入权重矩阵$W_{x h}$|$(d_{hidden}\times d_{input})$|将输入$x_t$映射到隐藏状态，确定当前输入对隐藏层状态的影响。|
|隐藏状态权重矩阵$W_{h h}$|$(d_{hidden}\times d_{hidden})$|将前一时间步的隐藏状态$h_{t - 1}$传递到当前时间步$h_t$，捕捉时间依赖关系。|
|输出权重矩阵$W_{h y}$|$(d_{output}\times d_{hidden})$|将隐藏状态$h_t$映射为输出$y_t$。|
|隐藏层偏置向量$b_{h}$|$(d_{hidden})$|增强隐藏层的灵活性，通过加偏置调整隐藏层的激活函数输出。|
|输出层偏置向量$b_{y}$|$(d_{output})$|增强输出层的灵活性，通过加偏置调整输出层的结果。|
|激活函数|—|用于隐藏层状态的非线性变换，常用$tanh$或$ReLU$。|


其他相关参数：
- **时间步（Time Steps）**：决定模型的循环次数，非可学习参数。
- **损失函数（Loss Function）**：指导模型参数更新的依据，非可学习参数。
- **学习率（Learning Rate）**：控制优化过程的步幅大小，超参数。
## RNN 的特点
1. **顺序处理**：RNN 可以**处理不同长度的输入序列**，这是由于其内部结构允许将前一步的信息作为当前步的输入之一。
2. **隐藏状态**：RNN 具有隐藏状态，隐藏状态是前一个时间步的信息的压缩，并与当前输入一起决定下一时间步的输出。
3. **权重共享**：RNN 中**每个时间步之间共享相同的网络权重( $W_{hh}$,  $W_{xh}$,  $W_{hy}$)**，减少了模型参数的数量，适合处理序列长度不同的问题。

## RNN 的局限性
1. **梯度消失与爆炸问题**：在长序列处理中，由于反向传播算法在计算梯度时，RNN 容易出现梯度消失（gradient vanishing）或梯度爆炸（gradient exploding）的现象，导致模型难以学习长期依赖关系。
2. **长时间依赖问题**：RNN 处理长序列时，无法有效捕捉到前后相距较远的依赖关系，导致模型的性能下降。
3. **并行化困难**：由于 RNN 是逐时间步处理序列的，因此不容易并行化处理，这使得其训练时间较长。
## 前向传播的核心计算
在 RNN 中，当前时间步的输出不仅依赖于**当前输入**，还依赖于之前**时间步的隐状态**（hidden state）。隐状态是 RNN 中的一个内部存储器，它能够保存之前的时间步的信息，使得网络具备记忆能力。RNN 的计算公式如下：
###  隐状态更新


$$ h_t=f(W_{h h}h_{t - 1}+W_{x h}x_t + b_{h}) $$
- $h_t$：时间步$t$的隐藏状态，是通过上一时间步$t - 1$的隐藏状态$h_{t - 1}$和当前的输入$x_t$计算得到的。
- $W_{h h}$：隐藏状态到隐藏状态的权重矩阵，用于表示时间步之间的状态传递。
- $W_{x h}$：输入到隐藏状态的权重矩阵，负责将当前输入$x_t$映射到隐藏层。
- $b_{h}$：隐藏层的偏置向量。
- $f$：激活函数，通常使用$tanh$或$ReLU$。

### 输出更新

$$ y_t = g(W_{hy}h_t + b_y) $$
- $y_t$：时间步$t$的输出。
- $W_{hy}$：隐藏状态到输出的权重矩阵。
- $b_y$：输出层的偏置。
- $g$：输出层的激活函数，取决于具体的任务，如分类任务常用Softmax。


## RNN 的训练流程

RNN 的训练主要包括以下步骤：
### 1. 输入准备

- 输入数据：RNN处理的是序列数据，输入可以是时间序列、文本、语音等。输入通常表示为$X = [x_1, x_2, \ldots, x_T]$，其中$x_t$代表时间步$t$时的输入。
- 标签数据（监督学习）：如果是监督学习任务，训练数据通常带有标签$Y = [y_1, y_2, \ldots, y_T]$，表示每个时间步$t$的目标输出。
### 2. 前向传播（Forward Pass）
逐个时间步执行前向传播，将**输入数据逐步传递到隐藏层，计算每个时间步的隐藏状态和输出**（上面的核心计算）。
- **初始化隐藏状态**：在时间步  t = 0  时，隐藏状态 $h_0$通常**初始化为 0 或随机值**。
- **逐时间步的状态更新**：对于每一个时间步  t  ，计算当前时间步的隐藏状态和输出：


(1). **隐藏状态更新：**
$$ h_t = f(W_{hh}h_{t - 1}+W_{xh}x_t + b_h) $$
   - $h_t$：时间步$t$的隐藏状态，是通过上一时间步$t - 1$的隐藏状态$h_{t - 1}$和当前的输入$x_t$计算得到的。
   - $W_{hh}$：隐藏状态到隐藏状态的权重矩阵，用于表示时间步之间的状态传递。
   - $W_{xh}$：输入到隐藏状态的权重矩阵，负责将当前输入$x_t$映射到隐藏层。
   - $b_h$：隐藏层的偏置向量。
   - $f$：激活函数，通常使用tanh或ReLU。

(2). **输出更新：**
$$ y_t = g(W_{hy}h_t + b_y) $$

   - $y_t$：时间步$t$的输出。
   - $W_{hy}$：隐藏状态到输出的权重矩阵。
   - $b_y$：输出层的偏置。
   - $g$：输出层的激活函数，取决于具体的任务，如分类任务常用Softmax。

### 3. 计算损失（Loss Calculation）
- **损失函数**：RNN 根据每个时间步的预测输出$y_t$ 和真实标签$\hat{y}_t$计算损失。常见的损失函数包括：
  - 分类任务使用 **交叉熵损失**。
  - 回归任务使用 **均方误差（MSE）**。


总的损失是各个时间步损失的累加：
$$ L=\sum_{t = 1}^{T}L_{oss}(y_t, \hat{y}_t) $$
### 4. 反向传播（Backward Pass）
RNN 的反向传播主要通过**反向传播通过时间（Backpropagation Through Time, BPTT）**来更新权重。BPTT 是对标准反向传播算法的扩展，沿着时间轴进行梯度传递。

**反向传播的核心步骤**：

- 从时间步$T$开始，逐步向前计算各个时间步的梯度，直至第一个时间步。
- 在每个时间步，计算损失相对于输出$y_t$、隐藏状态$h_t$以及参数（权重和偏置）的梯度。
	- 对损失函数$L$进行求导，得到每个时间步的梯度，并依次更新参数：
	$$ W_{xh}\leftarrow W_{xh}-\alpha\frac{\partial L}{\partial W_{xh}} $$
	$$ W_{hh}\leftarrow W_{hh}-\alpha\frac{\partial L}{\partial W_{hh}} $$
	$$ W_{hy}\leftarrow W_{hy}-\alpha\frac{\partial L}{\partial W_{hy}} $$
	 - 其中$\alpha$是学习率。
- **梯度消失问题**：BPTT 在长时间序列上可能出现梯度消失或爆炸问题，尤其是当梯度逐步传递时，这限制了 RNN 捕捉长时依赖的能力。
### 5. 参数更新
- 根据反向传播得到的梯度更新网络中的权重参数,如 $W_{xh}$, $W_{hh}$, $W_{hy}$，完成当前批次的训练。
- 继续执行下一个批次的训练，直至完成所有训练数据的迭代。

## RNN 的推理流程
RNN 的**推理**（inference）过程**与训练过程中的前向传播类似**，但**没有反向传播和参数更新**，主要是用于生成输出或进行预测。

### 1. 输入准备
- 输入序列数据 $X = [x_1, x_2, ..., x_T]$。
- 不需要提供标签数据，因为推理阶段是无监督的，RNN 根据输入数据生成输出。
### 2. 前向传播
推理阶段执行与训练相同的前向传播过程：

1. **初始化隐藏状态**：
   - 与训练时一样，隐藏状态 $h_0$初始化为 0 或随机值。  
2. **逐时间步的状态更新**：
- 对每个时间步$t$执行前向传播，计算隐藏状态$h_t$和输出$y_t$：
$$h_t = f(W_{xh}x_t + W_{hh}h_{t - 1}+b_h)$$
$$y_t = g(W_{hy}h_t + b_y)$$
### 3. 生成输出
- 推理过程中，RNN 根据输入数据在每个时间步生成相应的输出$y_t$。
- 在序列生成任务（如文本生成）中，RNN 的输出 $y_t$ 可以作为下一个时间步的输入 $x_{t+1}$，从而生成一个完整的输出序列。

### 4. 推理结束
- 推理结束时，RNN 已经生成了完整的输出序列或预测结果。

## RNN参数初始化
RNN 的参数初始化策略会影响训练过程的稳定性和收敛速度，以下是不同参数的初始化方法概述：

| **参数**                            | **初始化方法**                                                | **适用场景**                                         |
|---------------------------------|--------------------------------------------------------------|-----------------------------------------------------|
| **输入权重 \( $W_{xh}$ \)**  | Xavier 初始化、He 初始化、标准正态分布                        | 用于**将输入映射到隐藏状态**，适合不同激活函数场景       |
| **隐藏状态权重 \( $W_{hh}$ \)** | 正交初始化、Xavier 初始化、He 初始化                       | **隐藏状态循环权重**，正交初始化适合处理梯度问题         |
| **输出权重 \( $W_{hy}$ \)**  | Xavier 初始化、He 初始化                                   | 用于**将隐藏状态映射到输出层**，取决于任务类型          |
| **偏置项 \( $b_h, b_y$ \)**   | 零初始化、小随机数、遗忘门的偏置可初始化为较大正值（LSTM/GRU） | **偏置项通常为零初始化**，特殊情况下设定固定值           |

通过使用**合适的初始化方法**，可以显著提高 RNN 的收敛性和模型的训练效果，尤其在处理长序列时，**正交初始化**和 **Xavier 初始化**能帮助缓解梯度消失和梯度爆炸问题。
# LSTM（Long Short-Term Memory）
LSTM 是 RNN 的一种改进版本，旨在**解决 RNN 的长时间依赖问题**。LSTM 通过引入**记忆单元（cell state）** 和**门控机制（gates）** 来有效地控制信息流动，使得它在长序列建模中表现优异。
### LSTM 的核心部件
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/f8d8cec0cc37480a92629654c7c9415b.png)
LSTM 的核心结构由以下几部分组成：
- **记忆单元（Cell State）**：贯穿整个序列的数据流【图中的C】，能够存储序列中的重要信息，允许网络长时间保留重要的信息。
- **隐藏状态（Hidden State）**：每个时间步的输出，LSTM 通过它来决定当前的输出和对下一时间步的传递信息。【RNN中就有】
- **三个门控机制（Forget Gate、Input Gate、Output Gate）**：通过这些门控机制，LSTM 可以**选择性地遗忘、存储、或者输出信息**（具体在图中的结构参考下面具体介绍）。

LSTM 中最重要的概念是**记忆单元**状态和**门控机制**，它们帮助网络在长时间序列中保留重要的历史信息。
> 在 LSTM 中，**隐藏状态**是对当前时间步的即时记忆（**短期记忆**），而**记忆单元**是对整个序列中长期信息的存储（**长期记忆**）。

1. **遗忘门（Forget Gate）**：根据**当前输入和前一个时间步的隐藏状态**，决定**记忆单元哪些信息需要被遗忘**；
2. **输入门（Input Gate）**：根据**当前输入和前一时间步的隐藏状态**，决定当前时间步输入**对记忆单元的影响**；
3. **输出门（Output Gate）**：根据**当前的输入和前一时间步的隐藏状态以及记忆单元状态**，决定**当前时间步隐藏状态的输出/影响**；（输出内容是从记忆单元中提取的信息）；

### LSTM 的公式和工作原理

在 LSTM 中，每个时间步 \( t \) 的计算分为以下几步：
图像参考：[LSTM（长短期记忆网络）](https://mp.weixin.qq.com/s?__biz=MzUyODk0Njc1NQ==&mid=2247483926&idx=1&sn=8cb4861ad6f4a56f8f233c322ebcc5b3&chksm=fa69c13acd1e482ca143d21d476c4f7242a1e438c3a9103fe0931ee18b528176e5c765fa8678&scene=27)
#### (1) 遗忘门（Forget Gate）
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c2418e7f679346f491c03333eec30dc8.png)



- 计算公式：
  $$f_t=\sigma(W_f\cdot[h_{t - 1},x_t]+b_f)$$
   - $f_t$：遗忘门的输出，值介于0到1之间，表示记忆单元中的每个值需要被保留的比例。
   - $h_{t - 1}$：上一时间步的隐藏状态（短期记忆）。
   - $x_t$：当前时间步的输入。
   - $W_f$、$b_f$：遗忘门的权重和偏置。
   - $\sigma$：sigmoid函数，将值限制在0到1之间。


**遗忘门的作用**：它根据**当前输入**和**前一个时间步的隐藏状态**，选择哪些来自过去的**记忆单元信息需要被遗忘**。

#### (2) 输入门（Input Gate）
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/7a29ace8644646a6a04c76c510e2fb8c.png)



- 计算公式：
  $$i_t=\sigma(W_i\cdot[h_{t - 1},x_t]+b_i)$$
   - $i_t$：输入门的输出，值介于0到1之间，表示是否更新记忆单元。
   - $W_i$、$b_i$：输入门的权重和偏置。
- 候选记忆生成：
  $$\tilde{C}_t=\tanh(W_c\cdot[h_{t - 1},x_t]+b_c)$$
   - $\tilde{C}_t$：候选记忆，是根据当前输入生成的新的记忆内容，值在$[- 1,1]$之间。
   - $W_c$、$b_c$：生成候选记忆的权重和偏置。



**输入门的作用**：输入门通过 sigmoid 激活函数决定**当前输入** \( $x_t$ \) 和**前一时间步的隐藏状态** \( $h_{t-1}$ \) **对记忆单元的影响**。结合候选记忆 \($\tilde{C}_t$\)，输入门决定**是否将当前输入的信息入到记忆单元中**。 

#### (3) 更新记忆单元状态
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/4947a6b561ac44d6a3bdb39d1f2c1f12.png)



- 记忆单元状态更新公式：
  $$C_t=f_t*C_{t - 1}+i_t*\tilde{C}_t$$
   - $f_t*C_{t - 1}$：遗忘门决定了哪些来自前一时间步的记忆单元信息被保留。
   - $i_t*\tilde{C}_t$：输入门决定了新的候选记忆$\tilde{C}_t$需要被加入到记忆单元中的比例。

**记忆单元的作用**：记忆单元 \( $C_t$ \) 根据**遗忘门**和**输入门**的输出，**保留了来自过去的长期信息**，使得重要的历史信息能够长时间存储。

#### (4) 输出门（Output Gate）
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/ddefb84ab55e42ac92b04cd51e447c2e.png)

**输出门**控制从记忆单元中提取多少信息**作为当前时间步的隐藏状态** $h_t$ 并输出。


- 计算公式：
  $$o_t=\sigma(W_o\cdot[h_{t - 1},x_t]+b_o)$$
    - $o_t$：输出门的输出，决定隐藏状态的输出比例。
    - $W_o$、$b_o$：输出门的权重和偏置。
- 生成当前隐藏状态：
  $$h_t=o_t*\tanh(C_t)$$
    - $\tanh(C_t)$：对当前的记忆单元状态$C_t$进行非线性变换，生成当前时间步的隐藏状态。
    - 输出门$o_t$决定了多少信息从记忆单元状态$C_t$中提取，并输出为当前时间步的隐藏状态。


**输出门的作用**：输出门根据**当前的输入**和**前一时间步的隐藏状态**以及**记忆单元状态**，**决定当前的隐藏状态 \( $h_t$ \) 的值**，它不仅作为当前时间步的输出，还会传递到下一时间步。

### LSTM 的流程总结
在每个时间步 \( $t$ \)，LSTM 会执行以下步骤：
1. **遗忘门**：根据当前输入和前一个时间步的隐藏状态，控制哪些来自上一个时间步的记忆单元信息需要被保留或遗忘。
2. **输入门**：根据当前输入和前一时间步的隐藏状态，决定当前输入信息是否更新到记忆单元中，通过候选记忆生成新的信息。
3. **记忆单元状态更新**：根据遗忘门和输入门的输出，更新当前时间步的记忆单元状态 \( $C_t$ \)。
4. **输出门**：根据当前的输入和记忆单元状态，控制当前时间步的隐藏状态 \( $h_t$ \) 的输出，隐藏状态会传递到下一时间步，作为当前的输出结果。
### LSTM 的优点
LSTM 通过**引入门控机制**，可以**选择性地控制信息的流动**；记忆单元可以**有效地保留长期信息**，**避免**了传统 RNN 中的**梯度消失问题**。因此，LSTM 能够**同时处理短期和长期的依赖关系**，尤其在需要保留较长时间跨度信息的任务中表现优异。
### LSTM 的局限性
LSTM 的门控机制使得它的**结构复杂，训练时间较长**，需要**更多的计算资源**，尤其是在处理大规模数据时。依赖于序列数据的时间步信息，必须按顺序处理每个时间步，**难以并行化**处理序列数据。
# GRU（Gated Recurrent Unit）
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/819186a136a34e20a7551a03730ea66c.png)
### GRU 的基本结构
相比于 LSTM，GRU 的结构更为简洁：
- **少了一个门控**：GRU 合并 LSTM 中的**遗忘门**和**输入门**，变为一个**更新门**来实现。
- **没有单独的记忆单元状态**：GRU 将 LSTM 中的**记忆单元状态（Cell State）**合并到**隐藏状态（Hidden State）** 中，隐藏状态既用于存储当前时间步的信息，也用于在时间步之间传递长期信息。

这种简化使得 GRU 具有更少的参数，训练速度更快，并且在某些任务上，GRU 的性能和 LSTM 相当甚至更优。


GRU 主要由两个门组成：
**更新门**：
- **输入**：**前一时间步的隐藏状态 \( $h_{t-1}$ \)** 和**当前时间步的输入 \( $x_t$ \)**。
- **作用**：决定**当前时间步的隐藏状态** **多少旧的信息需要保留 \( $h_{t-1}$ \) ，以及多少新的信息需要加入 \( $x_t$ \)**

**重置门**：
- **输入**：**前一时间步的隐藏状态 \( $h_{t-1}$ \)** 和**当前时间步的输入 \( $x_t$ \)**。
- **作用**：控制**生成新的候选隐藏状态**时，**前一时间步的隐藏状态 \( $h_{t-1}$ \) 被遗忘或保留的程度**。

这两个门决定了每个时间步的信息如何被保留、更新或丢弃。通过这两个门，GRU 能够高效地捕捉短期和长期的依赖关系。
### GRU 的工作原理与公式

在 GRU 中，给定一个输入序列 \( $x_1, x_2, ......, x_T$\)，每个时间步 \( $t$ \) 的计算过程分为以下几个步骤：
####  (1) 更新门（Update Gate）
更新门$Z_t$ 控制着当前时间步的**隐藏状态**与**前一时间步的隐藏状态**之间的融合。它决定了**多少旧的信息$h_{t-1}$需要保留**，以及**多少新的信息$x_t$需要加入**，从而控制隐藏状态的更新。


- 计算公式：
  $$z_t=\sigma(W_z\cdot[h_{t - 1},x_t]+b_z)$$
    - $z_t$：更新门的输出，表示前一时间步隐藏状态$h_{t - 1}$和当前时间步输入$x_t$的融合比例。
    - $h_{t - 1}$：前一时间步的隐藏状态（即记忆）。
    - $x_t$：当前时间步的输入。
    - $W_z$、$b_z$：更新门的权重和偏置。
    - $\sigma$：sigmoid函数，用于将输出压缩到0和1之间。


#### (2) 重置门（Reset Gate）

重置门$r_t$决定如何**将前一时间步的隐藏状态\( $h_{t-1}$ \) **和**当前输入 \( $x_t$ \) 结合**，**生成新的候选隐藏状态**。它控制**前一时间步的隐藏状态** \( $h_{t-1}$ \) 在**多大程度上被遗忘**。


- 计算公式：
  $$r_t=\sigma(W_r\cdot[h_{t - 1},x_t]+b_r)$$
    - $r_t$：重置门的输出，决定是否丢弃前一时间步的隐藏状态信息。
    - 其他符号与更新门公式中的符号相同。

重置门$r_t$控制了从前一时间步的隐藏状态$h_{t - 1}$中提取多少信息。如果$r_t$值接近0，则大部分历史信息将被忽略，这在捕捉短期依赖时尤为重要；如果$r_t$值接近1，则保留更多的历史信息，有助于捕捉长期依赖。




#### (3) 生成候选隐藏状态（Candidate Hidden State）
结合当前输入 \( $x_t$ \) 和部分前一时刻隐藏状态信息和重置门的输出 \( $r_t * h_{t-1}$ \) 生成的候选隐藏状态 \( $\tilde{h}_t$ \) 

- 计算公式：
  $$\tilde{h}_t=\tanh(W_h\cdot[r_t*h_{t - 1},x_t]+b_h)$$
    - $\tilde{h}_t$：候选隐藏状态，表示当前时间步的临时记忆。
    - $r_t*h_{t - 1}$：将重置门的输出$r_t$与前一时间步的隐藏状态$h_{t - 1}$进行逐元素乘法，这决定了在生成候选状态时保留多少过去的信息。
    - $W_h$、$b_h$：候选隐藏状态的权重和偏置。
    - $\tanh$：$\tanh$函数将输出压缩到$[-1,1]$，确保隐藏状态的平稳更新。

#### (4) 隐藏状态更新
**最终的隐藏状态** \( $h_t$ \) 是根据**更新门** \( $z_t$ \) 的输出来决定的。更新门 \( $z_t$ \) 控制了当前隐藏状态 \( $h_t$ \) 是更依赖于上一时刻的隐藏状态 \( $h_{t-1}$ \) 还是新计算的候选隐藏状态 \( $\tilde{h}_t$ \)。


- 更新公式：
  $$h_t=z_t*h_{t - 1}+(1 - z_t)*\tilde{h}_t$$
    - $z_t*h_{t - 1}$：保留一部分前一时间步的隐藏状态。
    - $(1 - z_t)*\tilde{h}_t$：将新计算的候选隐藏状态加入隐藏状态中。

如果更新门$z_t$接近0，则保留更多的候选隐藏状态$\tilde{h}_t$，意味着网络更多地使用当前时间步的新信息；如果$z_t$接近1，则保留更多的历史信息$h_{t - 1}$，使得网络保持之前的记忆。


### GRU 的优点
- GRU 和 LSTM 一样，通过门控机制可以有效地缓解梯度消失问题。
- 由于 GRU 的结构比 LSTM 简单，没有记忆单元状态，门的数量也较少，因此 GRU 的训练速度更快，**计算效率高**。
- 在许多任务中，GRU 的表现与 LSTM 相当，尤其**在较短的序列数据上，GRU 甚至表现得更好**。

### GRU 的局限性
- **灵活性稍低**：由于缺少独立的记忆单元状态，GRU 对非常长的依赖关系和复杂序列的处理能力可能不如 LSTM 强大。
- **统一的隐藏状态**：在 GRU 中，隐藏状态既负责短期信息的存储，也负责长期信息的传递。无法更好地分离短期和长期信息。


# GRU 与 LSTM 的对比
| 特性                    | GRU                                | LSTM                              |
|-------------------------|------------------------------------|-----------------------------------|
| **门控机制**             | 2 个门：更新门、重置门              | 3 个门：输入门、遗忘门、输出门     |
| **记忆状态**             | 没有单独的记忆单元状态，只有隐藏状态  | 有记忆单元状态和隐藏状态           |
| **结构复杂性**           | 结构较简单，参数较少                | 结构复杂，参数较多                 |
| **处理长依赖**           | 表现较好，能捕捉长期依赖             | 更适合处理复杂的长时间依赖          |
| **计算效率**             | 相对较高，训练时间更快              | 计算开销较大，训练时间较长         |

- **序列较短时**：在处理较短的序列时，GRU 的表现通常与 LSTM 相当甚至更好，由于 GRU 的结构更简单、计算效率更高，通常优先选择 GRU。
- **序列较长或依赖关系复杂时**：在处理较长序列或有复杂长期依赖关系的任务时，LSTM 的表现通常更优，因为 LSTM 能够更灵活地通过记忆单元状态存储和传递长期信息。
- **计算资源有限时**：如果对计算资源或训练时间有较高要求，GRU 是更好的选择，因为它参数更少、计算效率更高。
#  Bi-RNN
Bi-RNN 通过**引入双向信息流**，使网络能够同时考虑从左到右（前向）和从右到左（后向）的信息流动。计算每个时间步的输出时，**Bi-RNN 不仅考虑当前时间步之前的信息，还考虑之后的信息**，从而提升了模型对时间序列依赖的捕捉能力。

### Bi-RNN 的结构
Bi-RNN 的结构与传统 RNN 不同的是，它由**两个独立的 RNN 层**组成：一个是处理输入序列的**前向 RNN**，另一个是处理输入序列的**后向 RNN**。这两个 RNN 层分别处理序列的正向和反向信息流，然后将它们的输出进行组合，生成最终的输出。

### Bi-RNN 的工作原理
Bi-RNN 的基本工作流程如下：
1. **输入序列**：网络接收一个输入序列 \( $x_1, x_2, ..., x_T$ \)，并同时将序列传递给前向 RNN 和后向 RNN。
2. **前向 RNN**：前向 RNN 从序列的第一个时间步开始，逐步处理输入序列，并生成每个时间步的前向隐藏状态 \( $\overrightarrow{h_t}$ \)。
3. **后向 RNN**：后向 RNN 从序列的最后一个时间步开始，逐步反向处理输入序列，并生成每个时间步的后向隐藏状态 \( $\overleftarrow{h_t}$ \)。
4. **组合输出**：在每个时间步 \( $t$ \)，将前向隐藏状态和后向隐藏状态进行组合，生成最终的输出 \( $y_t$ \)。
5. **输出序列**：最终输出的序列 \( $y_1, y_2, ..., y_T$ \) 同时考虑了输入序列的前后信息。

### Bi-RNN 的特点
1. **前后信息的整合**：双向 RNN 能够同时利用序列的前后信息，因此在处理像语音、文本等需要上下文信息的任务中表现更好。
2. **适合全局依赖任务**：Bi-RNN 通过从两个方向处理数据，可以捕捉到更全局的依赖关系。
3. **双倍计算量**：由于需要处理两个方向的序列，因此计算复杂度比单向 RNN 高。

# RNN、LSTM、GRU 和 Bi-RNN 总结

| **特性**                | **RNN**                                  | **LSTM**                                | **GRU**                                | **Bi-RNN**                                     |
|-------------------------|------------------------------------------|-----------------------------------------|----------------------------------------|------------------------------------------------|
| **结构**                | 单向循环结构，只有隐藏状态                | 有记忆单元状态和隐藏状态，三门控制信息流 | 只有隐藏状态，合并了输入门和遗忘门      | 双向结构，有前向和后向两个独立的 RNN            |
| **记忆能力**            | 适合处理短期依赖，长序列时有梯度消失问题  | 能有效处理长时依赖，解决梯度消失问题    | 简化的版本，参数更少，能处理长短期依赖  | 能同时捕捉序列的前后信息，适合上下文相关任务    |
| **门机制**              | 无门控机制                               | 有输入门、遗忘门和输出门                 | 有更新门和重置门，控制信息的保留和更新  | 没有门机制，双向结构通过前向和后向的组合增强    |
| **计算复杂度**          | 低，参数少，计算速度快                    | 复杂度较高，三门结构增加计算量           | 相对简单，参数和计算量比 LSTM 少         | 计算量大，需两次遍历输入序列，较高开销          |
| **优点**                | 结构简单，适合短序列建模                 | 能捕捉长短期依赖，解决梯度消失问题       | 训练效率高，能捕捉长短期依赖            | 能同时利用历史和未来信息，捕捉前后文依赖       |
| **缺点**                | 难以捕捉长时间依赖，容易梯度消失或爆炸    | 训练时间较长，计算资源需求大             | 虽然简化了结构，但对复杂依赖建模较弱     | 需要双向处理，实时性差，计算开销大              |
| **适用场景**            | 短期时间序列预测、简单的文本处理任务      | 长时间序列任务，如机器翻译、文本生成     | 适合资源受限、需要较快训练的任务        | 自然语言处理、语音识别、视频分析等上下文任务   |
| **应用示例**            | 基本序列预测、时间序列分类                | 机器翻译、语言模型、文本分类、对话系统   | 语音识别、时间序列预测、情感分析        | 命名实体识别、语义角色标注、情感分类、语音识别 |


- **RNN**：适合处理短期依赖的简单任务，结构较为简单，但在处理长序列时容易出现梯度消失问题。
- **LSTM**：通过记忆单元和门控机制解决了梯度消失问题，擅长处理长时依赖任务，适合复杂的时间序列建模。
- **GRU**：LSTM 的简化版本，具有较少的参数和更高的计算效率，适合在需要较快训练的任务中使用，性能上通常与 LSTM 相当。
- **Bi-RNN**：双向结构能够同时捕捉序列中的前向和后向信息，非常适合自然语言处理和语音处理等需要利用上下文信息的任务。

这些网络结构都是处理序列数据的基础工具，选择哪一种网络通常取决于任务的复杂性和计算资源。
# 全连接层在各神经网络模型中的作用

全连接层（Fully Connected Layer, FC Layer）广泛应用于分类任务或回归任务的**最后阶段**。全连接层的主要作用是**将上层提取的特征转换为具体的决策结果或输出**。它在不同类型的神经网络模型中具有不同的作用。以下是全连接层在各类神经网络模型中的作用详细解释：

1. 在**MLP**中，全连接层是主要计算单元，**完成输入到输出的映射**。
2. 在**CNN**中，全连接层主要用于**整合局部特征并生成分类结果**。
3. 在**RNN**和**LSTM**中，全连接层**将时间序列特征映射为输出结果**。
4. 在**Transformer**模型中，全连接层**参与注意力机制和输出映射**。
5. 在**GAN**中，全连接层**用于潜在空间和图像特征之间的映射**。
6. 在**自编码器**中，全连接层**用于特征压缩和重构**。
7. 在**注意力机制**中，全连接层用于**计算注意力得分并变换上下文向量**。

无论在哪种神经网络中，全连接层的核心作用都是将前一层的特征进一步映射到目标空间，形成最后的输出或决策。



# 历史文章

## 机器学习

[机器学习笔记——损失函数、代价函数和KL散度](https://blog.csdn.net/haopinglianlian/article/details/143831958?)
[机器学习笔记——特征工程、正则化、强化学习](https://blog.csdn.net/haopinglianlian/article/details/143832118?)
[机器学习笔记——30种常见机器学习算法简要汇总](https://blog.csdn.net/haopinglianlian/article/details/143832321)
[机器学习笔记——感知机、多层感知机(MLP)、支持向量机(SVM)](https://blog.csdn.net/haopinglianlian/article/details/143832552)
[机器学习笔记——KNN（K-Nearest Neighbors，K 近邻算法）](https://blog.csdn.net/haopinglianlian/article/details/143832692)
[机器学习笔记——朴素贝叶斯算法](https://blog.csdn.net/haopinglianlian/article/details/143832781?)
[机器学习笔记——决策树](https://blog.csdn.net/haopinglianlian/article/details/143834363)
[机器学习笔记——集成学习、Bagging（随机森林）、Boosting（AdaBoost、GBDT、XGBoost、LightGBM）、Stacking](https://blog.csdn.net/haopinglianlian/article/details/143834494?)
[机器学习笔记——Boosting中常用算法（GBDT、XGBoost、LightGBM）迭代路径](https://blog.csdn.net/haopinglianlian/article/details/143834628)
[机器学习笔记——聚类算法（Kmeans、GMM-使用EM优化）](https://blog.csdn.net/haopinglianlian/article/details/143834707)
[机器学习笔记——降维](https://blog.csdn.net/haopinglianlian/article/details/143834847)

## 深度学习
[深度学习笔记——优化算法、激活函数](https://blog.csdn.net/haopinglianlian/article/details/143835137)
[深度学习——归一化、正则化](https://blog.csdn.net/haopinglianlian/article/details/143835273)
[深度学习笔记——前向传播与反向传播、神经网络（前馈神经网络与反馈神经网络）、常见算法概要汇总](https://blog.csdn.net/haopinglianlian/article/details/143835406)
[深度学习笔记——卷积神经网络CNN](https://blog.csdn.net/haopinglianlian/article/details/143841327)



