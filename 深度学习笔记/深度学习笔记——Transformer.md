
> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本文详细介绍面试过程中可能遇到的Transformer知识点。


![在这里插入图片描述](https://github.com/GoodnoteX/Ai_Interview/blob/main/深度学习笔记/image/2.png)



> @[toc]

# 初识Transformer 
Transformer 是一种革命性的神经网络架构，最早由 Vaswani 等人在 2017 年提出，它**彻底改变了自然语言处理（NLP）的领域**。与传统的递归神经网络（RNN）和长短期记忆网络（LSTM）不同，Transformer**不依赖于序列的时间步** ，而是通过**注意力机制  并行处理**整个输入序列。它在诸如机器翻译、文本生成和语言模型等任务中表现出色。下面详细介绍 Transformer 的原理、结构和应用。

## 1. 编码器-解码器架构
Transformer 的架构由两个部分组成：
- **编码器（Encoder）**：将**输入序列** -> (映射) -> **连续的表示空间**。
- **解码器（Decoder）**：根据**编码器的输出** ->**生成目标序列**。

编码器和解码器**均由多个相同的层堆叠**而成。**每层都有两个主要组件**：
- **多头自注意力机制（Multi-Head Self-Attention Mechanism）**
- **前馈神经网络（Feed-Forward Neural Network）**。

### 解码器的额外结构
解码器除了与编码器相同的结构外，还多了一个**编码器-解码器注意力机制**，它在生成每个目标时**依赖于编码器的输出**。

## 2. 自注意力机制（Self-Attention Mechanism）
Transformer 的核心是自注意力机制，它能够**捕捉输入序列中的全局依赖关系**，并且**不需要像 RNN 那样逐步计算**。

**自注意力的计算过程：**
给定输入序列 $X=(x_1,x_2,\ldots,x_n)$，每个元素 $x_i$ 会生成三个向量：查询（Query）、键（Key）和值（Value）。这些向量通过三个不同的权重矩阵 $W_Q$、$W_K$、$W_V$ 得到：

$$Q = XW_Q,\quad K = XW_K,\quad V = XW_V$$

其中，$Q$、$K$ 和 $V$ 分别代表查询、键和值。

接下来，计算查询和键之间的相似性得出注意力分数。具体公式为：

$$ \text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

这里 $d_k$ 是键向量的维度，$\sqrt{d_k}$ 是缩放因子，用于避免在点积中值过大而导致梯度爆炸。

**多头注意力机制（Multi - Head Attention）**
Transformer进一步引入了多头注意力机制，它不是一次计算一个查询 - 键值对的注意力，而是使用多个不同的权重矩阵并行计算多个“头”的注意力。每个头可以关注输入序列中的不同部分，最后将这些不同头的结果拼接起来并通过线性变换得到最终的输出：

$$\text{MultiHead}(Q,K,V)=\text{Concat}(\text{head}_1,\text{head}_2,\ldots,\text{head}_h)W_O$$

其中 $W_O$ 是线性变换的权重矩阵。多头注意力机制的主要优势在于它能从不同的子空间中同时捕捉输入序列的不同特征，从而提高模型的表现。
### 解码器中的注意力机制的2点特殊
1. **编码器-解码器注意力机制**：解码器中，不仅包含自注意力机制，还包括一个**与编码器输出交互**的注意力机制，它允许**解码器在生成每个词时参考编码器的上下文信息**。
2. **未来遮蔽**：**确保在生成第 t 个词时**，只能看到之前生成的词，而**看不到未来的词**。

## 3. 位置编码（Positional Encoding）
因为 Transformer **不依赖输入序列的顺序信息**，它需要**显式地将位置信息引入模型中**。为此，Transformer 使用位置编码（Positional Encoding）来为每个输入向量添加位置信息。
位置编码通过**正弦和余弦函数生成**，并按照序列的索引位置注入到每个词向量中：

$$ PE_{(pos,2i)}=\sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right) $$
$$ PE_{(pos,2i + 1)}=\cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right) $$


其中 \( $pos$ \) 是词在序列中的位置，\( $i$ \) 是词向量的维度索引，\( $d$ \) 是词向量的总维度。

## 4. 前馈神经网络（Feed-Forward Neural Network, FFN）
在每个编码器和解码器层中，除了自注意力机制外，还包括一个前馈神经网络。每个时间步上的前馈网络是**独立应用**的，它的公式如下：


$$ FFN(x)=\max(0,xW_1 + b_1)W_2 + b_2 $$

其中，$W_1$ 和 $W_2$ 是权重矩阵，$b_1$ 和 $b_2$ 是偏置项，$\text{ReLU}$ 是非线性激活函数。


## 5. 残差连接和层归一化（Residual Connections and Layer Normalization）
Transformer 的**每个子层**（如自注意力机制或前馈神经网络）**后**面都有一个残差连接（Residual Connection）和层归一化（Layer Normalization），确保在网络训练的过程中信息可以直接传递，**缓解深层网络中的梯度消失问题**。
残差连接的形式为：

$$ \text{Output}=\text{LayerNorm}(x+\text{SubLayer}(x)) $$

这里 $x$ 是输入，$\text{SubLayer}(x)$ 是注意力机制或前馈神经网络的输出。




## 6. Transformer 的应用
Transformer 由于其并行计算的优势，以及在处理长距离依赖关系上的优越表现，在许多任务中取得了卓越的成果：
- **机器翻译**：例如谷歌翻译。
- **文本生成**：例如 OpenAI 的 GPT 系列模型。
- **文本摘要**：自动生成文档摘要。
- **语言建模**：如 BERT、GPT 及其变体，用于各种 NLP 任务。
- **图像处理&多模态**：ViT、ViLT。

## 7. Transformer 的优势
- **并行处理**：**不依赖时间步顺序，可以并行处理整个序列**，因此训练速度更快，特别适合大规模数据集。
- **长距离依赖处理能力强**：**注意力机制，能够轻松处理序列中远距离的依赖关系**，而不像 RNN 或 LSTM 那样受限于序列的长度。
- **广泛的应用**：Transformer 已成为 NLP 的标准架构，并被扩展到其他领域，如图像处理（Vision Transformer, ViT）和语音处理等。

---
# 编码解码的过程
## 编码过程（Encoder）
主要任务是**将输入序列转换为一种捕捉全局上下文信息的表示**。这一过程确保输入序列中的**每个元素**（如文本中的单词或句子中的其他单位）都能够**有效地“理解”整个输入序列中的其他元素**，从而为后续任务（如机器翻译、文本生成等）提供丰富的上下文信息。
### 输入数据的准备（输入数据向量化+位置编码）

$$ x_{\text{final}}=x_{\text{embedding}}+PE_{\text{position}} $$


其中$x_{embedding}$是输入数据向量化；$PE_{position}$是位置编码，具体实现如下：


**输入数据的准备**
- **输入序列**：假设我们要处理一个输入序列（例如，一个句子的单词序列），每个单词 $x_1,x_2,\ldots,x_n$ 都被表示为向量（通常通过嵌入矩阵映射得到）。
- **输入嵌入（Input Embedding）**：首先，通过嵌入层将每个单词映射为一个固定维度的嵌入向量。嵌入可以理解为字典，用来将离散的单词或符号映射为连续向量空间中的表示。


$$ \text{Embedding}(x_i)=W_{\text{embedding}}x_i $$

其中，$W_{\text{embedding}}$ 是嵌入矩阵。
- **位置编码（Positional Encoding）**
由于Transformer结构本身不保留输入序列的顺序信息（不像RNN那样逐步处理输入），需要通过位置编码显式引入序列的顺序。位置编码是通过正弦和余弦函数生成的，加入到每个输入嵌入向量中：

$$ PE_{(pos,2i)}=\sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right),\quad PE_{(pos,2i + 1)}=\cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right) $$

其中 $pos$ 是输入序列中位置，$d$ 是嵌入向量的维度。


### 自注意力机制（循环计算多次——多头）


**自注意力机制（Self - Attention Mechanism）**
接下来，输入序列被送入自注意力机制。自注意力机制允许输入序列中的每个单词可以“关注”到其他单词，从而捕捉全局上下文信息。
- **查询、键和值向量（Q,K,V）**：每个输入嵌入向量通过三个不同的权重矩阵变换为查询（Query）、键（Key）和值（Value）向量。
- 
$$ Q = XW_Q,\quad K = XW_K,\quad V = XW_V $$

其中，$X$ 是输入嵌入，$W_Q$、$W_K$、$W_V$ 是权重矩阵。
- **注意力得分计算**：查询和键的点积表示单词之间的相似性。点积的结果经过缩放并通过softmax函数，生成每个单词对其他单词的注意力权重：
- 
$$ \text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

这里 $d_k$ 是键向量的维度。注意力机制将这些权重用于加权求和，以生成每个单词的新的上下文表示。

**多头自注意力机制（Multi - Head Attention）**
为了让模型能从不同的子空间中关注不同部分，Transformer引入了多头注意力机制，它并行计算多个注意力头，每个头有独立的查询、键和矩阵。不同的头关注序列中的不同信息，最后将结果拼接并线性变换：

$$ \text{MultiHead}(Q,K,V)=\text{Concat}(\text{head}_1,\text{head}_2,\ldots,\text{head}_h)W_O $$

这里 $W_O$ 是线性变换矩阵，$h$ 是注意力头的数量。


### 前馈神经网络
在每个编码器层的注意力机制之后，接着使用一个前馈神经网络，它对每个位置上的向量独立进行非线性变换：

$$ FFN(x)=\max(0,xW_1 + b_1)W_2 + b_2 $$
这是一个两层的全连接网络，使用 ReLU 作为激活函数。
### 残差连接和归一化
每一层的注意力机制和前馈神经网络的输出都会经过残差连接 (Residual Connection) 与输入相加，并通过层归一化 (Layer Normalization)：

$$ \text{Output} = \text{LayerNorm}(x+\text{SubLayer}(x)) $$


这样可以**缓解梯度消失问题**，加速训练。
编码器通常由多层(例如 6 层)堆叠组成。最终，编码器的输出是一个包含全局上下文信息的序列表示，作为解码器的输入。
## 解码过程（Decoder）
解码过程是**自回归**的，目的是**根据编码器生成的上下文信息，以及已经生成的部分目标序列，逐步生成完整的输出序列**（例如，翻译后的句子）。解码器的每一层与编码器类似，但有几个额外的步骤。
> 自回归模型：Transformer 的解码器是自回归的，这意味着解码器会**根据已经生成的输出**（即部分目标序列）**来生成下一个时间步的输出**。
### 输入嵌入和位置编码
解码器首先将**已经生成的目标序列进行嵌入**，并**加入位置编码**，确保模型感知目标序列的顺序。
### 计算自注意力时加入遮蔽机制（Masked Self-Attention Mechanism）
在解码过程中，**使用遮蔽自注意力机制**。通过**对未来词进行掩蔽，确保在生成某个位置上的词时，解码器只能看到之前生成的词，而不能看到未来的词。**
这个步骤**确保**生成过程是按**顺序**进行的。
### 增加编码器-解码器注意力机制（Encoder-Decoder Attention）
在每一层中，解码器还使用编码器-解码器注意力机制。这个机制使**解码器在生成每个目标词时参考编码器提供的上下文信息，帮助生成更合适的输出**。

**编码器-解码器注意力与自注意力的区别在于**：
- **作用不同**：
  - **自注意力机制（Self-Attention Mechanism）**：作用于**单一序列**（即源序列或目标序列），用于**捕捉序列内部元素之间**的上下文信息。
  - **编码器-解码器注意力机制（Encoder-Decoder Attention Mechanism）**：作用于**两个序列之间**（源序列和目标序列），用于**将编码器的输出信息*传递到* 解码器中**。解码器的每个位置会基于编码器输出的特征来生成当前步的输出，从而实现对源序列和目标序列的关联建模。
- **输入不同**：
  - **自注意力（Self-Attention）**：
    - 查询 \( Q \)、键 \( K \)、值 \( V \) 都来自**同一个输入序列**
  - **编码器-解码器注意力（Encoder-Decoder Attention）**：
     - 查询 \( Q \) 来自**解码器**；
     - 键 \( K \) 和值 \( V \) 来自**编码器的输出序列**。

### 前馈神经网络
类似于编码器的每一层，解码器每层的输出也会通过一个前馈神经网络进行非线性变换。
### 残差连接和层归一化
与编码器相同，解码器也使用残差连接和层归一化来稳定训练过程。
### 输出生成
最后一层解码器的输出经过一个**线性层和 softmax 函数，用来生成目标词的概率分布**。在每个时间步，解码器根据概率分布选择下一个词，逐步生成整个目标序列。
## 整体编码解码流程总结
1. **编码过程**：编码器通过**多头自注意力机制和前馈神经网络**，将输入序列中的**每个位置的单词转化为一个上下文相关的表示**。经过**多层堆叠的编码器**，生成**输入序列的全局表示**，供解码器使用。
2. **解码过程**：解码器首先通过**遮蔽自注意力机制生成目标序列中的下一个词**，然后使用**编码器-解码器注意力机制**获取编码器输出的信息，结合目标序列的已有部分，**生成新的词**。**解码器通过循环逐步生成整个目标序列。**

这个过程确保模型能够捕捉输入序列的全局信息，并在生成目标序列时做到合理且流畅的生成。
# 训练、推理过程的不同
| 步骤               | **训练流程**                                        | **推理流程**                                  |
|--------------------|-----------------------------------------------------|------------------------------------------------|
| **输入准备**        | 有**完整的源序列和目标序列**作为输入                     | **只有源序列作为输入** （目标序列是空）                            |
| **解码器输入**      | 使用目标序列的前 \( m-1 \) 个词作为解码器输入（**强制教学**）| **自回归生成**，每步使用解码器生成的词作为下一步输入|
| **生成机制**        | 每步根据目标序列计算输出，并与真实词对比计算损失       | 逐步生成输出，依赖之前生成的词作为输入         |
| **损失计算**        | 计算每个时间步与真实词之间的损失                      | 无损失计算，只预测输出                         |
| **反向传播**        | 使用反向传播计算梯度，并更新模型参数                   | 不涉及反向传播，只进行前向传播和输出预测       |
# Transformer 推理的步骤




> 详细全文请移步公主号：Goodnote。  
参考：[欢迎来到好评笔记（Goodnote）！](https://mp.weixin.qq.com/s/lCcceUHTrM7wOjnxkfrFsQ)


