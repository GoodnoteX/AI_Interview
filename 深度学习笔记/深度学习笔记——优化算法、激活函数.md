
> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本笔记介绍深度学习中常见的优化算法、激活函数。

![在这里插入图片描述](https://github.com/GoodnoteX/Ai_Interview/blob/main/深度学习笔记/image/1.png)


---
@[toc]
# 优化算法
在深度学习中，优化算法用于**调整模型的参数**（如权重和偏置）以**最小化损失函数**。
## 方法
### 梯度下降 (Gradient Descent, GD)
**简介**：  
梯度下降是最基本的优化算法，通过**计算损失函数**相对于模型参数的**梯度**，**沿着梯度下降的方向更新参数**，以最小化损失函数。
- **批量梯度下降（Batch Gradient Descent）**：在每次迭代中**使用整个训练集**计算梯度，计算开销大但收敛稳定。
- **随机梯度下降（Stochastic Gradient Descent, SGD）**：每次迭代**只使用一个样本**计算梯度，效率高，但容易在训练中产生波动。
- **小批量梯度下降（Mini-batch Gradient Descent）**：在每次迭代中**使用一小部分训练集**进行梯度计算，平衡了批量梯度下降和随机梯度下降的优缺点。


**公式：**
$$\theta_{t + 1}=\theta_{t}-\eta \nabla_{\theta} J(\theta_{t})$$

- 其中：
	- $\theta_{t}$表示当前模型的参数，
	- $\eta$是学习率（也称为步长），
	- $\nabla_{\theta} J(\theta_{t})$是参数$\theta_{t}$对于损失函数$J(\theta_{t})$的梯度。


**优点**：
1. **简单直观**：梯度下降的原理简单，计算损失函数的梯度，沿着梯度最速下降方向进行优化。
   
2. **广泛适用**：梯度下降可以应用于各种类型的机器学习问题，包括线性回归、逻辑回归和神经网络等。在许多优化问题中，梯度下降是基本的选择，适用范围广泛。

3. **可扩展性强**：梯度下降可以在不同的规模和复杂度下使用。通过**调整批量大小**（如批量梯度下降、随机梯度下降、小批量梯度下降），可以适应不同数据集的规模。

4. **良好的收敛性**：对于**凸优化问题**，梯度下降可以保证收敛到**全局最优解**。而在**非凸问题**中，梯度下降也能找到**局部最优解**，具有良好的泛化能力。

**缺点**：
1. **收敛速度慢**：梯度下降尤其是**批量梯度下降（Batch Gradient Descent）**，收敛速度慢。在大型数据集上，批量梯度下降的效率较低。

2. **容易陷入局部最优**：在**非凸优化问题**中，梯度下降容易陷入**局部最优解**。

3. **对学习率敏感**：梯度下降依赖于超参数**学习率(learning rate)** 的选择。如果学习率太小，收敛速度非常慢；如果学习率太大，梯度下降可能无法收敛，甚至在训练过程中发散。

4. **梯度消失或梯度爆炸问题**：在深层神经网络中，由于链式法则，梯度值会在反向传播时逐层累积，容易导致梯度消失或梯度爆炸问题，导致网络训练不稳定或无法更新参数。

**适用场景**：
- 适用于数据量较大的场景，如深度学习中的大规模数据训练。
  

### 动量法 (Momentum)
**简介**：  
动量法是对GD的改进，它在更新参数时不仅考虑**当前的梯度**还考虑**前几次的梯度**。这样**就像给参数加上“惯性”** ，从而**避免震荡**。

**公式：**

$$v_{t + 1}=\beta v_{t}+(1 - \beta)\nabla_{\theta} J(\theta_{t})$$
$$\theta_{t + 1}=\theta_{t}-\eta v_{t + 1}$$

- 其中：
	- $v_{t}$ 是动量，表示历史梯度的加权平均，
	- $\beta$ 是动量系数，通常设定为接近1（例如0.9），
	- $\eta$ 是学习率，控制更新步长。

**优点**：
- **更快的收敛速度**：由于动量的存在，它可以在损失函数的谷底处快速收敛。
- **减少震荡**：动量法在梯度方向上能够更稳健，减少梯度震荡现象。

**缺点**：
- **需要调节动量系数**：动量的超参数（通常设为0.9左右）需要精心调节，使用不当可能导致训练失效。

**适用场景**：
- 适用于数据噪声较多或梯度震荡较大的场景，如深度神经网络中使用的卷积层。

---

### AdaGrad (Adaptive Gradient Algorithm)
**简介**：  
AdaGrad是一种**自适应学习率**的方法。它为**每个参数独立调整学习率**，**学习率的调整取决于历史梯度的平方和**，这使得**频繁更新的参数学习率逐渐减小，而较少更新的参数学习率保持较大**。这种方式**防止**了**步长过大导致的震荡**，也**避免**了**步长过小导致的收敛速度慢**。

### 总结

| **算法**   | **原理**                                                                                      | **优点**                                       | **缺点**                                    | **适用场景**                          |
|------------|-----------------------------------------------------------------------------------------------|------------------------------------------------|---------------------------------------------|---------------------------------------|
| **GD**    | 梯度下降算法，通过每次使用**整个训练集**，	或者**SGD和MGD**。  **计算损失函数相对于模型参数的梯度**，**沿着梯度下降**的方向更新参数，以最小化损失函数。   | 理论稳定，易于实现	                   | 计算成本高，特别是在大数据集上更新缓慢	                           | 小规模数据集或批量处理场景             |
| **Momentum**| 在 SGD 基础上增加**动量**，不仅考虑当前的梯度，还利用**过去的梯度积累**，加入**惯性**来加速收敛并**减少震荡**。                              | 提升收敛速度、减少震荡                        | 需调节动量系数                               | 数据噪声较多的场景                   |
| **AdaGrad**| 对**每个参数**采用**不同的学习率**，**自适应学习率**，并**根据历史梯度的平方和自动调整每个参数的学习率**。                        | 自适应学习率，适合稀疏数据                     | **学习率快速衰减**                              | **稀疏特征**数据场景                     |
| **RMSProp**| 在 **AdaGrad 基础上**，通过**引入梯度的二阶矩**来平衡学习率的更新，**解决**了 AdaGrad **学习率快速衰减的问题**。     | 学习率调整更稳健，解决 AdaGrad 衰减问题        | 需调节超参数                                | 非平稳目标、强化学习                 |
| **Adam**   | 结合了 **Momentum 和 RMSProp**，采用**自适应学习率和动量**来更新参数，适应性强，收敛速度快。  利用了梯度的**一阶矩**（动量），又利用了梯度的**二阶矩**（RMSprop）。            | 收敛快，适应性强，鲁棒性好                     | 可能过早收敛，需调整多个超参数               | 大多数深度学习任务，图像分类、语言模型 |
| **AdamW**  | 改进的 Adam 算法，结合**权重衰减正则化**，将**权重衰减**与**梯度更新**过程**分离**，解决过拟合。           | 改进的正则化，避免权重衰减导致的过拟合问题     | 增加超参数调试复杂度                        | 正则化场景，如图像分类、NLP任务       |

1. **GD（Gradient Descent，梯度下降）**：每次迭代使用整个数据集计算梯度，并更新模型参数，确保模型稳步收敛，但在大数据集上计算成本高且更新缓慢。
2. **SGD（Stochastic Gradient Descent）**：每次参数更新只使用**一个或小批量**样本进行计算，虽然计算效率高，但更新不稳定，容易出现震荡。
3. **Momentum**：通过在每次梯度更新时引入**动量**，动量会**积累之前梯度的方向**，帮助加速收敛，减少震荡。适合噪声较多的数据。
4. **AdaGrad**：**自适应调整学习率**，梯度较大的参数学习率变小，适合处理稀疏数据，但学习率在训练后期可能过小，导致收敛缓慢。
5. **RMSProp**：在 AdaGrad 基础上引入**滑动平均（梯度的二阶矩）的机制**，使学习率更新更加稳健，避免学习率过快衰减。常用于强化学习或非平稳目标。
6. **Adam**：**结合了 Momentum 和 RMSProp 的优势**，采用自适应学习率和动量机制，使其在大多数深度学习任务中表现良好，收敛快且鲁棒性好。
7. **AdamW**：在 Adam 的基础上，**改进了权重衰减的方式**，通过权重衰减的正则化防止模型过拟合，特别适用于需要正则化的任务，如图像分类和 NLP。




> 详细全文请移步公主号：Goodnote。  
参考：[欢迎来到好评笔记（Goodnote）！](https://mp.weixin.qq.com/s/lCcceUHTrM7wOjnxkfrFsQ)
