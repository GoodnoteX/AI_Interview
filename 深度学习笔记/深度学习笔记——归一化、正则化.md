


> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本笔记介绍深度学习中常见的归一化、正则化。

![在这里插入图片描述](https://github.com/GoodnoteX/Ai_Interview/blob/main/深度学习笔记/image/2.png)


@[toc]

# 各种优化的归一化介绍（本质上进行标准化）
下面的归一化。本质上进行的是标准化，与普通的归一化不同。
**普通归一化**（例如  BN、LN等）通常**放在全连接层或卷积层之后，并在激活函数之前**。
**权重归一化**直接应用于层的权重参数，因此它通常**在层的定义阶段就应用**。例如，在**卷积层或全连接层的权重初始化**或定义时。

## 普通归一化过程

归一化方法的统一步骤如下：

---
### 1. 确定归一化范围
- **批归一化（Batch Normalization）**  
- **层归一化（Layer Normalization）**  
- **实例归一化（Instance Normalization）**  
- **组归一化（Group Normalization）**
### 2. 计算均值和方差
- 对归一化范围内的元素计算均值 $\mu$ 和方差 $\sigma^2$。  
- 公式如下：  

  $$\mu = \frac{1}{N} \sum_{i=1}^{N} x_i$$  
  $$\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2$$  
  - 其中 $N$ 是归一化范围内的元素总数。


### 3. 标准化
- 将输入值进行标准化，使其具有均值 0 和方差 1：  
  $$\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}$$  
  - $\epsilon$ 是一个小常数，用于避免分母为零。

---

### 4. 缩放和平移
- 为了让模型在归一化后仍能保持灵活性，引入可学习的缩放参数 $\gamma$ 和平移参数 $\beta$：  
  $$y_i = \gamma \hat{x}_i + \beta$$  
  - 通过学习 $\gamma$ 和 $\beta$，模型可以调整归一化后的输出尺度和偏移，使其更加适应模型的需求。
## 普通归一化分类
### 1. 批归一化（Batch Normalization, BN）
#### 原理
对**一个批量中的样本**的**同一通道**进行归一化。它将每个神经元的输出值转换为**均值为0、方差为1的分布**，随后**再进行缩放和平移**。批归一化的公式如：

$$\hat{x}=\frac{x-\mu_{\text{batch}}}{\sqrt{\sigma_{\text{batch}}^{2}+\epsilon}}$$
其中 $\mu_{\text{batch}}$ 和 $\sigma_{\text{batch}}$ 是当前批次中的均值和方差，$\epsilon$ 是一个小常数，防止除零错误。 
#### 优点
- 减少了内部协变量偏移问题，使得模型更稳定。
- 加速收敛，允许使用更高的学习率。
- **有轻微的正则化效果，因为批次间的随机性类似于dropout的效果**。
#### 缺点
- 对小批量数据敏感。当批次较小时，均值和方差估计不准，导致效果下降。
- 在某些情况下（如RNN、Transformer中的序列任务），批次归一化的效果不如其他归一化方法。
- 训练和推理时行为不同：推理时使用全局的均值和方差，因此需要额外的记录和计算。
#### 使用场景
- 适用于大多数卷积神经网络（CNN）和全连接网络（MLP），如图像分类、物体检测等任务中。
- 不太适合处理序列任务，如RNN和LSTM。

### 2. 层归一化（Layer Normalization, LN）
#### 原理
对**单个样本的所有通道**进行归一化，不依赖批量。

- 公式类似于批归一化，但均值和方差是基于整个层（而不是批次）计算的。
$$\hat{x}=\frac{x-\mu_{\text{layer}}}{\sqrt{\sigma_{\text{layer}}^{2}+\epsilon}}$$



## 归一化方法对比总结
以下是归一化方法对比总结，其中加入了每种归一化方法的原理：

| 归一化方法         | 原理                                                         | 适用场景                  | 优点                            | 缺点                              |
|--------------------|--------------------------------------------------------------|---------------------------|---------------------------------|-----------------------------------|
| **批归一化（BN）**  | 对**一个批量**中的所有样本的**同一通道**进行归一化，基于批次的均值和方差调整  | 卷积网络、全连接网络       | 加快收敛，正则化，适应大批量训练 | 对小批次敏感，序列任务效果差      |
| **层归一化（LN）**  | 对**单个样本的所有通道**进行归一化，不依赖批量，计算层内均值和方差  | RNN、Transformer、序列任务 | 适应小批次训练，不依赖批次大小   | 计算量较大，收敛可能稍慢          |
| **实例归一化（IN）**| 对**单张图像的每个通道**分别独立进行归一化，计算每个样本的通道内均值和方差 | 图像生成、风格迁移         | 对风格敏感，适用于生成任务        | 不适合分类任务，无法捕捉全局信息   |
| **组归一化（GN）**  | 将**单个样本的特征通道分组**，对每一组进行归一化，计算组内均值和方差       | 小批次训练，卷积网络       | 适合小批次，不依赖批次大小       | 对卷积核大小和通道数较敏感         |
| **权重归一化（WN）**| 对神经元的**权重**向量**进行归一化**，将**方向和长度分开重新参数化**     | 卷积网络、全连接网络、生成模型 | 加速收敛，提高稳定性             | 效果不一定显著，某些任务中不如BN  |

> 注意，虽然他们是叫做归一化（批归一化、层归一化、实例归一化），是将多个输入特征**归一化为均值为 0、方差为 1 的分布**，使得网络的各层输入保持在较为稳定的范围内。**本质上是进行标准化。再进行引入两个可学习参数 γ 和 𝛽，分别表示缩放和平移操作。**
> 
> BN、LN、IN、GN 等归一化方法都**包含了标准化**的步骤，即它们都会将激活值调整为均值为 0、方差为 1 的分布，关键区别在于这些方法在不同的范围内计算均值和方差，以适应不同的训练场景和模型结构：





> 详细全文请移步公主号：Goodnote。  
参考：[欢迎来到好评笔记（Goodnote）！](https://mp.weixin.qq.com/s/lCcceUHTrM7wOjnxkfrFsQ)


