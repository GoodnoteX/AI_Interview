> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本文详细介绍了三种最火热的基于Transformer架构的NLP模型：GPT、BERT和T5。
> 

![在这里插入图片描述](https://github.com/GoodnoteX/Ai_Interview/blob/main/深度学习笔记/image/9.png)


> @[toc]

GPT（Generative Pre-trained Transformer）、BERT（Bidirectional Encoder Representations from Transformers）和T5（Text-To-Text Transfer Transformer）都是**基于Transformer架构**的自然语言处理模型，但它们在结构、训练方式和应用场景上有显著的区别。

# 一、GPT
GPT的全称是“**生成式预训练模型**”，其主要设计思想是通过大规模的语言建模任务进行预训练，并通过微调来解决下游的具体任务。GPT的结构特点如下：

1. **Transformer解码器架构**：
   - GPT使用的是Transformer的**解码器（Decoder）** 部分。这个架构主要由自注意力机制（self-attention）和前馈神经网络（Feedforward Neural Networks）组成。解码器可以处理序列数据，生成新的序列，因此适合生成任务，如文本生成。
   
2. **单向（Autoregressive）语言模型**：
   - GPT是**单向**模型，**即只能从左到右生成词语**。输入的每个词只能依赖于前面的词，不能看后面的词。这种设计使得它在处理语言生成任务时很有效，但在理解句子整体时略显不足。
   
3. **预训练任务**：
   - 预训练阶段，GPT通过**自回归的语言建模任务**进行训练，**目标是预测句子中下一个词**。然后，针对不同任务（如文本分类、问答等）进行微调。

4. **适合生成任务**：
   - GPT强大的生成能力，自回归的生成文本，每次生成一个词并将其作为输入，再生成下一个词。这种方式使它特别适合生成连贯的段落和长文本。

### 为何采用单向Transformer？
使用的是**解码器**，**自回归生成模式**。
使用了**Masked Self-Attention**(所谓Masked，即遮蔽的意思)，即句子中的每个词，都**只能对包括自己在内的前面所有词进行Attention**，这就是单向Transformer。


## 基于Transformer的优化
除了仅使用解码器堆叠的架构、未来遮蔽（自回归语言模型）外，还做了下面的优化。

### 层归一化提前
   - 在标准 Transformer 中，层归一化（Layer Normalization）通常放在残差连接和前馈网络之后，而 GPT 将层归一化放在残差连接之前，称为 **Pre-Layer Normalization**。
   - 这种调整有助于在深层网络中保持梯度的稳定性，并减轻训练深度模型时梯度消失的问题。

### RoPE旋转位置编码
   - GPT 最初使用的绝对位置编码（如 GPT-2 和 GPT-3 中使用的简单绝对位置编码），后续的 GPT-4 等新版本采用了 **混合位置编码** 或 **旋转位置编码** 等技术，使得位置编码对更长序列具有更好的泛化性。
   - 这些改进帮助模型在长文本和复杂结构的文本上表现更好。

RoPE旋转位置编码参考：[深度学习笔记——常见的Transformer位置编码](https://blog.csdn.net/haopinglianlian/article/details/144021458)
### 后续改进：指令微调和人类反馈强化学习（RLHF）
   - 在 GPT-3.5 和 GPT-4 中，引入了 **指令微调** 和 **基于人类反馈的强化学习（RLHF）**。通过在人类指令和偏好的数据上微调模型，使其在对话、问答等任务中更符合用户意图。
   - RLHF 训练方法提高了生成文本的自然性和用户满意度，使得模型在提供建议和解释复杂问题时更加精确。

# 二、BERT
BERT全称是“Transformers的双向编码表示”，它是为了**提升语言理解任务**（如文本分类、问答等）的效果而设计的。BERT的结构特点如下：

1. **Transformer编码器架构**：
   - BERT使用的是Transformer的**编码器（Encoder）**部分。与解码器不同，编码器主要用于表示输入序列中的每个词，并关注该词与序列中其他**词之间的关系**。

2. **双向（Bidirectional）语言模型**：
   - BERT是一种**双向**模型，即在处理某个词时，既可以考虑到该词左边的词，也可以考虑到右边的词。通过这种双向机制，BERT在理解句子的**全局语义上有显著优势**。

3. **预训练任务**：
   - BERT的预训练主要基于两项任务：
     - **Masked Language Model (MLM)**：随机遮盖输入序列中的部分词语，然后**让模型预测被遮盖的词**。这使得模型在训练过程中学会了利用上下文信息。
     - **Next Sentence Prediction (NSP)**：**预测两句话是否相邻**。这个任务增强了模型理解句间关系的能力。

4. **适合理解任务**：
   - 由于BERT的双向特性，它在处理语言理解任务时表现极为出色，如文本分类、问答系统、命名实体识别等。

## 预训练任务
**MLM（Masked Language Model）** 和 **NSP（Next Sentence Prediction）** 是 BERT（Bidirectional Encoder Representations from Transformers）模型的两种预训练任务。它们的设计目的是**帮助BERT 更好地理解语言结构和语义关系**，尤其是在下游自然语言处理任务（如分类、问答、命名实体识别等）中增强模型的表现。下面分别介绍这两个任务的原理和目的。
## 1. MLM（Masked Language Model）——掩码语言模型
### 1.1 原理
MLM 通过**随机掩盖**输入序列中的一些词，并要求模型通过上下文信息预测这些被掩盖的词，来**帮助模型学习上下文中的词语关系**。
### 1.2 步骤
1. **输入序列**：BERT 接收输入序列，如一个句子或者一段文本。
2. **随机掩盖部分词汇**：BERT 随机选择输入序列中**15%** 的词，并将它们替换为特殊的 [MASK] 标记。
3. **模型预测被掩盖的词**：模型的目标是**基于上下文**（即未被掩盖的其他词），预测被 [MASK] 掩盖的词。BERT 通过**最大化被掩盖词的正确预测概率**来进行训练。
### 1.3 掩码的机制
被**掩盖的词**中：
- **80%**：用 [MASK] 代替这个词。
- **10%**：随机替换为词汇表中的其他词。
- **10%**：保持词语不变。

这种机制是为了让模型不仅能学会对 [MASK] 标记进行预测，还能在下游任务中更好地**处理实际出现的词**，因为在推理阶段不会有 [MASK] 标记。
### 1.4 MLM 的作用
- **双向上下文学习**：通过预测被掩盖的词，BERT 能够同时利用词的左侧和右侧上下文进行训练，**学习语言的全局信息**。这使得 BERT 成为双向语言模型，能够捕捉到句子中的前后依赖关系。
- **语言理解增强**：MLM 帮助 BERT **学习丰富的词汇表示和句子结构**，使其在处理复杂的自然语言理解任务时具备更强的能力。
## 2. NSP（Next Sentence Prediction）——下一句预测
### 2.1 原理
NSP 目的是**帮助模型理解句子之间的关系**，尤其是在涉及多个句子上下文的任务中，如阅读理解、句子推理等。具体来说，NSP 任务要求模型**判断两个句子是否在原文中是连续的**。
### 2.2 步骤
1. **输入两句话**：在训练过程中，模型会被输入两个句子（句子 A 和句子 B）。
2. **判断句子关系**：
   - **50% 的情况下**：句子 B 是句子 A 的实际后续句子，即**这两个句子在原始文本中是连续的**。
   - **另外 50% 的情况下**：句子 B 是随机选择的一个句子，**两个句子没有关系**。
3. **模型预测**：BERT 的任务是判断句子 B 是否为句子 A 的真实后续句子。这一任务通过**在BERT的 [CLS] 标记位置添加一个分类层来完成**，输出是 "是" 或 "否" 的二分类问题。
### 2.3 NSP 的作用
- **学习句子间的语义关系**：通过 NSP 任务，BERT 能够理解句子之间的逻辑顺序和语义关联。这对于多句子的自然语言推理、文档级别的任务、问答等场景非常重要。
- **增强文本的全局理解**：NSP 不仅帮助模型理解单个句子的上下文，还让模型能够推理出跨句子的语义关系，从而在一些任务（如文本相似度、上下文依赖问答等）中表现更好。
### 2.4 NSP 的局限性与改进
- **改进版本（RoBERTa）**：在后来的研究中，例如 RoBERTa 模型中，研究者发现 NSP 任务对某些下游任务的提升有限，因此 RoBERTa 移除了 NSP 任务，只使用了更大规模的语料库和更长的训练时间，但仍然在许多任务中取得了更好的性能。
## 3. MLM 和 NSP 的结合
- **MLM 和 NSP 结合训练**：
  - 通过 MLM，学习了如何**从局部上下文中预测词语**，从而增强语言的理解能力；
  - 通过 NSP，学会了如何**理解句子之间的逻辑顺序和语义联系**，从而提升了其在跨句子任务上的表现。

- **MLM 和 NSP 的相互作用**：**MLM 关注单个句子的上下文**，而 **NSP 通过跨句子预测**使模型掌握了多句子理解的能力。它们共同作用，使得 BERT 在各类自然语言处理任务（如文本分类、问答系统、文本生成等）中表现优异。
## 4. MLM和NSP任务对比
| **任务**                 | **MLM（Masked Language Model）**                        | **NSP（Next Sentence Prediction）**                 |
|-------------------------|-------------------------------------------------------|---------------------------------------------------|
| **目的**                 | 预测被掩盖的词语，帮助模型**学习上下文中的词语关系**        | 判断两句话是否在原文中连续，**学习句子间的语义关系**   |
| **输入**                 | 一个被随机掩盖词语的句子                              | 两个句子 A 和 B                                   |
| **输出**                 | 预测掩盖词的正确词语                                  | 判断句子 B 是否是句子 A 的真实后续句               |
| **任务目标**             | 学习**句子内部**的词语依赖关系，增强词表示                 | 学习**句子之间**的语义联系，增强句子对的理解           |
| **对模型的作用**         | 强化模型对上下文的理解能力，**提高词表示的质量**            | **增强句子间关系的推理能力**，适合多句子任务     |
| **局限性与改进**         | 使用掩码词语导致预测任务偏离真实场景                    | 对部分任务贡献较小（如在 RoBERTa 中被移除）        |



> 详细全文请移步公主号：Goodnote。  
参考：[欢迎来到好评笔记（Goodnote）！](https://mp.weixin.qq.com/s/lCcceUHTrM7wOjnxkfrFsQ)



