> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本文简要介绍深度学习的前向传播与反向传播，以及前馈神经网络与反馈神经网络。

![在这里插入图片描述](https://github.com/GoodnoteX/Ai_Interview/blob/main/深度学习笔记/image/4.png)

---
@[toc]


# 前向传播与反向传播

## 前向传播（Forward Propagation）

前向传播是将输入数据从输入层依次通过神经网络的各个隐藏层，最后输出预测结果的过程。该过程**用于计算网络的预测结果**，以及在训练过程中**计算损失函数值**。它的基本步骤如下：

1. **输入数据**：将输入数据传递给神经网络的输入层。
2. **线性变换**：在每一层中，神经元接收前一层输出的线性组合（加权求和），计算公式为：
    $$z = w \cdot x + b$$
     - 其中，$w$是权重，$x$是输入，$b$是偏置。
3. **激活函数**：将线性组合$z$通过激活函数$f(z)$生成非线性的输出$a$，即：
    $$a = f(z)$$
     - 常用的激活函数有ReLU、Sigmoid、Tanh等。
4. **逐层传播**：将每一层的输出作为下一层的输入，依次进行线性变换和激活，直至输出层。
5. **生成输出**：
   - **多分类任务**：在多分类问题中，通常使用Softmax激活函数，将网络输出转换为概率分布，表示每个类别的预测概率。公式为：

     $$\hat{y}_i=\frac{e^{z_i}}{\sum_{j}e^{z_j}}$$
     - 其中，$\hat{y}_i$是类$i$的概率输出。
   - **二分类任务**：在二分类问题中，使用Sigmoid激活函数，将输出变为0到1之间的概率值。公式为：
     $$\hat{y}=\frac{1}{1 + e^{-z}}$$
     - 其中，$\hat{y}$是预测的类别为1的概率。
   - **回归任务**：对于回归任务，通常不使用激活函数，直接输出一个连续的值作为预测结果。

6. **计算损失**：
   - **多分类任务**：使用交叉熵损失（Cross-Entropy Loss）来衡量预测的概率分布与真实标签之间的差异。公式为：
     $$L =-\sum_{i}y_{i}\log(\hat{y}_{i})$$
     - 其中，$\hat{y}_{i}$是模型预测的类别$i$的概率，$y_{i}$是真实标签的独热编码值。
   - **二分类任务**：使用二元交叉熵损失（Binary Cross-Entropy Loss）来衡量预测概率与真实标签的差异。公式为：
     $$L =-(y\log(\hat{y})+(1 - y)\log(1-\hat{y}))$$
     - 其中，$y$是真实标签（0或1），$\hat{y}$是预测的概率值。
   - **回归任务**：使用均方误差（Mean Squared Error，MSE）来衡量预测值与真实值之间的差异。公式为：
     $$L=\frac{1}{n}\sum_{i = 1}^{n}(y_{i}-\hat{y}_{i})^{2}$$
     - 其中，$y_{i}$是真实值，$\hat{y}_{i}$是预测值。



**前向传播的作用：**
- **计算输出**：将输入数据通过网络生成预测结果。
- **计算损失**：在训练过程中，通过输出结果与真实标签计算损失函数值（如均方误差、交叉熵等），衡量模型的预测误差。

## 反向传播（Back Propagation）
反向传播是通过**计算损失函数相对于每层参数（权重和偏置）的梯度**，从输出层向输入层**更新参数**，从而**最小化损失函数**的过程。使用梯度下降法（还有动量梯度下降法、Adagrad、RMSprop、Adam）更新参数具体步骤如下：




> 详细全文请移步公主号：Goodnote。  
参考：[欢迎来到好评笔记（Goodnote）！](https://mp.weixin.qq.com/s/lCcceUHTrM7wOjnxkfrFsQ)



