> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本文简要介绍深度学习的前向传播与反向传播，以及前馈神经网络与反馈神经网络。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/bcae9025328f4ee68dda1483d515bbae.png#pic_center)

---
@[toc]
# 前向传播与反向传播
## 前向传播（Forward Propagation）
前向传播是将输入数据从输入层依次通过神经网络的各个隐藏层，最后输出预测结果的过程。该过程**用于计算网络的预测结果**，以及在训练过程中**计算损失函数值**。它的基本步骤如下：

1. **输入数据**：将输入数据传递给神经网络的输入层。
2. **线性变换**：在每一层中，神经元接收前一层输出的线性组合（加权求和），计算公式为：
    $$ z = w \cdot x + b $$
     - 其中，$w$是权重，$x$是输入，$b$是偏置。
3. **激活函数**：将线性组合$z$通过激活函数$f(z)$生成非线性的输出$a$，即：
    $$ a = f(z) $$
     - 常用的激活函数有ReLU、Sigmoid、Tanh等。
4. **逐层传播**：将每一层的输出作为下一层的输入，依次进行线性变换和激活，直至输出层。
5. **生成输出**：
- **多分类任务**：在多分类问题中，通常使用Softmax激活函数，将网络输出转换为概率分布，表示每个类别的预测概率。公式为：
  $$ \hat{y}_i=\frac{e^{z_i}}{\sum_{j}e^{z_j}} $$
   - 其中，$\hat{y}_i$是类$i$的概率输出。
- **二分类任务**：在二分类问题中，使用Sigmoid激活函数，将输出变为0到1之间的概率值。公式为：
  $$ \hat{y}=\frac{1}{1 + e^{-z}} $$
   - 其中，$\hat{y}$是预测的类别为1的概率。
 - **回归任务**：对于回归任务，通常不使用激活函数，直接输出一个连续的值作为预测结果。
6. **计算损失**
- **多分类任务**：使用交叉熵损失（Cross - Entropy Loss）来衡量预测的概率分布与真实标签之间的差异。公式为：
  $$ L =-\sum_{i}y_{i}\log(\hat{y}_{i}) $$
   - 其中，$\hat{y}_{i}$是模型预测的类别$i$的概率，$y_{i}$是真实标签的独热编码值。
- **二分类任务**：使用二元交叉熵损失（Binary Cross - Entropy Loss）来衡量预测概率与真实标签的差异。公式为：
  $$ L =-(y\log(\hat{y})+(1 - y)\log(1-\hat{y})) $$
   - 其中，$y$是真实标签（0或1），$\hat{y}$是预测的概率值。
- **回归任务**：使用均方误差（Mean Squared Error，MSE）来衡量预测值与真实值之间的差异。公式为：
  $$ L=\frac{1}{n}\sum_{i = 1}^{n}(y_{i}-\hat{y}_{i})^{2} $$
   - 其中，$y_{i}$是真实值，$\hat{y}_{i}$是预测值。



**前向传播的作用：**
- **计算输出**：将输入数据通过网络生成预测结果。
- **计算损失**：在训练过程中，通过输出结果与真实标签计算损失函数值（如均方误差、交叉熵等），衡量模型的预测误差。

## 反向传播（Back Propagation）
反向传播是通过**计算损失函数相对于每层参数（权重和偏置）的梯度**，从输出层向输入层**更新参数**，从而**最小化损失函数**的过程。使用梯度下降法（还有动量梯度下降法、Adagrad、RMSprop、Adam）更新参数具体步骤如下：


1. **计算损失的梯度**：根据损失函数$L$对输出层激活值$a$的偏导数，计算输出层激活值对损失的影响：
$$ \frac{\partial L}{\partial a} $$
2. **误差反向传播**：将损失的梯度从输出层逐层反向传递到每一层，计算各层神经元的梯度。误差信号$\delta$的公式为：
    $$ \delta=\frac{\partial L}{\partial z}=\frac{\partial L}{\partial a}\cdot\frac{\partial a}{\partial z} $$
     - 其中，$\frac{\partial a}{\partial z}$是激活函数的导数。
3. **参数梯度计算**：对每一层的权重$w$和偏置$b$计算其梯度：
    $$ \frac{\partial L}{\partial w}=\delta\cdot a_{prev},\quad\frac{\partial L}{\partial b}=\delta $$
     - 其中，$a_{prev}$是前一层的激活值。
4. **参数更新**：使用梯度下降法更新网络的参数。更新公式为：
    $$ w = w-\eta\cdot\frac{\partial L}{\partial w},\quad b = b-\eta\cdot\frac{\partial L}{\partial b} $$
     - 其中，$\eta$是学习率。

**反向传播的作用：**
- **计算梯度**：**计算**损失函数相对于每层参数（权重和偏置）的**梯度**。
- **误差传播**：将输出层的误差逐层传播到隐藏层和输入层，从而计算出每个参数的梯度。
- **参数优化**：计算每个参数对损失的影响，并通过梯度下降法更新参数，使模型的损失逐步减小。

## 总结
- **前向传播**：用于计算网络的**预测结果**和**损失函数值**。
- **反向传播**：用于**计算损失函数**相对于每个参数的**梯度**，**误差传播**，并**更新参数**以最小化损失。

# 神经网络
## 简介
**人工神经元、节点**：神经网络的**基本单元**，接收输入，进行计算，并生成输出。每个神经元通过权重连接到其他神经元。
**神经网络（Neural Networks）**：由多个相互连接的神经元组成的计算模型，模拟生物神经网络的工作原理。神经网络能够自动**从数据中学习复杂的模式和特征**，是机器学习和深度学习的基础。

## 结构
1. **输入层（Input Layer）**：接收外部数据输入，**每个神经元代表一个特征**。
2. **隐藏层（Hidden Layers）**：包含**一个或多个**隐藏层，负责**提取特征和模式**。每个隐藏层由若干神经元组成。
> 隐藏层的神经元之间复杂的连接模式，是网络学习复杂特征的主要部分。
> 深度网络（Deep Networks）：多个隐藏层的网络被称为深度神经网络（DNN），每一层提取不同层次的特征，逐步提取数据中的更高阶、更抽象的特征。。
3. **输出层（Output Layer）**：**生成最终的预测或分类结果**。根据任务不同，输出层的神经元数量和**激活函数**也不同。
> 分类任务：
> - 二分类：1 个神经元，使用 **Sigmoid 激活函数**，输出 0 到 1 的概率。
> - 多分类：**类别数个**神经元，使用 **Softmax 激活函数**，输出类别概率分布。
>
> 回归任务：
> - 1 个神经元，通常**不使用激活函数**，输出一个**连续数值**。
## 类型
1. **前馈神经网络（Feedforward Neural Networks, FFNN）**：信息单向流动，没有循环和反馈，是最基础的网络结构。
2. **反馈神经网络（Feedback Neural Networks）**：信息可以在网络中循环流动，具有记忆能力，能够处理时间序列数据。
### 前馈神经网络（Feedforward Neural Network, FFNN）
前馈神经网络是最简单和基础的神经网络结构，其中信息从输入层流向隐藏层，再流向输出层，**没有环路或反馈结构**。
#### 特点
- **信息单向流动**：数据从输入层经由隐藏层传递到输出层，**不存在信息的回流或反馈**。
- **无记忆能力**：当前时刻的输出仅依赖当前的输入和网络参数，**不记忆过去的信息**。
- **应用场景**：适用于结构化数据的分类、回归任务，如手写数字识别、图像分类等。
#### 常见变体
- **多层感知机（MLP）**：最常见的前馈神经网络形式，包括一个或多个隐藏层。
- **卷积神经网络（CNN）**：专门用于处理图像数据的前馈神经网络，利用卷积层提取空间特征。

###  反馈神经网络（Feedback Neural Network, RNN）
**反馈神经网络**也称为**递归神经网络（Recurrent Neural Network, RNN）**。反馈神经网络是一种**具有循环结构的神经网络模型**，可以将信息在网络中反复传递，因此具有**记忆能力**，适用于**处理时间序列数据**。

#### 特点
- **循环结构**：隐藏层的神经元可以**接收自身或者前一时间步的输出，形成信息反馈和环路**。
- **记忆能力**：能够**记住之前的信息**，从而在处理序列数据（如文本、时间序列预测）时表现出色。
- **应用场景**：适用于时间依赖性任务，如文本生成、机器翻译、语音识别等。

#### 常见变体
- **基本 RNN**：最简单的反馈神经网络，但存在梯度消失和梯度爆炸问题。
- **长短时记忆网络（LSTM）**：引入记忆单元和门控机制，能够有效处理长期依赖问题。
- **门控循环单元（GRU）**：LSTM 的简化变体，性能与 LSTM 类似，但计算更高效。

### 总结
- **神经网络结构**：神经网络由**输入层、隐藏层和输出层**组成，能够模拟复杂的函数关系。
- **前馈神经网络**：信息**单向流动，无记忆能力，适用于静态任务**。
- **反馈神经网络**：信息可以**循环流动，具有记忆能力，适用于时间序列和动态任务**。

# 常见深度学习算法分类
## 前馈神经网络（Feedforward Neural Networks, FFNN）

### 1. 多层感知机（MLP）
   - 全连接层（Fully Connected Layer） 或 密集层（Dense Layer）神经网络结构，适用于结构化数据的分类和回归任务。

### 2. 卷积神经网络（CNN）
   - 用于处理具有空间结构的输入数据（如图像），应用于图像分类、目标检测等任务。

### 3. AlexNet
   - 卷积神经网络的早期经典模型，用于大规模图像分类。

### 4. VGG 网络
   - 深层卷积神经网络，使用小卷积核来提升图像分类精度。

### 5. Inception 网络（GoogLeNet）
   - 引入 Inception 模块，通过多尺度特征提取减少参数量，提高模型性能。

### 6. ResNet（残差网络）
   - 引入残差连接，能够训练非常深的网络，缓解梯度消失问题。

### 7. 生成对抗网络（GAN）
   - 包括生成器和判别器两部分，尽管用于生成任务，但其生成器和判别器均为前馈网络。

### 8. U-Net
   - 用于图像分割任务的卷积神经网络，采用对称的编码器-解码器结构。

### 9. 自编码器（Autoencoder）
   - 由编码器和解码器组成的前馈网络，用于数据降维、去噪和特征学习。

### 10. Transformer 模型
  - 基于**自注意力机制的前馈结构**，用于自然语言处理、机器翻译等任务。

## 反馈神经网络（Recurrent Neural Networks, RNN）

### 1. 基本循环神经网络（RNN）
   - 具有循环连接的神经网络，适用于处理时间序列数据，但存在梯度消失问题。

### 2. 长短时记忆网络（LSTM）
   - 解决 RNN 的长期依赖问题，通过门控机制控制信息流动。

### 3. 门控循环单元（GRU）
   - LSTM 的简化版本，计算效率更高，也适用于时间序列数据。

### 4. 深度 Q 网络（DQN）
   - 虽然主要用于深度强化学习，但具有某种循环依赖的特性，可以视为一种特殊的反馈网络结构。

## 其他算法
以下几种算法不属于前馈神经网络或反馈神经网络的范畴，具有不同的结构和特点：
### 1. 图神经网络（GNN）
   - 处理图结构数据，节点之间的信息传递不属于单向的前馈网络或时间循环的反馈网络。
### 2. 自监督学习（Self-supervised Learning）
   - 一种训练策略，可以用于前馈网络或反馈网络中，并不限定特定的网络结构。
### 3. 对比学习（Contrastive Learning）
   - 也是一种训练方法，旨在学习更好的特征表示，同样可以用于前馈网络或反馈网络。
### 4. 迁移学习（Transfer Learning）
   - 一种学习策略，将预训练模型应用于新任务上，不属于特定的网络结构类别。

总结起来，前馈神经网络和反馈神经网络各有特点，前者主要用于处理静态数据，后者则用于处理动态序列数据。在深度学习中，根据任务的不同需求选择合适的网络结构至关重要。

