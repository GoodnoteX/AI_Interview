> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本文详细介绍这几年AIGC火爆的隐藏功臣，多模态模型：CLIP、BLIP。
> 

![在这里插入图片描述](https://github.com/GoodnoteX/Ai_Interview/blob/main/多模态论文笔记/image/3.png)

> @[toc]
# CLIP（Contrastive Language-Image Pre-training）
**CLIP** 是由 OpenAI 提出的一个用于**多模态**学习的模型，通过**对比学习**（contrastive learning）进行**图像-文本联合学习**的创新模型。CLIP 训练图像和文本的联合表示。

论文：[Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020)
## 1. CLIP 的核心思想
CLIP 的核心思想是**将图像和文本映射到一个共享的嵌入空间**中，并通过**对比学习**来**最大化匹配图像-文本对之间的相似度，最小化不匹配图像-文本对的相似度**。模型通过大量数据上进行预训练，具备强大的通用化能力，即**零样本学习（zero-shot learning）**，这意味着它可以处理没有见过的任务或类目而无需重新训练。

## 2. CLIP 的模型架构
CLIP 的架构包括**图像编码器**和**文本编码器**，它们分别将图像和文本输入嵌入到同一个向量空间。图像和文本分别经过编码后，**计算它们在向量空间中的相似度来进行对比学习**。
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/529af14905eb48abb40e7b75716fa0e0.png)
### 2.1 图像编码器
 **CNN**（如 **ResNet**）或 **Vision Transformer (ViT)** 作为图像编码器
- **ResNet** 或 **ViT** 接受图像作为输入，并**输出包含了图像的高层语义信息的向量**。
### 2.2 文本编码器
**Transformer** 作为文本编码器。这个编码器会将输入的文本描述（自然语言）转化为一个向量表示。
- 文本编码器会将每个文本通过多层 Transformer 的处理，**生成包含了文本的语义信息向量**。
### 2.3 对比学习机制
**CLIP 的训练目标**：通过**对比学习（contrastive learning）** 的损失函数 **让正确的图像-文本对的表示在向量空间中尽可能接近**，而**错误的图像-文本对在向量空间中尽可能远离**。

### 2.4 对比损失（Contrastive Loss）
**对比损失（Contrastive Loss）**：CLIP 使用了一种基于**InfoNCE**的对比损失函数。对于每一对图像-文本，模型会计算图像和所有文本对（以及文本和所有图像对）的相似度。通过**最大化匹配对的相似度，同时最小化不匹配对的相似度**，CLIP 可以学到更强的多模态表示。
#### InfoNCE
损失函数的目标是让图像 $x_i$;与正确文本描述 $t_i$的相似度最大化，同时与所有其他不相关文本 $t_j$ 的相似度最小化，公式为:

$$ \mathcal{L} = - \log\frac{\exp(\text{sim}(x_i,t_i)/\tau)}{\sum_{j = 1}^{N}\exp(\text{sim}(x_i,t_j)/\tau)} $$

- $(x_i)$：第$(i)$个图像样本。
- $(t_i)$：第$(i)$个图像样本的正确文本描述。
- $(t_j)$：其他文本描述（包括$(t_i)$和其他与$(x_i)$不匹配的文本描述）。
- $\text{sim}(x_i,t_j)$：图像$(x_i)$和文本$(t_j)$或者$(t_i)$的相似度，一般使用余弦相似度来计算。
- $(\tau)$：温度参数，用于控制相似度分布的平滑程度。


>$\text{sim}(x_i, t_j)$ 可以使用余弦相似度：
>$$\text{sim}(v_i,t_j)=\frac{v_i\cdot t_j}{\|v_i\|\|t_j\|}$$
>其中 \( $v_i$ \) 是图像 \( $x_i$ \) 的嵌入向量，\( $t_j$ \) 是文本 \( $t_j$ \) 的嵌入向量。这样计算得到一个 **相似度矩阵**，矩阵中的每个元素表示批次中任意一对图像和文本的相似度。

由于CLIP 包含两个主要的编码器部分：图像编码器、文本编码器，所以，损失函数需要分为两部分，针对之后**图像编码器的损失函数** 和 **文本编码器的损失函数**。之后根据各自的损失函数**优化两部分构件的权重**。
>- 其实损失函数都是一样的，只不过因为CLIP组成构件是两部分，所以需要分两部分，方便优化各自的权重参数，当**单独使用**图像编码器或者文本编码器时候（SD模型单独使用Text Encoder），**也会有很好的效果**。
>- 确保图像和文本的嵌入能够在共享的嵌入空间中**彼此对齐**（无论是从图像到文本，还是从文本到图像，匹配的对之间的相似度都被最大化，不匹配的对之间的相似度都被最小化。），从而在跨模态任务中实现一致性和相互匹配的能力。

#### 图像编码器损失函数
作用于**图像检索文本**：给定一个图像，可以找到与之最匹配的文本描述。

**图像损失部分**：对于每一个图像 \( $x_i$ \)，该部分的损失最大化它与正确文本 \( $t_i$ \) 的相似度，同时最小化它与其他错误文本 \( $t_j$ \) 的相似度。这一部分确保了**图像能够找到正确的文本**，也就是说图像编码器能够将图像嵌入到一个空间中，使得匹配的文本描述与它更接近。





> 详细全文请移步公主号：Goodnote。  
参考：[欢迎来到好评笔记（Goodnote）！](https://mp.weixin.qq.com/s/lCcceUHTrM7wOjnxkfrFsQ)
