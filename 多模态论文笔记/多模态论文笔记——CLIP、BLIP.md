> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本文详细介绍这几年AIGC火爆的隐藏功臣，多模态模型：CLIP、BLIP。
> 

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/06ff913f43d54234bed816cd361627a1.png#pic_center)

> @[toc]
# CLIP（Contrastive Language-Image Pre-training）
**CLIP** 是由 OpenAI 提出的一个用于**多模态**学习的模型，通过**对比学习**（contrastive learning）进行**图像-文本联合学习**的创新模型。CLIP 训练图像和文本的联合表示。

论文：[Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020)
## 1. CLIP 的核心思想
CLIP 的核心思想是**将图像和文本映射到一个共享的嵌入空间**中，并通过**对比学习**来**最大化匹配图像-文本对之间的相似度，最小化不匹配图像-文本对的相似度**。模型通过大量数据上进行预训练，具备强大的通用化能力，即**零样本学习（zero-shot learning）**，这意味着它可以处理没有见过的任务或类目而无需重新训练。

## 2. CLIP 的模型架构
CLIP 的架构包括**图像编码器**和**文本编码器**，它们分别将图像和文本输入嵌入到同一个向量空间。图像和文本分别经过编码后，**计算它们在向量空间中的相似度来进行对比学习**。
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/529af14905eb48abb40e7b75716fa0e0.png)
### 2.1 图像编码器
 **CNN**（如 **ResNet**）或 **Vision Transformer (ViT)** 作为图像编码器
- **ResNet** 或 **ViT** 接受图像作为输入，并**输出包含了图像的高层语义信息的向量**。
### 2.2 文本编码器
**Transformer** 作为文本编码器。这个编码器会将输入的文本描述（自然语言）转化为一个向量表示。
- 文本编码器会将每个文本通过多层 Transformer 的处理，**生成包含了文本的语义信息向量**。
### 2.3 对比学习机制
**CLIP 的训练目标**：通过**对比学习（contrastive learning）** 的损失函数 **让正确的图像-文本对的表示在向量空间中尽可能接近**，而**错误的图像-文本对在向量空间中尽可能远离**。

### 2.4 对比损失（Contrastive Loss）
**对比损失（Contrastive Loss）**：CLIP 使用了一种基于**InfoNCE**的对比损失函数。对于每一对图像-文本，模型会计算图像和所有文本对（以及文本和所有图像对）的相似度。通过**最大化匹配对的相似度，同时最小化不匹配对的相似度**，CLIP 可以学到更强的多模态表示。
#### InfoNCE
损失函数的目标是让图像 $x_i$;与正确文本描述 $t_i$的相似度最大化，同时与所有其他不相关文本 $t_j$ 的相似度最小化，公式为:

$$ \mathcal{L} = - \log\frac{\exp(\text{sim}(x_i,t_i)/\tau)}{\sum_{j = 1}^{N}\exp(\text{sim}(x_i,t_j)/\tau)} $$

- $(x_i)$：第$(i)$个图像样本。
- $(t_i)$：第$(i)$个图像样本的正确文本描述。
- $(t_j)$：其他文本描述（包括$(t_i)$和其他与$(x_i)$不匹配的文本描述）。
- $\text{sim}(x_i,t_j)$：图像$(x_i)$和文本$(t_j)$或者$(t_i)$的相似度，一般使用余弦相似度来计算。
- $(\tau)$：温度参数，用于控制相似度分布的平滑程度。


>$\text{sim}(x_i, t_j)$ 可以使用余弦相似度：
>$$ \text{sim}(v_i,t_j)=\frac{v_i\cdot t_j}{\|v_i\|\|t_j\|} $$
>其中 \( $v_i$ \) 是图像 \( $x_i$ \) 的嵌入向量，\( $t_j$ \) 是文本 \( $t_j$ \) 的嵌入向量。这样计算得到一个 **相似度矩阵**，矩阵中的每个元素表示批次中任意一对图像和文本的相似度。

由于CLIP 包含两个主要的编码器部分：图像编码器、文本编码器，所以，损失函数需要分为两部分，针对之后**图像编码器的损失函数** 和 **文本编码器的损失函数**。之后根据各自的损失函数**优化两部分构件的权重**。
>- 其实损失函数都是一样的，只不过因为CLIP组成构件是两部分，所以需要分两部分，方便优化各自的权重参数，当**单独使用**图像编码器或者文本编码器时候（SD模型单独使用Text Encoder），**也会有很好的效果**。
>- 确保图像和文本的嵌入能够在共享的嵌入空间中**彼此对齐**（无论是从图像到文本，还是从文本到图像，匹配的对之间的相似度都被最大化，不匹配的对之间的相似度都被最小化。），从而在跨模态任务中实现一致性和相互匹配的能力。

#### 图像编码器损失函数
作用于**图像检索文本**：给定一个图像，可以找到与之最匹配的文本描述。

**图像损失部分**：对于每一个图像 \( $x_i$ \)，该部分的损失最大化它与正确文本 \( $t_i$ \) 的相似度，同时最小化它与其他错误文本 \( $t_j$ \) 的相似度。这一部分确保了**图像能够找到正确的文本**，也就是说图像编码器能够将图像嵌入到一个空间中，使得匹配的文本描述与它更接近。

$$ \mathcal{L}_{\text{image}} = - \frac{1}{N}\sum_{i = 1}^{N}\log\frac{\exp(\text{sim}(v_i,t_i)/\tau)}{\sum_{j = 1}^{N}\exp(\text{sim}(v_i,t_j)/\tau)} $$

#### 文本编码器损失函数
作用于**文本检索图像**：给定一个文本描述，可以找到与之最匹配的图像。
- **文本损失部分**：对于每一个文本 \( $t_i$ \)，该部分的损失最大化它与正确图像 \( $x_i$ \) 的相似度，同时最小化它与其他错误图像 \( $x_j$ \) 的相似度。这一部分确保了**文本能够找到正确的图像**，也就是说文本编码器能够将文本嵌入到一个空间中，使得匹配的图像与它更接近。
$$ \mathcal{L}_{\text{text}} = - \frac{1}{N}\sum_{i = 1}^{N}\log\frac{\exp(\text{sim}(v_i,t_i)/\tau)}{\sum_{j = 1}^{N}\exp(\text{sim}(v_j,t_i)/\tau)} $$


#### 总损失函数
最大化图像和其正确文本描述之间的相似度，同时最小化图像和其他不匹配文本描述之间的相似度。

$$ \mathcal{L}_{\text{CLIP}}=\frac{1}{2}(\mathcal{L}_{\text{image}}+\mathcal{L}_{\text{text}}) $$
- **\( $\mathcal{L}_{\text{image}}$ \)**：文本编码器损失函数
- **\( $\mathcal{L}_{\text{image}}$ \)**：图像编码器损失函数
### 2.5 共享嵌入空间
CLIP 将图像和文本映射到**相同的嵌入空间**的向量，可以直接进行相似度计算。
## 3. CLIP 的训练方式
CLIP 的训练使用了大量的**图像-文本配对数据**进行对比学习。这些数据通常来自网络，例如图像和它们的自然语言描述（如社交媒体图片和它们的描述文本）。OpenAI从**互联网收集了共4个亿的文本-图像对**。
## 4. CLIP 的推理过程
在推理过程中，CLIP 通过**计算图像和文本描述的相似度来执行分类或检索任务**
### 4.1 图像分类
在**图像分类任务**中，CLIP 可以通过以下步骤进行推理：
1. 给定一个**输入图像**，将其通过图像编码器生成一个向量表示。
2. 使用一组**标签（例如“猫”、“狗”、“汽车”等）的文本描述**，将这些描述通过文本编码器生成一组向量表示。
3. 计算**图像向量与每个文本向量的相似度**，并选择相似度最高的标签作为分类结果。
这种方式使 CLIP 能够在没有特定类别标签的情况下进行**零样本分类（zero-shot classification）**。
### 4.2 跨模态检索
在**跨模态检索任务**中，CLIP 可以使用**文本编码器执行文本检索图像**或使用**图像编码器执行图像检索文本**。例如：
- 输入一个文本描述，检索与之相关的图像。
- 输入一个图像，检索与之语义相关的文本描述。

## 5. CLIP 的优势
**1 零样本学习**
CLIP 最具创新的特性之一是它在很多任务中可以执行**零样本学习**。可以通过它的预训练模型处理从未见过的新任务。例如，CLIP 可以在未见过的分类标签下进行分类。

**2 跨模态能力**
CLIP 的跨模态能力使得它在图像和文本的任务中都表现出色。进行**跨模态检索**。

**5.3 灵活性和通用性**
CLIP 能够在广泛的应用场景中工作，涵盖图像分类、检索、零样本推理等任务，而不需要为每个任务单独设计和训练模型。
## 6. CLIP 的应用场景
### 6.1 零样本学习
CLIP 不依赖于特定类别标签，而是通过自然语言描述进行分类。因此，它可以在开放领域的任务中对图像进行分类，不需要专门的任务训练。
### 6.2 跨模态检索
CLIP 的跨模态能力使它能够通过文本查询图像，或者通过图像查询相关的文本。这种灵活性使 CLIP 在图像搜索和检索任务中表现突出。
### 6.3 多模态理解任务
CLIP 可以应用于**图像-文本匹配**、**视觉问答**等任务，模型能够理解图像和文本的联合语义，进而执行多模态的复杂任务。
## 7. CLIP 的局限性
- **依赖大规模数据，计算资源需求高**：CLIP 的预训练需要大量的图像-文本配对数据和计算资源，这对于小型项目或研究可能是一个挑战。

---
# BLIP（Bootstrapping Language-Image Pre-training）
**BLIP** 是旨在改进图像-文本联合学习的效率多模态模型，特别是通过**生成任务**和**对比学习结合**的方式，在**低监督甚至无监督情况下提升模型性能**。BLIP 的创新点在于它通过**多任务预训练和自引导学习（bootstrapping）机制**，能够以更少的数据达到更好的性能表现。

BLIP 主要用于处理图像与文本的多模态任务，例如**图像描述生成**、**文本到图像的检索**、**图像到文本的匹配**等。

论文：[BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/pdf/2201.12086)

---
## 1. BLIP 的核心思想
BLIP 通过结合**对比学习**、**图文匹配**和**生成任务**，模型能够更高效地学习图像与文本之间的关系。还在模型训练的过程中，引入了**自引导**机制，使得模型在训练的过程中，可以自我提升性能。
## 2. BLIP 的模型架构和损失函数
BLIP 的架构设计包含**图像编码器**、**文本编码器**、**视觉文本编码器**、**视觉文本解码器**。它结合了对比学习和生成式任务，以**自引导的方式**提升模型性能。
参考：[AI绘画原理解析：从CLIP、BLIP到DALLE、DALLE 2、DALLE 3、Stable Diffusion](https://blog.csdn.net/v_JULY_v/article/details/131205615)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1cd89b57f6de4c489fa6355f346efc80.png)

> 原文解读：Pre-training model architecture and objectives of BLIP (same parameters have the same color). We propose multimodal mixture of encoder-decoder, a unified vision-language model which can operate in one of the three functionalities: (1) Unimodal encoder is trained with an image-text contrastive (ITC) loss to align the vision and language representations. (2) Image-grounded text encoder uses additional cross-attention layers to model vision-language interactions, and is trained with a image-text matching (ITM) loss to distinguish between positive and negative image-text pairs. (3) Image-grounded text decoder replaces the bi-directional self-attention layers with causal self-attention layers, and shares the same cross-attention layers and feed forward networks as the encoder. The decoder is trained with a language modeling (LM) loss to generate captions given images.

BLIP模型主要由4个部分组成，从左至右分别是
### 视觉编码器Image Encoder(ViT)——提取图片特征
视觉编码器本质就是**ViT 的架构**：将输入图像分割成一个个的 Patch 并将它们编码为一系列 Image Embedding，并使用**额外的 [CLS] token 来表示全局的图像特征**
### 文本编码器Text Encoder(BERT)——提取文本特征
文本编码器就是 **BERT 的架构**，其中 **[CLS] token**附加到文本输入的开头以总结句子，作用是**提取文本特征**与第1部分的图像特征做对比学习

#### 对比学习目标函数ITC
在这个过程中会训练一个**文本-图像特征对比学习目标函数** (Image-Text Contrastive Loss, **ITC**)。ITC 作用于第1部分的视觉编码器(ViT)和第2部分的文本编码器(BERT)，目标是 **对齐视觉和文本的特征空间** ，方法是使得正样本图文对的相似性更大，负样本图文对的相似性更低。【和CLIP一样】

**ITC 使用对比学习损失（如 InfoNCE Loss）**，它与 CLIP 类似。给定一个图像和文本的批次，模型最大化图像和其匹配文本的相似度，同时最小化图像和其他文本的相似度。

**损失函数**可以表示为：

$$ \mathcal{L}_{LTC} = - \frac{1}{N}\sum_{i = 1}^{N}\left(\log\frac{\exp(\text{sim}(v_i,t_i)/\tau)}{\sum_{j = 1}^{N}\exp(\text{sim}(v_i,t_j)/\tau)}+\log\frac{\exp(\text{sim}(v_i,t_i)/\tau)}{\sum_{j = 1}^{N}\exp(\text{sim}(v_j,t_i)/\tau)}\right) $$

其中，\( $\tau$ \) 是温度参数，\( $v_i$ \) 和 \( $t_i$ \) 分别是图像和文本的嵌入表示。
### 视觉文本编码器Image-grounded Text Encoder(变种 BERT)——BERT中插入交叉注意层，从而针对图片特征和文本特征做二分类
视觉文本编码器的具体做法是在文本编码器（比如BERT）的每个transformer block的**自注意(Bi Self-Att)层和前馈网络(Feed Forward)之间额外插入一个交叉注意(Cross-Attention)，以引入视觉特征**，作用是根据 ViT 给的图片特征和文本输入**做二分类**，所以使用的是**编码器**，且注意力部分是**双向**的 Self-Attention，且添加一个**额外的 [Encode] token**，作为图像文本的**联合表征**

#### 图文匹配目标函数ITM
在这个过程中则训练一个**图文匹配目标函数** (Image-Text Matching Loss, **ITM**)
ITM 作用于**第1部分的视觉编码器**和**第3部分的视觉文本编码器**，是一个**二分类**任务，目标是**学习图像文本的联合表征**，使用一个分类头来预测 image-text pair 的 正匹配 还是 负匹配，目的是**学习 image-text 的多模态表示，调整视觉和语言之间的细粒度对齐**，作者在这里依然使用了 ALBEF 中的 hard negative mining 技术。

**ITM 使用二分类交叉熵损失**。对于每一对图像和文本，模型会预测它们是否匹配，匹配的对标签为 1，不匹配的对标签为 0。
损失函数可以表示为：

$$ \mathcal{L}_{ITM} = - \frac{1}{N}\sum_{i = 1}^{N}(y_i\log(p_i)+(1 - y_i)\log(1 - p_i)) $$

其中，\( $y_i$ \) 表示是否匹配（1 表示匹配，0 表示不匹配），\( $p_i$ \) 是模型预测的匹配概率。
### 视觉文本解码器Image-grounded Text Decoder(变种 BERT)——根据图片特征和文本特征做文本生成
视觉文本解码器使用 Cross-Attention，作用是根据 ViT 给的**图片特征和文本输入做文本生成的任务**，所以使用的是**解码器**，且将 上图第3部分的 Image-grounded Text Encoder 结构中的 Bi Self-Att 替换为 **Causal Self-Att，目标是预测下一个 token**，且添加一个**额外的 [Decode] token 和结束 token**，作为生成结果的起点和终点

一个需要注意的点是：相同颜色的部分是参数共享的，即视觉文本编码器和视觉文本解码器共享除 Self-Attention 层之外的所有参数。每个 image-text 在输入时，image 部分只需要过一个 ViT 模型，text 部分需要过3次文本模型
#### 语言模型目标函数LM
过程中训练一个**语言模型目标函数** (Language Modeling Loss, **LM**)。
毕竟由于**BLIP 包含解码器，用于生成任务**。既然有这个任务需求，那就意味着需要一个针对于生成任务的语言模型目标函数，LM 作用于**第1部分的视觉编码器**和**第4部分的视觉文本解码器**，目标是根据给定的图像以自回归方式来**生成关于文本的描述**。

> 与 VLP(Vision-Language Pre-training 的简称，指视觉-语言联合预训练模型。其目标是通过联合训练视觉和语言模态，学习一个能够理解和生成多模态信息的模型。) 中广泛使用的 MLM 损失(完形填空)相比，LM 使模型能够将视觉信息转换为连贯的字幕

- **目标**：根据输入图像生成自然语言描述。该任务用于训练模型生成语义丰富的文本，用于**描述输入图像的内容**。
- **损失函数**：LM  使用**自回归的交叉熵损失**。给定图像的特征表示，解码器生成描述性文本，每一步的**预测结果**与**目标词进行对比**，计算交叉熵损失。
  
  损失函数可以表示为：
  - LM 损失计算**生成序列中每个词与目标词的交叉熵**，公式如下：
    $$
     \mathcal{L}_{LM} = - \sum_{t=1}^{T} \log P(y_t | y_{<t}, \text{image features})
    $$
     - \( $y_t$ \) 是目标序列的第 \( $t$ \) 个词。
     - \( $y_{<t}$ \) 表示生成的先前序列。
     - 图像特征来自视觉编码器。
     - \( $P(y_t | y_{<t}, \text{image features})$ \) 是模型生成 $y_t$ 的预测概率。

### 总结

1. **第1部分的视觉编码器(ViT)和第2部分的文本编码器(BERT)用于对比学习任务**，处理跨模态检索和分类任务。
2. **第1部分的视觉编码器和第3部分的视觉文本编码器用于图文匹配**，学习 image-text 的多模态表征学习任务，**调整视觉和语言之间的细粒度对齐，处理多模态理解任务**。
3. **第1部分的视觉编码器和第4部分的视觉文本解码器用于文本生成任务**，目标是根据给定的图像以**自回归**方式来生成关于文本的描述。


BLIP 的**总损失是多种任务损失的加权和**，具体为：

$$
\mathcal{L}_{BLIP} = \lambda_{ITC} \mathcal{L}_{ITC} + \lambda_{ITM} \mathcal{L}_{ITM} + \lambda_{LM} \mathcal{L}_{LM}
$$
- \( $\lambda_{ITC}$ \)、\( $\lambda_{ITM}$ \) 和 \( $\lambda_{LM}$ \) 是损失的权重，用于平衡各任务的重要性。



BLIP 的损失函数通过整合 **ITC、ITM 和 LM** 三种目标，兼顾多模态对齐、理解和生成任务：
1. **ITC Loss**：优化图像和文本模态的联合表示。
2. **ITM Loss**：增强模型的匹配能力和判别能力。
3. **LM Loss**：用于生成任务，使模型能够生成连贯的文本描述。


> 
>   - **自引导学习** 是一种**自我改进的学习机制**，主要用于训练阶段，通过**利用模型自身生成的输出来进一步优化模型**，尤其适用于数据较少或无监督的场景。
>    - **自回归** 则是一种**生成式模型**，在**生成阶段**使用，按顺序逐步生成输出，每一步依赖于之前的生成结果，主要用于自然语言生成和时间序列等任务。据较少或无监督的场景，**强调逐步生成每个元素**。

## 3. BLIP 的多任务学习框架

BLIP 的多任务学习框架包括以下几种核心任务：
### 3.1 对比学习任务
- 与 CLIP 类似，BLIP 使用**对比学习**来将图像和文本映射到一个共享的嵌入空间中。在该任务中，模型会**最大化正确图像-文本对之间的相似度**，**最小化不匹配对的相似度**。
- 这种对比学习使模型能够很好地理解图像和文本之间的关系，适用于**图像到文本匹配**和**文本到图像检索**任务。
### 3.2 图文匹配任务
-  **CLIP** 的训练目标是让图像和文本对彼此**对齐**，但它并**没有显式地引入一个图文匹配任务来判断图像和文本是否匹配**。CLIP 通过对比学习获得图文对的相似性，而不是直接判断配对关系。
- BLIP 引入了 图文匹配（ITM） 任务，用于**判断图像和文本是否匹配**。这一任务使用了**二分类交叉熵损失**，直接对匹配和不匹配的图文对进行区分。ITM 任务的引入可以让 BLIP 更有效地进行图文匹配和图文检索，尤其在需要判断图像和文本是否相关的任务中具有优势。

ITM 优化的优势：ITM 任务的引入使 BLIP 能够更精确地判断图像和文本之间的语义一致性，从而提升了图文检索任务的效果。
### 3.3 图像到文本生成任务
- BLIP 引入了**图像到文本生成任务**，这是 BLIP 的一个关键创新。模型通过**生成与图像对应的文本描述，来提升对图像语义的理解能力**。
- 该任务不仅让 BLIP 能够处理多模态检索，还使它具有很强的生成能力，能够自动生成与图像相关的自然语言描述。

### 3.4 广义的文本到图像重构任务
- BLIP 还支持**文本到图像重构**任务，这一任务要求模型**通过给定的文本生成与之相关的图像表示**。这种方式**使模型理解文本语义的同时，推断出相应的视觉信息**。
- 文本到图像的重构任务强化了模型的多模态对齐能力，使其在理解复杂文本和图像的关联时表现更好。
> 文本到图像重构：BLIP 可以根据文本线索推测图像的特征或对图像进行重构，但这更多是指在**已有图像的基础上(**已经有一个图像的嵌入表示)进行细化和调整，而**不是从头生成图像**。

## 4. BLIP 的训练方式
BLIP的学习框架如下所示，详细介绍了模型训练中的数据优化流程：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/f5d42d615c8b424e98f43c488af5d625.png)
> 图 3. BLIP 的学习框架。我们引入了一个生成器（Captioner）来为网络**图像生成合成的描述**，并引入了一个过滤器（Filter）来**去除噪声图像-文本对**。生成器和过滤器均从同一个预训练模型初始化，并在一个小规模的人工标注数据集上分别进行微调。通过这种方式生成的增强数据集被用于训练一个新的预训练模型。

整个**数据优化流程**如下：
1. **数据过滤**：
   - 初始 Web 数据对$(I_W, T_W)$ Filter 过滤，得到高质量子集$(I_W, T_W')$。

2. **合成数据生成**：
   - Captioner 为 Web 图像生成合成的描述文本$((I_W, T_S)$。

3. **数据增强**：
   - 将过滤后的 Web 数据$(I_W, T_W')$和合成的描述文本数据$(I_W, T_S)$加入到原始数据集中，进一步增强数据量和质量。

4. **人工标注数据的贡献**：
   - 高质量的人工标注数据$(I_H, T_H)$作为模型优化的重要参考，用于对 Filter 和 Captioner 进行微调。

最终，扩展和优化后的数据集被用来**训练新的多模态模型。**
### 4.1 输入数据质量处理
BLIP 使用了图像-文本配对数据进行训练。类似于 CLIP，这些配对数据可能来自社交媒体上的图像和其描述文本。

**CLIP 的数据来源于 Web 上爬来的图像-文本对**，采用对比学习的方式，基本属于**自监督**了，不太需要做数据标注；

而BLIP的输入数据集由两部分组成：
  1. **Web 数据**：从互联网上获取的大规模图像和文本对（\((I_W, T_W)\)）。
  2. **人工标注数据**：小规模的高质量人工标注图像-文本对（\((I_H, T_H)\)）。

BLIP 为了去除**Web 数据噪声大的缺点**，提出了**Captioning and Filtering (CapFilt) 模块**  ，并使用**人工标注数据**进行微调。这个模块就是**用来减小噪声、丰富数据**的，主要包括两个模块：即字幕与过滤器方法CapFilt (Captioning and Filtering)，详细介绍参考4.2节的自引导学习。

### 4.2 自引导学习（Bootstrapping Learning）
BLIP 的**自引导机制允许模型根据自身的输出**来**改进输入**的表示。这意味着模型在训练过程中，可以**根据自己生成的文本或图像**表示进一步**调整权重**，从而改进模型性能。**自引导机制在低监督或无监督学习场景中表现出色，能够有效提高训练效率**。

这一机制主要通过以上一节提到的CapFilt实现，CapFilt 方法包含两个模块：：
- **字幕器 Captioner**：**用于生成给定Web图像的字幕**，改进低质量输入文本表示（如网络爬取文本）。相当于给一张网络图片，生成字幕（合成文本）。它是一个视觉文本解码器(类似上述BLIP模型结构的**第4部分**)。
- **过滤器 Filter**：对**图像-文本对进行质量评估**，**去除嘈杂的图像-文本对**。过滤掉噪声图文对image-text pair，它是一个视觉文本编码器(对类似上述BLIP模型结构的第3部分和第一部分)，看原始 Web 文本/合成文本是否与图像匹配。

生成器 (Captioner) 和 过滤器 (Filter) 都是从相同的**预训练MED模型**初始化的，并在**COCO数据集上单独微调**。

> - MED（Multimodal Encoder-Decoder）模型：一个多模态模型，用于编码图像和文本之间的语义关系，支持生成任务（如图像描述）和匹配任务（如文本与图像的相关性判断）。
> - COCO 数据集：一个常用的高质量数据集，包含丰富的图像和其详细的文本描述，是多模态任务的重要基准。

**自引导学习和自回归的区别：**

>    - **自引导学习** 是一种**自我改进的学习机制**，主要用于训练阶段，通过利用模型自身生成的输出来进**改进输入**，进而优化训练。
>    - **自回归** 则是一种**生成式模型**，在**生成阶段**使用，按顺序逐步生成输出，每一步依赖于之前的生成结果，通过利用模型已经生成的输出，**优化新的输出**。

### 4.3 多任务联合训练
BLIP 的训练过程中，会同时**优化对比学习、生成任务和图文匹配**。这种多任务学习方式让模型不仅能够进行**对比学习**，还能进行**处理图像-文本匹配**和**图生文**。

- **多任务损失**：模型会根据对比学习任务和生成任务的表现来**计算联合损失**，从而优化图像编码器和文本编码器/解码器。

## 5. BLIP 的应用场景
### 5.1 图像描述生成
- **步骤**：
  1. 图像编码器生成图像的嵌入表示。
  2. 文本解码器根据图像嵌入，逐步生成文本描述。
### 5.2 跨模态检索
**文本检索图像**
- **步骤**：
  1. 文本编码器将输入的文本描述编码成向量。
  2. 模型在嵌入空间中找到与该文本向量最接近的图像向量。

**图像检索文本**
- **步骤**：
  1. 图像编码器将输入的图像编码成向量。
  2. 文本编码器将文本候选描述编码为向量，模型找到与图像向量最接近的文本向量。
### 5.3 零样本学习
BLIP 在**零样本学习**场景中也具有一定的潜力。通过预训练的对比学习和生成任务，BLIP 可以在没有特定领域标注数据的情况下，进行图像分类、描述生成和跨模态检索。

## 6. BLIP 的优势
### 6.1 多任务学习
BLIP 结合了**对比学习**、**生成任务**和**图文匹配**，使其能够同时在理解和生成方面表现优异。这种多任务学习方式让 BLIP 能够更好地理解图像和文本之间的关系。

### 6.2 自引导学习机制
BLIP 引入了**自引导学习机制**，使得**模型能够通过自己的输出改进训练**。这一机制提高了模型在低监督或无监督场景中的表现，极大提升了数据效率。

### 6.3 强大的生成能力
BLIP 不仅能够理解多模态数据，还能够**生成自然语言描述**。这使得 BLIP 在生成任务中也表现出色，特别适合图像描述生成任务。

## 7. BLIP 的局限性

尽管 BLIP 在多模态任务中表现优异，但它也有一些局限性需要注意：

### 7.1 计算资源需求
BLIP 模型的训练依赖于大量的计算资源。
### 7.2 数据依赖
虽然 BLIP 引入了**自引导学习机制，能够在低监督环境下提升性能**，但它依然**依赖于大量的高质量图像-文本配对数据**来进行预训练。
### 7.3 复杂的场景生成文本质量不精确
某些复杂的场景中，生成的文本可能缺乏细节或者不够精确。

---
# CLIP 与 BLIP 的对比
## 总结

**CLIP（Contrastive Language-Image Pretraining）** 和 **BLIP（Bootstrapping Language-Image Pretraining）** 都是多模态学习模型。

| **特性**                | **CLIP（Contrastive Language-Image Pretraining）**                          | **BLIP（Bootstrapping Language-Image Pretraining）**                           |
|-------------------------|----------------------------------------------------------------------------|-------------------------------------------------------------------------------|
| **核心思想**            | 通过**对比学习**将图像和文本映射到一个共享的嵌入空间，进行匹配与分类         | 结合**对比学习**、**生成任务**、**图文匹配**，通过**自引导学习**提高生成能力  |
| **模型架构**            | 独立的**图像编码器**（ResNet/ViT）和**文本编码器**（Transformer）            | **图像编码器**、**文本编码器**、**视觉文本编码器**、**视觉文本解码器**，支持生成任务     |
| **训练方式**            | 基于**对比学习**，依赖大规模图像-文本配对数据                               | 通过**对比学习**、**图文匹配**、**生成任务**和**自引导学习**结合进行多任务联合训练          |
| **优势**                | - 强大的**零样本学习**能力<br> - 优秀的**跨模态检索**能力      <br> -  多模态理解任务             |- 强大的**零样本学习**能力<br> - 优秀的**跨模态检索**能力      <br> -  多模态理解任务   <br> - **生成能力强**，支持图像描述生成<br> - **自引导学习**提升低监督数据效率     |
| **局限性**              | - 依赖大规模数据，计算资源需求高<br> - 缺乏生成能力                          | - 计算资源需求更高<br> - 数据依赖大，复杂场景生成文本质量可能不精确           |
| **自引导学习**          | 无                                                                            | **自引导学习机制**：通过模型自我生成输出改进训练，适合低监督场景               |
| **生成能力**            | 无                                                                            | 具备强大的**图像到文本生成能力**，能够生成自然语言描述                         |
| **复杂任务处理**        | 主要擅长**图像分类**和**跨模态检索**                                         | 支持更复杂的**图像到文本生成**和**图文匹配**任务                         |

---

## BLIP 解决了 CLIP 的哪些问题？
### 生成能力的增强
   - **CLIP** 主要专注于图像和文本的对比学习，但**不具备生成能力**。CLIP 无法自动生成与图像对应的文本描述，这限制了它在生成类任务中的表现。
   - **BLIP** 通过引入**图像到文本生成任务**，补足了这一缺陷。BLIP 不仅可以进行图像-文本匹配，还能够生成与图像相关的自然语言描述，适用于**图像描述生成任务**，扩展了模型的应用场景。
> **广义的文本到图像重构**：BLIP 可以根据文本线索推测图像的特征或对图像进行重构，但这更多是指在已有图像的基础上(已经有一个图像的嵌入表示)进行细化和调整，而**不是从头生成图像**。
### 图文匹配任务
-  **CLIP** 的训练目标是让图像和文本对彼此**对齐**，但它并**没有显式地引入一个图文匹配任务**来判断图像和文本是否匹配。CLIP 通过对比学习获得图文对的相似性，而不是直接判断配对关系。
- BLIP 引入了 图文匹配（ITM） 任务，用于**判断图像和文本是否匹配**。这一任务使用了二分类交叉熵损失，直接对匹配和不匹配的图文对进行区分。ITM 任务的引入可以让 BLIP 更有效地进行图文匹配和图文检索，尤其在需要判断图像和文本是否相关的任务中具有优势。

ITM 优化的优势：ITM 任务的引入使 BLIP 能够更精确地判断图像和文本之间的语义一致性，从而提升了图文检索任务的效果。

### 自引导学习机制
   - **CLIP**使用的是**大规模的图像-文本对数据**，这些数据是通过**自动从 Web 上爬取**的，没有经过传统的人工标注。在大数据集上表现优异。然而，在**低数据或无监督场景下，CLIP 的效果可能不如预期**。
   - **BLIP** 引入了**自引导学习机制（Bootstrapping Learning）**，使得模型能够**通过自我监督改进**。在**低监督或无监督的环境下也能表现良好**，解决了 CLIP 在数据较少场景中的表现问题。

### 多任务学习能力
   - **CLIP** 主要依赖于**对比学习**进行图像和文本的匹配，在分类和检索任务中表现突出，但在**复杂的多模态任务（如图像描述生成）中存在局限性**。
   - **BLIP** 引入了**多任务学习框架**，包括**对比学习**、**图像到文本生成**、**图文匹配**等任务，进一步提升了模型在多模态任务中的表现能力。通过多个任务的联合学习，BLIP 能够更好地理解和生成图像和文本之间的语义关系，增强了模型的灵活性。

### 数据效率提升
   - **CLIP** 需要大量的图像-文本配对数据进行预训练，虽然它在**大数据集**上表现良好，但在一些特定领域或小数据集下，训练难度较大，模型性能可能受到限制。
   - **BLIP** 通过多任务学习和**自引导机制**，在**数据效率**方面有所提升，能够更好地适应数据**较少或无标注数据的场景**，减少了对大规模数据的依赖。通过自引导机制，BLIP 能够利用生成任务中的结果来进一步优化模型，使得模型在数据受限的情况下表现优异。

### 图像-文本对齐的增强
   - **CLIP** 主要**通过对比学习进行图像和文本对齐**，这种方法虽然在大多数任务中表现优异，但对一些复杂的图像-文本关系（例如描述非常详细或涉及复杂场景的图像）可能存在局限。
   - **BLIP** 通过**结合对比学习、图文匹配和生成任务，对图像和文本进行更精细的对齐**。BLIP 能够生成与图像匹配的文本描述，并通过广义的重构任务来推测文本对应的图像，从而增强了图像和文本的语义对齐能力。
>**对齐**指的就是模型能够**将同样语义的图像和文本特征映射到共享的表示空间中，使它们在这个空间里距离很近**。比如，一张有猫的图像和一句描述“这是一只猫”的文本，经过对齐后，它们在共享的向量空间中会有相近的向量表示。

---

### 总结

**BLIP** 在解决了 **CLIP** 的一些局限问题后，扩展了多模态任务的能力：
- **增强了生成能力**：能够进行图像到文本的生成任务。
- **引入自引导学习**：在低数据或无监督情况下仍能有效训练。
- **多任务学习能力**：包括**对比学习**、**图文匹配**、**图像到文本生成**等任务。
- **提高数据效率**：减少了对大规模数据的依赖。
- **加强了图像-文本对齐**：能够通过多任务学习更好地对齐图像和文本语义。

总的来说，**BLIP** 通过结合生成任务和对比学习，使其在生成类任务和低数据场景中表现优异，解决了 **CLIP** 在某些任务和数据场景下的不足。然而，**CLIP** 在开放领域图像分类和跨模态检索任务中仍表现出色，特别是在大规模数据集下。两者在不同应用场景中各有优势，用户可以根据任务需求选择合适的模型。



# 为什么 Stable Diffusion 选择 CLIP 而非 BLIP？
1. **CLIP 的架构简洁高效**：CLIP 的架构简洁高效，能够以**低计算开销**提供高质量的文本到图像对齐，**BLIP**由于其图像特征**受到了**图文匹配（ITM）和图像条件语言建模(LM)的**影响**。
2. **文本编码高效**：CLIP 的文本编码器经过大规模数据训练，在语义捕捉和泛化能力上表现优秀，适合文本到图像的生成任务。
3. **语义一致性与泛化能力**：CLIP 通过对比学习能够很好地维持文本和图像之间的语义一致性，尤其是在未见过的文本描述下。
4. **轻量、直接，适配生成任务**：相比 BLIP 复杂的多任务学习架构，CLIP 更加轻量、直接，适合 Stable Diffusion 这样的扩散生成模型。

- **BLIP 和 ViT 的局限性**：BLIP 更适合**图像描述生成任务**，ViT 专注于**图像分类**，而它们在基于文本的图像生成中并没有 CLIP 那样专注和高效的表现。
 
# VILT
参考： [深度学习笔记——ViT、ViLT](https://blog.csdn.net/haopinglianlian/article/details/144093215?)



# 历史文章

## 机器学习

[机器学习笔记——损失函数、代价函数和KL散度](https://blog.csdn.net/haopinglianlian/article/details/143831958?)
[机器学习笔记——特征工程、正则化、强化学习](https://blog.csdn.net/haopinglianlian/article/details/143832118?)
[机器学习笔记——30种常见机器学习算法简要汇总](https://blog.csdn.net/haopinglianlian/article/details/143832321)
[机器学习笔记——感知机、多层感知机(MLP)、支持向量机(SVM)](https://blog.csdn.net/haopinglianlian/article/details/143832552)
[机器学习笔记——KNN（K-Nearest Neighbors，K 近邻算法）](https://blog.csdn.net/haopinglianlian/article/details/143832692)
[机器学习笔记——朴素贝叶斯算法](https://blog.csdn.net/haopinglianlian/article/details/143832781?)
[机器学习笔记——决策树](https://blog.csdn.net/haopinglianlian/article/details/143834363)
[机器学习笔记——集成学习、Bagging（随机森林）、Boosting（AdaBoost、GBDT、XGBoost、LightGBM）、Stacking](https://blog.csdn.net/haopinglianlian/article/details/143834494?)
[机器学习笔记——Boosting中常用算法（GBDT、XGBoost、LightGBM）迭代路径](https://blog.csdn.net/haopinglianlian/article/details/143834628)
[机器学习笔记——聚类算法（Kmeans、GMM-使用EM优化）](https://blog.csdn.net/haopinglianlian/article/details/143834707)
[机器学习笔记——降维](https://blog.csdn.net/haopinglianlian/article/details/143834847)

## 深度学习
[深度学习笔记——优化算法、激活函数](https://blog.csdn.net/haopinglianlian/article/details/143835137)
[深度学习——归一化、正则化](https://blog.csdn.net/haopinglianlian/article/details/143835273)
[深度学习——权重初始化、评估指标、梯度消失和梯度爆炸](https://blog.csdn.net/haopinglianlian/article/details/143835336)
[深度学习笔记——前向传播与反向传播、神经网络（前馈神经网络与反馈神经网络）、常见算法概要汇总](https://blog.csdn.net/haopinglianlian/article/details/143835406)
[深度学习笔记——卷积神经网络CNN](https://blog.csdn.net/haopinglianlian/article/details/143841327)
[深度学习笔记——循环神经网络RNN、LSTM、GRU、Bi-RNN](https://blog.csdn.net/haopinglianlian/article/details/143841402)
[深度学习笔记——Transformer](https://blog.csdn.net/haopinglianlian/article/details/143841447)
[深度学习笔记——3种常见的Transformer位置编码](https://blog.csdn.net/haopinglianlian/article/details/144021458)
[深度学习笔记——GPT、BERT、T5](https://blog.csdn.net/haopinglianlian/article/details/144092300)
[深度学习笔记——ViT、ViLT](https://blog.csdn.net/haopinglianlian/article/details/144093215)
[深度学习笔记——DiT（Diffusion Transformer）](https://blog.csdn.net/haopinglianlian/article/details/144094540)

