> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本笔记介绍机器学习中常见的特征工程方法、正则化方法和简要介绍强化学习。

![在这里插入图片描述](https://github.com/GoodnoteX/Ai_Interview/blob/main/机器学习笔记/image/2.jpg)


@[toc]
# 特征工程（Fzeature Engineering）
## 1. 特征提取（Feature Extraction）
**特征提取**：从**原始数据**中**提取**能够有效表征**数据特征**的过程。它将原始数据转换为适合模型输入的特征表示。
### 手工特征提取（Manual Feature Extraction）：
  - **文本数据**：
    - **词袋模型**（Bag of Words）：将文本数据转化为**词频向量**，每个单词是一个维度，值为该单词在文本中出现的次数。
    - **TF-IDF**：为词袋模型加入**词频-逆文档频率**（Term Frequency-Inverse Document Frequency），**降低常见词语的权重**，**提升重要词语的权重**。
    - **N-gram**：将连续的 N 个词作为一个特征，捕捉**词语间的局部依赖关系**。
  - **图像数据**：
    - **边缘检测**：使用 Sobel 算子、Canny 边缘检测等方法提取图像边缘信息。
    - **SIFT（尺度不变特征变换）**：提取图像的关键点和局部特征，具有尺度不变性和旋转不变性。
    - **HOG（方向梯度直方图）**：将图像分块，并统计每块的梯度方向直方图，用于描述局部形状和纹理特征。
  - **时间序列数据**：
    - **移动平均**：对时间序列进行平滑，消除短期波动。
    - **傅里叶变换**：将时间域的信号转化为频域信号，分析数据的周期性。
    - **窗口函数**：将时间序列分为若干窗口，分别计算每个窗口的统计特征，如均值、方差等。

### 自动特征提取（Automated Feature Extraction）：
- 使用**卷积神经网络（CNN）**：从图像中自动提取高级特征，如边缘、纹理、形状等。
- 使用**循环神经网络（RNN）**：处理时间序列数据，捕捉长时间依赖关系。
- 使用**BERT（Transformer）**：通过自监督学习自动提取上下文敏感的文本特征。
- **自动编码器（Autoencoder）**：使用无监督学习从数据中提取低维特征表示，捕捉数据的潜在结构和模式。

## 2. 特征选择（Feature Selection）
特征选择（Feature Selection）是指**从原始特征集中挑选出与目标任务最相关的特征**，以提高模型的性能、减少训练时间以及降低过拟合的风险。特征选择方法主要分为三类：**过滤法（Filter Methods）**、**包裹法（Wrapper Methods）** 和 **嵌入法（Embedded Methods）**。
### 1. 过滤法（Filter Methods）
   - **原理**：独立于模型，**训练前**首先根据某些**统计指标对特征进行评分**，然后选择得分较高的特征。这种方法不依赖于特定的学习算法，只是**基于数据本身的特性进行筛选**。
   - **常见方法**：
     - **方差选择法**：**剔除方差较小的特征**，认为方差小的特征对目标值影响小。
     - **皮尔森相关系数**：计算特征与目标变量之间的线性相关性，**选择线性相关性较高的特征**。
     - **互信息**：衡量特征与目标变量之间的**信息增益**，选择信息量大的特征。
   - **优点**：**计算效率高，易于实现**。
   - **缺点**：未考虑特征之间的相互作用，可能遗漏组合特征的重要性。

### 2. 包裹法（Wrapper Methods）
   - **原理**：在**训练中**，通过训练模型**评估特征子集的表现**，使用搜索策略找到对目标任务最优的**特征组合**。包裹法直接**根据模型的性能进行选择**，通常**通过交叉验证**来评估特征子集的好坏。
   - **常见方法**：
     - **前向选择（Forward Selection）**：从**空集开始**，**逐步添加**对模型性能提升最大的特征。
     - **后向消除（Backward Elimination）**：从**所有特征开始**，**逐步移除**对模型性能影响最小的特征。
   - **优点**：能够**考虑特征之间的相互作用**，适合复杂的特征选择任务。
   - **缺点**：计算开销大，尤其是当特征数目较多时，训练多个模型的过程会非常耗时。

### 3. 嵌入法（Embedded Methods）
   - **原理**：嵌入法**结合了过滤法和包裹法**的优点，直接**在模型训练过程**中**自动选择特征**。它通过学习算法自动选择最重要的特征，使特征选择与模型训练同时进行。
   - **常见方法**：
     - **L1正则化（Lasso回归）**：通过在损失函数中**添加L1正则化项**，使**部分特征的系数变为零，从而进行特征选择。
     - **决策树及其变体（如随机森林、XGBoost）**：树模型的特征重要性得分可以用于选择重要特征。
     - **Elastic Net**：结合L1和L2正则化的优势，在保持模型稀疏性的同时，减少了多重共线性的影响，进行特征选择和模型优化。
   - **优点**：特征选择与模型训练同时完成，考虑特征间的相互作用，效率较高。
   - **缺点**：需要根据特定算法来进行选择，不具有模型无关性。

### 4. 其他方法
   - **PCA（主成分分析）**：虽然PCA是降维方法，但在某些场景下可以间接用于特征选择。通过对数据进行线性变换，将多个原始特征组合成少数几个主成分。
   - **LDA（线性判别分析）**：常用于分类问题的降维，也可以视作一种特征选择方法。
   - **基于稳定性选择（Stability Selection）**：通过在多次子样本集上重复训练模型，并选择那些在多个子集上都表现重要的特征，从而增强选择的鲁棒性。

### 5. 选择方法的应用场景
   - **过滤法**适用于快速预筛选大量特征的情况，计算效率高，但可能丢失特征之间的组合信息。
   - **包裹法**在特征数不多时（例如几十个或上百个）效果较好，能找到最佳的特征组合，但计算开销较大。
   - **嵌入法**通常适用于大多数场景，尤其是使用线性模型（Lasso）或树模型时，既能训练模型又能自动选择特征。

### 总结
下面是特征选择方法的总结表格，保留了原有的描述信息：

| **方法类别**                  | **原理**                                                                                     | **常见方法**                                                                                                                                           | **优点**                                                   | **缺点**                                                     | **适用场景**                                                    |
|---------------------------|--------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------|------------------------------------------------------------|------------------------------------------------------------|
| **过滤法（Filter Methods）**    | 独立于模型，基于统计指标对特征评分，并选择得分较高的特征。                                                                    | - 方差选择法：剔除方差较小的特征<br>- 皮尔森相关系数：选择线性相关性高的特征<br>- 互信息：选择信息增益大的特征                                           | 计算效率高，易于实现                                           | 未考虑特征间相互作用，可能遗漏重要的组合特征                                  | 快速预筛选大量特征的情况，适合初步筛选特征                                   |
| **包裹法（Wrapper Methods）**    | 通过训练模型评估特征子集表现，使用搜索策略找到最优特征组合。                                                                    | - 递归特征消除（RFE）：删除不重要的特征<br>- 前向选择：逐步添加性能提升最大的特征<br>- 后向消除：逐步移除对模型性能影响小的特征                           | 能考虑特征间的相互作用，适合复杂任务                                      | 计算开销大，训练多个模型耗时长                                             | 特征数较少（几十到上百个），适合需要精确特征选择的任务                               |
| **嵌入法（Embedded Methods）**   | 结合过滤法和包裹法的优点，在模型训练过程中选择特征。                                                                        | - L1正则化（Lasso回归）：通过L1正则化项使部分特征系数为零<br>- 决策树及其变体（随机森林、XGBoost）：根据特征重要性评分选择特征<br>- Elastic Net：结合L1和L2正则化 | 特征选择与模型训练同时进行，考虑特征间相互作用，效率高                              | 需要根据特定算法选择，不具有模型无关性                                        | 适合使用线性模型（如Lasso）或树模型的场景，大多数现代复杂模型都适用                     |
| **其他方法**                  | PCA、LDA等方法虽然是降维方法，但可间接用于特征选择。                                                                          | - PCA：通过线性变换将多个特征组合成少数几个主成分<br>- LDA：常用于分类问题的降维方法<br>- 稳定性选择（Stability Selection）：通过在子样本集上选择表现稳定的特征 | 能够进行有效降维，有时可以间接用于特征选择                                   | 降维后特征解释性较弱                                               | 数据维度较高的情况下，可以用作降维手段，间接提高特征选择效果                             |

- **过滤法**：速度快，适合**预处理**大量特征，但可能丢失特征间的组合信息。
- **包裹法**：精度高，适合特征数较少且精度要求高的任务，但计算成本大。
- **嵌入法**：**性能和效率兼顾，适合大多数场景，尤其是使用线性模型（Lasso）或树模型时**。
- **其他方法**：如PCA、LDA等可以作为降维手段，间接用于特征选择，适合高维数据的场景。

选择合适的特征选择方法能够有效提升模型性能，降低训练时间，避免过拟合。

## 3. 特征构造（Feature Construction）

**特征构造**是通过**对已有特征进行组合、变换或生成新特征**来增强模型表达能力的过程。它可以将隐含的关系显式化，提高模型的拟合能力。

| 类别                | 主要方法                                             | 适用场景                                                   |
|---------------------|------------------------------------------------------|------------------------------------------------------------|
| 数值特征构造        | 变换、分箱                                           | 处理数值特征、非线性关系                                   |
| 类别特征构造        | 编码、组合                                           | 处理类别特征、捕捉特征间关系                               |
| 时间特征构造        | 时间提取、周期特征、时间差                           | 时间序列数据、周期性特征                                   |
| 文本特征构造        | 词袋、TF-IDF、词向量、N-grams                        | 文本数据、自然语言处理                                     |
| 特征交互与组合      | 特征交互、多项式特征                                 | 捕捉特征间的复杂关系，适合增强线性模型的非线性拟合能力     |
| 聚合与统计特征      | 聚合、统计、窗口聚合                                 | 大规模表格数据、时间序列数据                               |
| 生成模型特征        | 降维、聚类、自编码器生成特征                         | 复杂高维数据、需要特征压缩的场景                           |
| 特征选择与构造结合  | 筛选后构造、嵌入法生成特征                           | 大规模数据集、特征选择与构造结合的场景                     |

特征构造是一项创造性和技术性并重的任务，需要结合领域知识、数据分析技巧以及机器学习经验来挖掘出更有利于模型训练的特征，从而提升模型的表现。
## 4. 特征缩放

 1. **归一化**：通常是指将数据**缩放到一个特定的范围，如[0, 1]**。目的是**让不同特征的值处于相同的尺度上**，【同时也有消除不同特征量纲的影响的作用】**大范围的特征值可能会导致梯度更新过慢或不稳定**。
 2. **标准化**：是指对数据进行**均值0、标准差1**的变换，**更关注数据的分布形态**。目的是**消除不同特征的物理单位和量纲（如重量、温度、距离等）差异**，同时保持特征间的相对比例关系。




> 详细全文请移步公主号：Goodnote。  
参考：[欢迎来到好评笔记（Goodnote）！](https://mp.weixin.qq.com/s/lCcceUHTrM7wOjnxkfrFsQ)

