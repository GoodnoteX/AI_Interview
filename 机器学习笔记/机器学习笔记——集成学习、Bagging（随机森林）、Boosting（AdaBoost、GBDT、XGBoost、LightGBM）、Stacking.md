> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本笔记介绍机器学习中常见的集成学习：Bagging和Boosting。

![在这里插入图片描述](https://github.com/GoodnoteX/Ai_Interview/blob/main/机器学习笔记/image/8.png)


---

>@[toc]
---
# 集成学习（Ensemble Learning）概述
集成学习是一种**通过组合多个模型（通常是弱模型）来提升整体预测性能的技术**，主要分为 **Bagging** 和 **Boosting** 两类方法。Bagging 和 Boosting 是构建强大的集成模型的基础，并衍生出了多种经典的算法，如随机森林、AdaBoost、Gradient Boosting、XGBoost 和 GBDT。
## Bagging 和 Boosting 的对比

| **属性**           | **Bagging**                      | **Boosting**                   |
|--------------------|----------------------------------|--------------------------------|
| **模型训练方式**   | **并行**训练多个模型                 | **串行**训练多个模型               |
| **每次训练的数据集** | 每次从原始数据集进行**有放回随机采样**生成不同的**子集** | 每次使用**相同的数据集**，但样本的权重会随错误分类情况调整 |
| **关注样本方式**   | 每个模型使用**相同的样本权重**        | **不同的权重**，后续模型重点关注前一模型的错误 |
| **结果集成方式**   | 平均（回归）或投票（分类）        | 加权组合模型                   |
| **主要优点**       | 降低方差，**减少过拟合**             | 降低偏差，提高模型精度，**减少欠拟合**         |
| **主要缺点**       | 对偏差问题无明显改善             | 对噪声敏感，训练时间长         |
| **代表算法**       | 随机森林                         | AdaBoost、Gradient Boosting    |

---

- **Bagging**：通过并行训练多个模型，降低模型的方差，更适合**高方差模型**（减轻过拟合），如随机森林是其经典应用。
- **Boosting**：通过串行训练多个模型，逐步提升模型性能，更适合提升**高偏差模型**（减轻欠拟合），经典算法包括 AdaBoost 和 Gradient Boosting。

> 高方差是过拟合，高偏差是欠拟合

---
# Bagging 算法
## 什么是 Bagging？
Bagging（Bootstrap Aggregating）是一种**并行集成学习方法**，它通过**在训练集中多次随机采样（有放回的抽样）生成多个不同的子集，并在每个子集上训练模型，最终通过平均（回归）或投票（分类）得到集成模型的预测结果**。
## Bagging 的主要特点
- **并行训练**：每个模型是独立训练的，因此可以并行计算。
- **有放回的随机采样**。
- **减小方差，降低过拟合**：通过组合多个模型，可以有效降低模型的方差，提高稳定性。
- **不易过拟合，容易欠拟合**：对于不稳定的模型（如决策树），Bagging 能有效减少过拟合的风险。
## 常用的 Bagging 算法
### 随机森林（Random Forest）
  - 随机森林是在 Bagging 的基础上，对每个子模型（决策树）进一步**随机选择特征**进行训练。它通过**引入特征选择的随机性，进一步减少了模型的方差，提高了模型的泛化能力**。
  - **工作原理**：
    1. 从训练集中随机**有放回地抽样**，生成多个子集。
    2. 对**每个子集训练**一个决策树模型。
    3. 在每棵树的节点划分时，**随机选择部分特征**进行划分。
    4. 对于**分类任务**，通过所有树的**投票结果**决定最终的类别；对于**回归任务**，通过**所有树的平均值**决定最终的预测值。

# Boosting 算法
## 什么是 Boosting？
Boosting 是一种**串行集成**学习方法，它通过**逐步训练一系列弱模型，每个模型都试图纠正前一个模型的错误预测**。最终的**集成模型是所有弱模型的加权和**。
## Boosting 的主要特点
- **串行训练**：模型是依次训练的，后一个模型依赖于前一个模型的结果。
- **减小偏差，减轻欠拟合**：通过加权训练，Boosting 能够逐步减小模型的偏差。
- **易于过拟合**：由于模型不断地优化错误样本，容易在训练数据上表现过好，需要使用正则化等技术来防止过拟合。
## 常用的 Boosting 算法

| **算法**    | **原理**                                            | **优点**                                    | **缺点**                                      | **使用场景**                   |
|-------------|-----------------------------------------------------|---------------------------------------------|-----------------------------------------------|--------------------------------|
| **AdaBoost** | 通过**调整样本权重**，训练多个弱分类器，并**加权组合**。 | 简单易实现，适合分类任务。                  | **对噪声敏感**，容易过拟合，需控制弱分类器数量。 | 分类任务，如垃圾邮件分类等。   |
| **GB**      | **泛指梯度提升框架**，通过**梯度下降拟合残差**优化模型。   | 提升模型精度，减少偏差，适合分类和回归任务。 | 训练时间较长，容易过拟合。                    | 分类和回归，非线性数据场景。   |
| **GBDT**    | **基于决策树的梯度提升方法**，通过拟合残差优化模型。     | 强大回归能力，处理复杂非线性关系。          | 训练时间长，需调参（树的数量、学习率等）。    | 分类和回归，复杂特征场景。     |
| **XGBoost** | **GBDT 的增强版**，引入**正则化、并行化和缺失值处理**等优化。 | 训练速度快，性能好，适用于大规模数据集。     | 模型复杂，超参数调优困难。                    | 大规模数据下的分类和回归任务。 |
| **LightGBM**| **GBDT 的优化版**，基于**直方图算法**和**叶子节点生长策略**。  | 训练速度极快，内存占用少，适合大数据集和高维数据 | **对小数据集不适合**，Leaf-wise 策略可能导致过拟合，需调参。        | 大规模数据下的分类、回归和排序任务。|

- **AdaBoost**：适合**分类任务**，通过**加权样本聚焦错分类**，但容易过拟合。
- **GB**：**梯度下降框架**，适合分类和回归，但训练慢。
- **GBDT**：是 **GB 框架的一个具体实现**，专用**决策树做弱学习器**，处理复杂关系，训练时间长。
- **XGBoost**：GBDT 的增强版，速度快，适合大规模数据，但需复杂调参。
- **LightGBM**：基于 GBDT 的改进算法，通过直方图方法加速训练，适合大规模数据和高维特征场景，具有极快的训练速度，但容易过拟合，需调参。
---
### AdaBoost（Adaptive Boosting）






> 详细全文请移步公主号：Goodnote。  
参考：[欢迎来到好评笔记（Goodnote）！](https://mp.weixin.qq.com/s/lCcceUHTrM7wOjnxkfrFsQ)
