> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本文主要阐述Boosting中常用算法（GBDT、XGBoost、LightGBM）的迭代路径。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/649b8a9130ca4c5fa3d9337e61a22e15.png#pic_center)

---
> @[toc]
---
# XGBoost 相对 GBDT 的改进
**GBDT（Gradient Boosting Decision Tree，梯度提升决策树）** 是一种集成学习算法。GBDT 使用**梯度提升（Gradient Boosting）**的思想，每一棵**决策树**都是基于前一轮预测的**残差**（即误差）来训练的，从而逐步逼近真实值。

**XGBoost** 相对传统 GBDT 在原理和实现上进行了多项改进，使得它在计算效率、模型精度、内存管理和并行性等方面有显著提升。以下是 XGBoost 相对 GBDT 的关键改进：

| 改进项                  | 具体描述                                                   | 优势                               |
|-------------------------|------------------------------------------------------------|------------------------------------|
| **正则化项**            | 引入 \($L_1$\) 和 \($L_2$\) 正则化控制叶节点权重                 | 防止过拟合，提高泛化能力           |
| **二阶导数信息**        | 使用梯度和 Hessian 信息进行优化                              | 提高收敛速度和精度                 |
| **列采样和行采样**      | 每棵树采样特征和样本                                        | 降低过拟合风险，提高泛化性         |
| **并行化处理**          | 特征分片和直方分裂，支持 GPU 加速                           | 提升训练速度                       |
| **缺失值处理**          | 自动选择缺失样本最佳分裂方向                                | 处理缺失值和稀疏数据               |
| **早停和学习率衰减**    | 监控验证集性能，学习率衰减控制每棵树贡献                     | 降低过拟合和节省计算开销           |
| **自定义目标和评估**    | 支持用户自定义目标函数和评估指标                             | 提高适应性，满足不同场景需求       |

## 引入正则化项，防止过拟合
在 GBDT 中，每棵树的叶子节点权重没有额外的正则化控制，容易导致模型过拟合。**XGBoost** 在每棵树的目标函数中**引入了 \( $L_1$ \) 和 \( $L_2$ \) 正则化项**，控制叶节点数量和权重大小，使模型更具泛化能力。目标函数为：

$$
\text{Obj} = \sum_{i=1}^N L(y_i, \hat{y}_i) + \sum_{m=1}^M \Omega(f_m)
$$

### 损失函数 $L(y_i, \hat{y}_i)$

损失函数 $L(y_i, \hat{y}_i)$ 测量每个样本的预测误差。例如，常用的损失函数有：
- **均方误差 (MSE)**：用于回归问题，定义为 $L(y_i, \hat{y}_i) = \frac{1}{2}(y_i - \hat{y}_i)^2$。
- **对数损失 (Log Loss)**：用于二分类问题。$L(y, \hat{y}) = - \left( y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - \hat{y}) \right)$

### 正则化项 $\Omega(f_m)$
正则化项 $\Omega(f_m)$ 用于控制模型复杂度，包含 L1 和 L2 正则化：

$$
\Omega(f_m) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_j^2 + \alpha \sum_{j=1}^T |w_j|
$$

其中：
- $T$ 是树的叶节点数目。
- $w_j$ 是第 $j$ 个叶节点的权重。
- $\gamma$ 控制叶节点的数量，较大的 $\gamma$ 倾向于减少叶节点数量，使模型更简单。
- $\lambda$ 是 L2 正则化系数，控制叶节点权重的平方和，有助于平滑叶节点的权重。
- $\alpha$ 是 L1 正则化系数，控制叶节点权重的绝对值之和，促使部分权重趋向于零，从而构建更稀疏的模型。

## 使用二阶导数信息，加速收敛
GBDT 只使用损失函数的一阶导数（负梯度）来更新模型，而 XGBoost 通过泰勒展开，将损失函数对模型输出进行二阶展开，使用**一阶导数（梯度）和二阶导数（Hessian）** 来**构建树**，优化效果更佳。**二阶导数带来更精确的梯度信息来改进模型的更新，使得模型能够更快地收敛**。目标函数的二阶近似为：
$$
\text{Obj}^{(m)} \approx \sum_{i=1}^N \left[ g_i f_m(x_i) + \frac{1}{2} h_i f_m(x_i)^2 \right] + \Omega(f_m)
$$

其中：
- $g_i = \frac{\partial L(y_i, \hat{y}_i)}{\partial \hat{y}_i}$ 是损失函数的一阶导数（梯度）。
- $h_i = \frac{\partial^2 L(y_i, \hat{y}_i)}{\partial \hat{y}_i^2}$ 是损失函数的二阶导数（Hessian）。

### 一阶导数与二阶导数的区别

 1. 一阶导数（梯度）：**表示损失函数的斜率**，指向使损失减少的方向，即当前点处的损失下降趋势。梯度**通常用于确定优化的方向**。
 2. 二阶导数（Hessian）：**表示一阶导数的变化率**，反映了**损失曲面的曲率信息**。 Hessian 的大小可以**帮助判断步长的合适大小**，使得更新**更加精确**。
### 二阶导数的优势
1. 更准确的步长控制：根据损失曲率调整步长，避免步长过大或过小。 
2. 更快速的收敛：二阶导数的曲率信息能帮助模型更快接近最优解，减少迭代次数。
3. 更稳定的优化过程：使得模型在平坦或复杂的损失函数下依然能够有效地更新。

#### 1. 更准确的步长选择，避免过大或过小的更新
使用二阶导数信息可以调整步长，使得模型更新在合适的尺度上进行。步长过大会导致优化震荡或跳过最优解，而步长过小会导致优化缓慢。引入二阶导数可以：

 - 自动缩小步长：在损失曲面曲率较大的地方，二阶导数值较大，步长会相应减小，避免更新过大。
 - 自动增大步长：在曲率较小的地方，二阶导数值较小，步长会相应增大，使得优化更快速。

这一点**在非线性损失函数中尤其重要**，因为仅靠一阶导数的信息无法准确反映损失的局部曲率。

#### 2. 更快速的收敛
二阶导数提供了**额外的曲率信息**，优化过程能够更接近牛顿法的优化效果。**牛顿法是一种使用二阶导数进行快速收敛的方法**，在每次迭代中更新量为：

$$\Delta x = - \frac{梯度}{二阶导数} $$

对于大多数损失函数，二阶导数的加入能显著加速收敛，减少所需的迭代次数。相较于仅使用梯度的信息，这种方法**能够更快地找到损失函数的最优解**，提升模型的训练效率。

#### 3. 更稳定的模型优化
在优化中，梯度可能会在平坦区域出现较小的值，导致步长极小，进展缓慢，而二阶导数能够平衡这一影响。在 XGBoost 中，使用一阶和二阶导数的信息构建目标函数，可以更好地处理一些曲面平坦或复杂的损失函数，保持更新的稳定性。

XGBoost 使用二阶泰勒展开将损失函数近似表示为一阶导数和二阶导数的组合。
分裂增益的计算公式也包含二阶导数，这种基于二阶导数的增益计算能够更准确地评估分裂效果，从而选择使得损失减少最大的分裂点。

## 支持列采样和行采样，提升模型的泛化能力
XGBoost 支持列采样（Column Subsampling）和行采样（Row Subsampling），即在每次构建树时**随机选取**一部分**特征**和**样本**，而**不是使用全部特征/样本**，类似于随机森林的特征采样。具体来说XGBoost 的采样方式包括：

 1. 行采样：在每棵树的构建过程中**随机选择部分样本**。 
 2. 列采样：在每次分裂节点时，**随机选择部分特征**。

这种采样策略有助于降低模型的方差和过拟合风险，提高泛化能力。

是的，**XGBoost** 支持**列采样（Column Subsampling）**和**行采样（Row Subsampling）**，这两种采样方式能够提高模型的泛化能力，减少过拟合，同时提升训练速度。

### 行采样（Row Subsampling）

- **含义**：在训练每棵树时，XGBoost会从训练数据集中**随机采样部分样本**（行）来训练该树，而不是使用全部样本。
- **参数**：`subsample`
  - **取值范围**：0 到 1。
  - **作用**：`subsample=0.8` 表示在训练每棵树时，随机采样 80% 的训练数据。
  - **优点**：通过减少训练数据量，加快了每棵树的训练速度，并且增加了模型的随机性，降低了过拟合风险。
  
### 列采样（Column Subsampling）

- **含义**：在训练每棵树或每个分裂节点时，XGBoost会**随机采样部分特征**（列）来进行训练，而不是使用全部特征。
- **参数**：XGBoost 提供了三种不同的列采样方式。
  - **`colsample_bytree`**：在构建每棵树时，选择一部分特征进行训练。默认值是 1（即使用全部特征）。
  - **`colsample_bylevel`**：在**每一层节点分裂**时，随机采样部分特征。
  - **`colsample_bynode`**：在**每个节点进行分裂**时，随机采样部分特征。
- **取值范围**：0 到 1。
  - 例如，`colsample_bytree=0.7` 表示在构建每棵树时，随机采样 70% 的特征。
- **优点**：通过减少特征的使用，增加了模型的多样性，进一步降低了过拟合风险，特别是在高维数据集上效果显著。

### 行采样与列采样的结合
XGBoost 支持将行采样和列采样结合使用，在训练过程中对样本和特征都进行随机采样，进一步提高模型的泛化能力。通过调节 `subsample` 和 `colsample_bytree`（或 `colsample_bylevel`、`colsample_bynode`）的值，找到一个在模型准确性和训练效率之间的平衡点。可以控制模型的复杂度和训练速度。

### 优势总结
- **降低过拟合**：通过引入随机性，行采样和列采样都能够减少模型对训练数据的依赖，防止模型过度拟合训练数据。
- **加快训练速度**：采样减少了模型在训练每棵树时使用的数据量和特征数量，从而加快了模型训练过程，尤其是在大数据集上。
- **增强模型的泛化能力**：适当的采样可以提高模型的泛化能力，提升在测试数据上的表现。
## 并行化处理，提升训练速度
传统的 GBDT 中，树的分裂点查找是顺序完成的，而 XGBoost 在此基础上进行了并行化优化。**XGBoost 的并行化**通过**特征分片**和**直方分裂**实现：

1. **特征分片**：将特征划分为多个分片，每个分片分配给一个线程或进程独立计算增益，最终选择增益最大的分裂点。
2. **直方分裂算法**：将连续特征值离散化成多个桶，通过桶的累计梯度和二阶导数和加速增益计算。直方分裂算法不仅减少计算量，还能够借助 GPU 进一步提升计算速度。
> 尽管最初版本的 XGBoost 并没有直方分裂算法这一功能。为了提升在大规模数据集上的训练效率，XGBoost 后来引入了基于直方图的分裂方法（直方分裂算法），与 LightGBM 类似。
### 结合特征分片与直方分裂的并行化优势
- **提高计算效率**：特征分片使得特征的增益计算可以并行执行，而直方分裂减少了增益计算的复杂度。两者结合可以显著加快分裂节点的速度。
- **降低内存消耗**：直方分裂通过离散化特征值减少了内存占用，适合大规模数据集。
- **适合大规模数据**：这种并行化方法使 XGBoost 在处理大规模数据时具有明显的速度优势，是 XGBoost 高效表现的关键因素之一。
### LightGBM 和 XGBoost 中直方分裂的差异
虽然 XGBoost 和 LightGBM 都使用直方分裂算法，但两者在实现上存在一些差异：

1. 默认使用：**LightGBM 从一开始就使用直方分裂方法**，并将其作为默认的分裂方法。而在 **XGBoost** 中，直方分裂并不是默认方式，**用户可以选择是否启用**。
2. 特征分桶方式：LightGBM 使用了更高级的**互斥特征绑定**（Exclusive Feature Bundling, EFB）来进一步优化直方分裂的内存使用；XGBoost 则没有使用此技术。
3. 树生长策略：**LightGBM 使用叶子生长策略**（Leaf-wise Growth Strategy），增益计算用于决定叶节点的分裂，而 **XGBoost 使用层级生长策略**（Level-wise Growth Strategy），增益计算用于分裂同一层的所有节点。

## 支持缺失值处理和稀疏特征优化
XGBoost 提供了对缺失值和稀疏特征的优化处理。对于数据集中缺失的特征，XGBoost 通过如下方式处理：

1. 自动分裂方向：在构建树时，XGBoost **自动决定缺失值样本应归入左子树还是右子树，选择带来最高增益的方向**。
2. 稀疏特征支持：XGBoost 支持稀疏输入矩阵，这在缺失值较多的场景下大大减少了存储和计算的开销。

## 提供早停机制和学习率衰减，控制训练过程
XGBoost 支持早停机制（Early Stopping），即通过在验证集上监控模型的表现，当验证集性能不再提升时自动终止训练。这减少了不必要的计算开销，节省了时间。此外，XGBoost 支持学习率衰减（Shrinkage），在每一轮的更新中衰减当前树的权重，平滑模型，提高泛化性能：

$$\hat{y}_i^{(m)} = \hat{y}_i^{(m - 1)} + \eta \cdot f_m(x_i)$$
其中，$\eta$ 是学习率，控制每棵树的贡献大小。


---

# LightGBM 相较于 XGBoost 的优势
以下是 LightGBM 和 XGBoost 在每个关键特性上的实现方式对比。每个步骤详细说明两者的具体实现差异，以便更好地理解它们的不同之处。

## 训练速度

- **LightGBM**：  
  - **直方分裂算法**：LightGBM 使用直方分裂算法，将连续特征值划分为固定数量的桶（如 256 个），然后在这些桶上计算增益。这大幅减少了计算量，因为只需要在桶级别计算增益，而非对所有特征值逐一计算。
  - **叶子生长策略**：LightGBM 使用叶子生长策略，每次优先分裂增益最大的叶节点。这使得树能够在更少的深度下达到更高的拟合效果，同时也减少了计算量。
  - **多线程并行支持**：LightGBM 支持多线程并行计算，在特征分片和桶级别上进行分裂增益计算，充分利用 CPU 资源。

- **XGBoost**：  
  - **直方分裂（后续版本引入）**：早期版本的 XGBoost 没有直方分裂算法，而是遍历每个可能的分裂点。后续版本引入了直方分裂算法，但不是默认的分裂方式。
  - **层级生长策略**：XGBoost 使用层级生长策略，即在每一层同时分裂所有节点。这种方法会构建出更平衡的树，但相对需要更大的树深度，计算开销也较大。
  - **多线程并行支持**：XGBoost 也支持多线程并行计算，但由于层级生长策略在分裂节点时存在更高的依赖性，在多线程的效率上通常低于 LightGBM。

**总结**：LightGBM 的直方分裂算法和叶子生长策略使其训练速度通常快于 XGBoost，尤其是在大规模数据上。


### 内存效率
- **LightGBM**：  
  - **互斥特征绑定（EFB）**：LightGBM 使用 EFB 技术，将一些互斥的稀疏特征捆绑在一起，减少特征维度并降低内存使用。EFB 可以显著减少特征存储需求，尤其适合高维稀疏数据。
  - **原生支持稀疏特征**：LightGBM 对稀疏特征矩阵和缺失值有原生支持。在计算分裂增益时能够跳过缺失值，避免存储和计算的冗余。

- **XGBoost**：  
  - **没有 EFB 支持**：XGBoost 没有互斥特征绑定技术，因此在处理稀疏高维特征时，内存使用更高。
  - **稀疏矩阵支持（数据填充方式不同）**：XGBoost 通过填充值的方式处理稀疏特征，在内存使用上相对高于 LightGBM，但也能有效处理缺失数据。

**总结**：LightGBM 的 EFB 技术和稀疏特征原生支持使其在处理高维稀疏数据时更节省内存，而 XGBoost 缺乏这一优化。


## 类别特征支持

- **LightGBM**：  
  - **原生类别特征支持**：LightGBM 允许用户直接指定类别特征。训练时无需将类别特征独热编码（One-Hot Encoding），而是通过增益计算自动选择最佳的类别组合分裂方式。这种方式不仅保留了类别特征的原始信息，还减少了内存和计算量。

- **XGBoost**：  
  - **不支持原生类别特征**：XGBoost 不支持直接处理类别特征，必须对类别特征进行独热编码或标签编码。这导致数据维度增大，计算和内存开销增加。

**总结**：LightGBM 能原生处理类别特征，节省内存和提升速度，而 XGBoost 需要手动对类别特征进行编码。


## 扩展性和分布式训练支持

- **LightGBM**：  
  - **分布式训练**：LightGBM 自带分布式支持，能够在多个节点上实现高效的分布式计算，适合在集群上处理超大数据集。
  - **GPU 加速**：LightGBM 支持 GPU 加速，特别适用于大数据集。其 GPU 实现主要基于直方分裂算法，对高并发的 GPU 硬件优化良好。

- **XGBoost**：  
  - **分布式训练**：XGBoost 也支持分布式训练，但在集群上的效率通常低于 LightGBM，因为其层级生长策略对分布式环境要求较高。
  - **GPU 加速**：XGBoost 也支持 GPU 加速，GPU 加速在基于直方分裂的计算上表现出色。但在大数据集上，LightGBM 的 GPU 加速往往效率更高。

**总结**：在分布式训练上，LightGBM 对超大规模数据的支持更高效；GPU 加速在两者中均有支持，但 LightGBM 的表现通常更快。


## 早停机制和自动调参

- **LightGBM**：  
  - **早停机制**：LightGBM 支持灵活的早停机制，能够在验证集上监控模型性能，若性能不再提升则提前停止训练。尤其在多线程和分布式训练时，LightGBM 的早停机制具有较好的效果。
  - **自动调参**：LightGBM 支持对一些关键参数（如叶节点数量、分裂增益阈值等）的自动调节，用户可以通过参数控制训练的精度和速度，简化调参过程。

- **XGBoost**：  
  - **早停机制**：XGBoost 也支持早停机制，用户可以通过验证集的性能表现来提前终止训练。其早停机制虽然有效，但在多线程环境下的表现通常不如 LightGBM 稳定。
  - **自动调参**：XGBoost 没有自动调参机制，用户通常需要手动进行超参数调节，在调整分裂增益阈值等关键参数时更为繁琐。

**总结**：LightGBM 提供了更灵活的早停机制和自动调参功能，XGBoost 虽然也支持早停，但在自动调参上更依赖用户手动设置。

## 过拟合控制

- **LightGBM**：  
  - **多种正则化参数**：LightGBM 通过设置最大深度、叶节点数、最小增益阈值等，细致控制模型的复杂度，防止模型过拟合。这些参数特别适合在叶子生长策略下防止模型的过拟合。
  - **数据采样**：LightGBM 支持行采样和列采样，以降低模型的方差并增强泛化能力。

- **XGBoost**：  
  - **正则化控制**：XGBoost 通过引入 \( L_1 \) 和 \( L_2 \) 正则化项控制叶节点权重，防止过拟合。虽然在控制过拟合上有效，但层级生长策略下的正则化效果不如 LightGBM 灵活。
  - **数据采样**：XGBoost 也支持行和列的随机采样，帮助控制模型的方差，但在设置上不如 LightGBM 直观。

**总结**：LightGBM 的正则化控制更加灵活，能够更好地控制过拟合，而 XGBoost 的层级生长策略对深度的控制不如 LightGBM。

### 总结

| 特性                   | LightGBM 优势                                                         | XGBoost 实现特点                                                |
|------------------------|-----------------------------------------------------------------------|-----------------------------------------------------------------|
| **训练速度**           | 直方分裂、叶子生长策略、多线程优化提升速度                              | 引入直方分裂（非默认），层级生长策略，多线程效果不及 LightGBM     |
| **内存效率**           | 互斥特征绑定和稀疏特征原生支持                                         | 没有 EFB，稀疏矩阵通过填充处理，内存效率低于 LightGBM           |
| **类别特征支持**       | 原生支持类别特征，节省内存，提升速度                                   | 不支持原生类别特征，需独热编码，内存和计算量增加                 |
| **扩展性和分布式训练** | 更高效的分布式支持和 GPU 加速                                         | 支持分布式和 GPU，但效率通常低于 LightGBM                        |
| **早停和自动调参** | 灵活的早停机制，自动调参简化调参过程                                   | 早停效果稍弱，缺乏自动调参，需手动设置                           |
| **过拟合控制**         | 更细粒度的参数控制，有效防止叶子生长策略的过拟合                        | L1 和 L2 正则化控制权重，层级生长策略对深度的控制较差             |

总体而言，LightGBM 相较于 XGBoost 在大数据处理、类别特征支持、内存优化和训练速度方面具有明显优势，特别适合处理大规模、高维稀疏数据的应用。



# 历史文章
本系列其他相关笔记参考如下：
[机器学习笔记——损失函数、代价函数和KL散度](https://blog.csdn.net/haopinglianlian/article/details/143831958?)
[机器学习笔记——特征工程、正则化、强化学习](https://blog.csdn.net/haopinglianlian/article/details/143832118?)
[机器学习笔记——30种常见机器学习算法简要汇总](https://blog.csdn.net/haopinglianlian/article/details/143832321)
[机器学习笔记——感知机、多层感知机(MLP)、支持向量机(SVM)](https://blog.csdn.net/haopinglianlian/article/details/143832552)
[机器学习笔记——KNN（K-Nearest Neighbors，K 近邻算法）](https://blog.csdn.net/haopinglianlian/article/details/143832692)
[机器学习笔记——朴素贝叶斯算法](https://blog.csdn.net/haopinglianlian/article/details/143832781?)
[机器学习笔记——决策树](https://blog.csdn.net/haopinglianlian/article/details/143834363)
[机器学习笔记——集成学习、Bagging（随机森林）、Boosting（AdaBoost、GBDT、XGBoost、LightGBM）、Stacking](https://blog.csdn.net/haopinglianlian/article/details/143834494?)
