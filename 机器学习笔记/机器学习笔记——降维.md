> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本笔记介绍机器学习中常见的无监督学习方法——降维。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/66c70de196a84a958553f5893690abf7.png#pic_center)

---

> @[toc]
---
# 降维方法概述

**降维**目的是将**高维**数据`映射到`**低维**空间中，同时**尽量保留数据的主要信息**。降维可以**减少数据冗余**、**降低计算复杂度**、**减轻过拟合**风险，并帮助我们更好地理解和可视化数据。降维方法主要分为**线性降维**和**非线性降维**两类。

# 线性降维方法

## 定义
线性降维方法假设数据可以通过**线性变换从高维空间映射到低维空间**。这类方法适用于**数据具有线性结构/线性分布**的情况。

## 特点
- **线性变换**：通过线性变换（如矩阵乘法）来将高维数据映射到低维空间。
- **全局结构**：关注数据的全局结构，试图在降维过程中保留整体几何和统计特性。
- **计算简单**：计算复杂度低，结果具有良好的可解释性。
- **局限性**：对具有复杂非线性结构的数据表现不佳。

## 常见方法
### PCA（Principal Component Analysis，主成分分析）
#### 原理
PCA 是一种**无监督**的**线性降维方法**，目的是通过**寻找数据中方差最大的投影方向**，将数据从高维空间映射到一个低维空间。PCA 的基本思想是：**最大化数据在降维空间中的方差**，即使得降维后的数据尽可能地保留原始数据的信息。
#### 步骤
1. **数据中心化**：将数据矩阵的每一列减去其均值，使数据均值为零。
2. **计算协方差矩阵**：对于 $n \times p$ 的数据矩阵 $X$，计算协方差矩阵 $C = \frac{1}{n-1} X^T X$。
3. **特征值分解**：对协方差矩阵 $C$ 进行特征值分解，得到特征值和对应的特征向量。特征值表示主成分的方差，特征向量表示主成分的方向。
4. **选择主成分**：选择前 $k$ 个最大特征值对应的特征向量，作为新的低维空间的基向量。
5. **数据投影**：将原始数据投影到选定的主成分上，得到降维后的数据。
#### 降维后的数据
经过降维后：
$$
Y = X W
$$

- **$X$**：**原始数据矩阵**，包含 $n$ 个样本和 $p$ 个特征。
- **$W$**：包含**前 $k$ 个主要特征向量的矩阵**，这些特征向量对应于最大特征值，表示数据的主要方向。
- **$Y$**：**降维后的数据矩阵**，表示将原始数据从 $p$ 维空间投影到 $k$ 维空间后的结果。

通过将 $X$ 与 $W$ 相乘，得到的 $Y$ 可以保留数据的主要特征信息，同时减少维度，便于后续分析。

#### 优点
- **最大化方差**：PCA 能找到数据方差最大的方向，并将数据投影到这些方向上，尽可能保留数据的主要信息。
- **降维效果好，计算简单**：只涉及矩阵分解和简单的线性代数运算，计算效率高，同时保持了数据的主要结构。

#### 缺点
- **无法处理非线性关系**：PCA 假设数据是线性分布的，无法处理非线性关系的数据。
- **对离群点敏感**：PCA 使用的协方差矩阵会受到离群点的影响，因此对异常值敏感。
- **解释性差**：主成分是线性组合，不总是容易解释为原始特征的物理含义。

#### 应用场景
- **数据降维**：在高维数据中提取主要特征，例如图像压缩、降维后用于可视化。
- **噪声消除**：通过保留方差较大的主成分，消除方差较小的噪声成分。
- **特征提取**：在分类或回归问题中，通过 PCA 提取主要特征用于建模。


### LDA（Linear Discriminant Analysis，线性判别分析）
#### 原理
LDA 是一种**有监督**的**线性降维方法**，目的是通过一个线性变换，**寻找一个最佳的投影方向**，使得变换后的**数据集类内差异最小，类间差异最大**。
> 在对比学习（Contrastive Learning）中，模型尝试**最大化**同类样本的相似性，同时**最小化**不同类样本之间的相似性。
#### 步骤
- **计算类内散布矩阵 $S_W$**：**反映同一类数据的分散程度**，类内散布矩阵的定义为各类别协方差矩阵的加权和：
  
  $$
  S_W = \sum_{i=1}^c S_i = \sum_{i=1}^c \sum_{x \in C_i} (x - \mu_i)(x - \mu_i)^T
  $$
  
  其中，$c$为类别数，$\mu_i$为第$i$类的均值向量。

- **计算类间散布矩阵 $S_B$**：**反映不同类别之间的分散程度**，类间散布矩阵的定义为各类别均值向量与总体均值向量的协方差矩阵：

  $$
  S_B = \sum_{i=1}^c n_i (\mu_i - \mu)(\mu_i - \mu)^T
  $$

  其中，$n_i$为第$i$类的样本数目，$\mu$为所有样本的均值向量。

- **求解广义特征值问题**：通过**求解$S_W^{-1} S_B$**的**特征值和特征向量**来找到最佳的投影方向。

- **选择投影方向**：选择前$k$个特征值对应的特征向量，作为新的低维空间的基向量。

- **数据投影**：将原始数据投影到这些基向量上，得到降维后的数据。


####  降维后的数据

经过降维后：
$$
Y = X W
$$

- **$X$**：**原始数据矩阵**，包含 $n$ 个样本和 $p$ 个特征。
- **$W$**：包含**前 $k$ 个主要特征向量的矩阵**，这些特征向量对应于最大特征值，表示数据的主要方向。
- **$Y$**：**降维后的数据矩阵**，表示将原始数据从 $p$ 维空间投影到 $k$ 维空间后的结果。

通过将 $X$ 与 $W$ 相乘，得到的 $Y$ 可以保留数据的主要特征信息，同时减少维度，便于后续分析。


#### 优点
- **区分性强**：LDA 利用类别信息，能够有效找到最有利于区分类别的投影方向。
- **降维与分类结合**：LDA 同时实现了降维和分类，适合用于分类任务中的特征提取。

#### 缺点
- **对非线性数据效果较差**：LDA 假设数据类别之间是线性可分的，对非线性数据效果较差。
- **需要类别标签对类别分布敏感**：需要类别标签，类别不均衡时效果不佳，LDA 假设各类别的协方差矩阵相同，若类别分布差异较大，效果可能较差。
> 处理类别不均衡的技巧：
> 1. 类权重调整：可以通过为**少数类别赋予更高的权重**，使 LDA 在投影时更多地考虑少数类别的分布。权重调整可以让模型对少数类别的数据给予更多的关注，减少类别不平衡的影响。
> 2. 重采样：在进行 LDA 之前，可以通过**欠采样多数类**或过**采样少数类**的方式来平衡类别分布。这样可以使模型在降维时更加均衡地考虑各个类别的特征。

>过采样（Oversampling）少数类：通过**复制少数类样本**，增加少数类样本的数量，使得少数类与多数类样本数量相当。常见的方法有 SMOTE（Synthetic Minority Over-sampling Technique），它通过生成少数类样本的合成数据来平衡数据集。
>欠采样（Undersampling）多数类：**减少多数类样本数量**，使其与少数类样本数量接近。虽然会丢失部分多数类数据，但能防止多数类对降维结果的主导。
#### 应用场景
- **分类任务中的降维**：在有标签数据中，用于降维和分类建模，例如人脸识别、文本分类等。
- **小样本问题**：在样本较少而特征较多的分类问题中，LDA 可以减少维度，缓解过拟合问题。


### SVD（Singular Value Decomposition，奇异值分解）——LORA使用
#### 原理
SVD 是一种**矩阵分解技术**，它将一个矩阵分解为**三个矩阵的乘积**： 
1. **左奇异矩阵**
2. **对角矩阵**
3. **右奇异矩阵**

> SVD 是一种广义的矩阵分解方法，应用广泛，特别是在降维、数据压缩和矩阵填充(如推荐系统)中。SVD 在数学上等价于 PCA，但应用更加灵活
> 
> 
> 

> 
> 在LoRA中，原始权重矩阵$W$的变化$\Delta W$被表示为：
>  $$\Delta W = A \times B$$
> 
> 其中：
> - $A$是一个低秩矩阵，其维度为$d \times r$，表示数据从高维空间（维度$d$）映射到一个低秩空间（维度$r$）。
> - $B$是另一个低秩矩阵，其维度为$r \times k$，表示从低秩空间再映射回到原始空间。


#### 步骤

实现步骤如下：

- **矩阵分解**：对**数据矩阵进行奇异值分解，得到 U、Σ 和 V 矩阵**。对于任意矩阵 $X \in \mathbb{R}^{m \times n}$，SVD 将其分解为三个矩阵的乘积：
$$
  X= U \Sigma V^T
$$
  其中：
  - $U$ 是 $m \times m$ 的正交矩阵，称为左奇异向量矩阵。
  - $\Sigma$ 是 $m \times n$ 的对角矩阵，对角元素为**奇异值**，按从大到小排列。
  - $V$ 是 $n \times n$ 的正交矩阵，称为右奇异向量矩阵。

- **奇异值与奇异向量**：**计算奇异值及对应的奇异向量。**
  - 奇异值是矩阵 $X$ 的特征值，可以反映矩阵的固有性质。
  - 左奇异向量和右奇异向量分别是矩阵 $XX^T$ 和 $X^T X$ 的特征向量。

- **低秩近似**：**选择前 $k$ 个最大的奇异值及其对应的左右奇异向量，数据投影**，构造一个低秩矩阵来近似原矩阵，实现降维和数据压缩。



#### 降维后的数据
设原始数据矩阵为 $X$，它经过 SVD 分解为：
$$
X = U \Sigma V^T
$$
在降维过程中，我们只保留前 $k$ 个奇异值和相应的奇异向量，得到一个低秩近似表示。这时，我们可以表示降维后的数据矩阵为：
$$
Y = X W
$$
其中：
- $Y$ 是降维后的数据矩阵，大小为 $n \times k$。
- $W$ 是投影矩阵，**由 $V$ 的前 $k$ 列组成**，大小为 $p \times k$。

在这个表达式中，$W = V_k$，表示将原始数据 $X$ 投影到由前 $k$ 个右奇异向量构成的空间中。因此，$Y = X W$ 等价于 $Y = U_k \Sigma_k$，实现了降维，同时保留了数据的主要信息。

#### 各矩阵含义
> 选择 $V$ 的前 $k$ 列来构造降维的投影矩阵 $W$，即 $W = V_k$。原因如下：
> 
> - $U$：描述**样本的投影坐标**，表明在主成分方向上每个样本的投影。
> - $\Sigma$：表示**奇异值**，反映不同方向的**重要性**和数据的伸缩量。
> - $V$：描述**特征的主方向**，用于找到数据中最重要的特征方向。

#### 优点
- **高效处理稀疏矩阵，适合大规模矩阵，保留主要结构信息**：SVD 能高效处理稀疏矩阵，特别适合在推荐系统中对评分矩阵进行降维。 能够提取数据中的主要模式和结构，减少噪声和冗余。

#### 缺点
- **计算复杂度高**：对于大规模数据集，SVD 的分解过程计算复杂度较高。
- **对噪声敏感**：奇异值分解容易受数据中的噪声影响，尤其是在高噪声数据中。
- **局限性**：无法处理非线性关系。
#### 应用场景
- **推荐系统**：在推荐系统中，SVD 用于分解用户-物品评分矩阵，找到潜在的关联模式。
- **图像处理**：在图像压缩中，SVD 被用于提取图像中的主要特征，从而减少图像数据量。
- **文本处理**：在自然语言处理中的词语共现矩阵或词向量的降维中，SVD 用于提取主要的语义信息。
### 比较总结

| **方法**  | **原理**                                                       | **优点**                                               | **缺点**                                                   | **应用场景**                                            |
|-----------|----------------------------------------------------------------|--------------------------------------------------------|-------------------------------------------------------------|---------------------------------------------------------|
| **PCA**   | **无监督**降维方式，通过线性变换，**寻找方差最大的方向作为主成分**。                   | 最大化方差，提取主要特征，计算简单。                    | **只捕捉线性关系**，对离群点敏感，解释性差。                    | 数据降维、噪声消除、特征提取、图像压缩。                |
| **LDA**   | **有监督**降维方式，**最大化类间方差，最小化类间方差**与类内方差的比值，找到区分类别的投影方向。       | 结合降维与分类，区分性强，适合分类任务。                | 假设线性可分，**对非线性数据效果较差**，需要类别标签，**对类别分布敏感**。                              | 分类任务中的降维，人脸识别、文本分类等。                 |
| **SVD**   | **无监督**降维方式，将**矩阵分解为三个矩阵的乘积**，提取矩阵中的主要模式和信息。       | **适合处理稀疏矩阵**，保留主要结构信息，数学基础坚实。      | 计算复杂度高，对噪声敏感。                                  | 推荐系统、**图像处理**、文本处理中的降维。                  |

选择哪种方法取决于数据的特性和具体任务。如果是**无标签数据降维，可以使用 PCA 或 SVD**；如果是**有标签的分类问题，可以使用 LDA**。

- PCA：主要用于无监督的降维，目标是最大化数据的方差，保留数据的主要信息。适用于数据的初步分析和可视化。
- LDA：主要用于有监督的降维，目标是最大化类别分离度。适用于分类问题中的特征提取和降维。
- SVD：广泛应用于各种数据处理任务，特别是矩阵分解和数据压缩。适用于降维、矩阵填充和数据压缩等任务。它在数学上等价于 PCA，但没有特别针对数据方差或类别分离度的优化目标。

---
# 非线性降维方法
## 总结：不同核方法的核函数选择策略
核函数参考：[万字长文解读机器学习——感知机、MLP、SVM](https://lichuachua.blog.csdn.net/article/details/142449632)

| **方法**           | **常用核函数**                        | **适用场景**                                                                                                                                                                                                 |
|--------------------|---------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **SVM**            | 线性核、RBF 核、多项式核、Sigmoid 核  | - 线性核：高维稀疏数据（如文本分类）。<br>- RBF 核：非线性分类问题，复杂模式识别（如图像分类）。<br>- 多项式核：低维数据，类别边界呈现多项式关系。<br>- Sigmoid 核：神经网络式问题。                       |
| **Kernel PCA**      | RBF 核、多项式核、线性核              | - RBF 核：非线性降维，保留复杂的非线性模式（如图像处理、模式识别）。<br>- 多项式核：数据间呈现多项式关系的降维任务。<br>- 线性核：线性降维问题，数据结构简单。                                             |
| **NDA**            | RBF 核、多项式核、线性核              | - RBF 核：非线性分类任务，复杂类别关系（如手写体识别）。<br>- 多项式核：类别间多项式关系的分类问题。<br>- 线性核：线性可分的分类任务。                                                                 |

- 对于**非线性数据**，**低维数据**，**高斯核/RBF 核** 是最通用且常用的选择，能够有效处理复杂模式的分类、降维等任务。
- 如果**低维数据**，数据**类别**间存在明显的**多项式模式**或关系，可以选择**多项式核**。
- 对于**线性数据**或高维稀疏数据，**线性核**计算简单高效，通常是最佳选择。
## 定义
非线性降维是指数据分布在高维空间中的某个非线性流形上，**无法通过简单的线性变换**，**将高维数据映射到低维空间**，从而需要**通过非线性变换将高维数据映射到低维空间**。这些方法适用于数据结构复杂、无法通过线性变换良好表示的情况。
## 特点
- **非线性变换**：使用非线性函数（如核函数、神经网络等）将数据映射到低维空间。
- **局部结构**：能够捕捉数据的局部非线性结构，也可以兼顾全局结构。
- **灵活性**：适用于具有复杂、非线性分布的数据，能够揭示数据的潜在低维流形结构。
- **计算复杂**：计算复杂度较高，结果的可解释性较差。
## 常见方法
### Kernel PCA（核主成分分析, Kernel Principal Component Analysis）
#### 原理
Kernel PCA 是主成分分析（PCA）的非线性扩展，**使用核函数将数据映射到高维特征空间**，在**高维空间中进行线性 PCA 操作**，捕捉数据中的**非线性结构**，**再将结果映射回原始空间**。
#### 步骤
1. **选择核函数：**首先选择一个合适的核函数$k(x_i,x_j)$，例如高斯核（RBF核）或多项式核。这个核函数隐式地将原始数据从低维空间映射到高维特征空间。
2. **构建核矩阵：**通过核函数计算数据点对之间的核矩阵$K$，即：
$K_{ij}=k(x_i,x_j)$
3. **中心化核矩阵：**为了与PCA一致，核矩阵需要进行零均值化处理。计算中心化后的核矩阵$\tilde{K}$。
4. **计算特征值和特征向量：**对核矩阵进行特征值分解，得到特征值和对应的特征向量。较大的特征值对应较重要的主成分方向。
5. **投影数据：**将原始数据投影到选定的特征向量方向上，实现降维。


> #### 优点
> - **处理非线性数据**：通过核函数，Kernel PCA 能够处理线性不可分的数据，捕捉复杂的非线性结构。
> - **灵活性强**：可以通过选择不同的核函数处理不同的数据分布和模式。
> #### 缺点
> - **结果难以解释**：由于在高维特征空间中进行操作，降维后的结果难以映射回原始空间进行解释。
> - **计算复杂度高**：核矩阵的计算复杂度较高，特别是对于大规模数据集。
> #### 应用场景
> - 图像处理、模式识别中的非线性降维。
> - 特征提取与数据可视化，特别适合具有复杂非线性结构的数据。

### NDA（Nonlinear Discriminant Analysis，非线性判别分析）
#### 原理
NDA 是**线性判别分析（LDA）**的核函数扩展，适用于**分类任务中的降维**。**通过核函数将数据非线性映射到高维特征空间中**。在高维空间中执行类似于 LDA 的操作，找到能够**最大化类别间差异**的方向。**显式地将结果映射回原始空间。**
#### 步骤

1. **选择核函数：** 选择核函数$k(x_i,x_j)$将数据从原始空间映射到高维空间。
2. **计算类内散布矩阵和类间散布矩阵：** 在高维特征空间中，类似于LDA，计算类内散布矩阵$S_w$和类间散布矩阵$S_b$，但这些矩阵是基于核函数的。
3. **最大化类间方差与类内方差的比值：** 通过求解特征值问题，找到使类间方差与类内方差最大化的特征向量。
4. **投影到降维空间：** 将数据投影到这些特征向量方向上，实现降维和分类。


> 
> #### 优点：
> - **适合非线性分类问题**：通过核函数的作用，NDA 能够处理线性不可分的分类任务。
> - **分类效果好**：NDA 结合了核函数和判别分析的优点，能够有效区分类别。
> #### 缺点
> - **依赖标签信息**：NDA 是有监督的降维方法，需要类别标签进行计算。
> - **计算复杂**：计算核矩阵和类内、类间散布矩阵的过程相对复杂，尤其在高维数据中。
> #### 应用场景
> - 在分类任务中用于降维和特征提取，如人脸识别、文本分类等。
- 适合需要同时进行降维和分类的应用场景。

###  T-SNE（t-Distributed Stochastic Neighbor Embedding，t 分布随机邻域嵌入）
#### 原理
T-SNE 是一种用于高维数据**非线性降维**和可视化的技术，特别适合处理高维数据中的局部结构。通过在**高维和低维空间中分别计算点对的相似度**，然后**最小化两个分布之间的差异，使相似数据点在低维空间中靠近**。

> T-SNE 的目标是将高维数据点嵌入到低维空间中，使得在**高维空间中彼此接近的数据点**在**低维空间中仍然保持接近**，而那些在高维空间中远离的数据点在低维空间中也保持远离。
#### 步骤

1. **高维空间中的相似性计算**：在高维空间中，通过**高斯分布**计算每对数据点之间的相似性，得到条件概率 $P_{ij}$ 表示点 $x_i$ 和 $x_j$ 之间的相似性。
2. **低维空间中的相似性计算**：在低维空间中，使用 **t 分布**计算数据点之间的相似性，得到 $Q_{ij}$，表示低维空间中的相似性。
3. **最小化 KL 散度**：通过最小化高维和低维空间中相似性概率分布之间的 KL 散度，确保在低维空间中保持高维数据的局部结构。


> 
> **1. 高维空间中的相似性计算：**
> 
> 在原始的高维空间中，t - SNE使用高斯分布来衡量数据点之间的相似性。具体来说，给定数据点$x_i$和$x_j$，t - SNE通过如下的条件概率来衡量数据点$x_i$和$x_j$的相似度： 
> $$p_{j|i}=\frac{\exp(- \| x_i - x_j
> \|^2/2\sigma_i^2)}{\sum_{k \neq i} \exp(- \| x_i - x_k
> \|^2/2\sigma_i^2)}$$
> 
> - $\sigma_i$是数据点$x_i$的局部参数，用来控制邻居点的范围。
> - 这个概率$p_{j|i}$表示在高维空间中，数据点$x_j$作为数据点$x_i$的邻居的概率。 
> 
>通过这样的概率分布，t - SNE建立了高维空间中局部邻域的结构。
> 
> **2. 低维空间中的相似性计算：**
> 
> 在目标的低维空间（如二维或三维），t - SNE使用t分布来计算数据点之间的相似性，定义如下的条件概率： 
> $$q_{i|j}=\frac{(1+ \| y_i - y_j \|^2)^{-1}}{\sum_{k \neq i}(1 + \| y_k - y_l \|^2)^{-1}}$$
> 
> - $y_i$和$y_j$是低维空间中的点。
> - t - SNE选择使用t分布（而不是高斯分布），是为了在低维空间中更好地处理远离数据点的距离，因为t分布的尾部较高斯分布更重，有助于避免远离的数据点在低维空间中坍缩到一起。
> 
> **3. 最小化高维和低维空间的差异**
>    - t - SNE通过最小化高维空间和低维空间的概率分布之间的差异来实现降维。这个差异通过**KL散度（Kullback - Leibler Divergence）** 来衡量：    
>    $$KL(P||Q) = \sum_{i}\sum_{j} p_{ij} \log \frac{p_{ij}}{q_{ij}} $$
>    - $P$是高维空间的相似性分布，$Q$是低维空间的相似性分布。
>    - KL散度度量两个分布之间的差异，t - SNE的目标是最小化这个差异，使得低维空间中的点能够保留高维空间中的邻居关系。
>    
> **4. 梯度下降优化**
>    - t - SNE通过梯度下降法来最小化KL散度，将高维数据点逐渐优化到低维空间中，以便尽可能保留局部的邻域结构。


> **如何找到降维后的低维空间？**
> - **初始化**：从高维空间的原始数据中开始，通过**随机初始化来得到初步**的低维坐标 \( y_i \)。
> - **高维与低维空间相似度计算**：在高维空间中使用高斯分布定义相似度，在低维空间中使用 t 分布定义相似度。
> - **优化**：通过最小化高维和低维空间中相似度分布的差异（KL 散度），使用梯度下降不断优化低维坐标。
> - **找到低维空间**：经过优化后，t-SNE 输出的低维嵌入 \( y_i \) 保留了高维数据的局部结构。



> #### 优点
> - **局部结构保留良好**：t-SNE 强调保持高维数据中的局部结构，非常适合数据的可视化。
> - **适合复杂数据集**：t-SNE 对于复杂的非线性数据表现良好，尤其适用于图像、文本等高维数据。

> #### 缺点
> - **全局结构可能丢失**：t-SNE 主要关注局部结构，可能忽略全局数据的分布。
> - **计算复杂度高**：t-SNE 的时间复杂度较高，尤其在大规模数据集上。
> - **结果不稳定**：t-SNE 对参数（如 perplexity）敏感，不同初始条件可能会得到不同的结果。

> #### 应用场景
> - 高维数据的可视化，如图像、基因表达数据、文本嵌入等。
> - 聚类结果的展示和分析，用于理解复杂数据集的内部结构。
#### Autoencoder（自编码器）
#### 原理
详细介绍参考：[万字长文解读深度学习——AE、VAE](https://lichuachua.blog.csdn.net/article/details/143067237)

自编码器是一种基于**神经网络**的非线性降维方法，通过**压缩和重构**数据的方式实现降维。自编码器由两个部分组成：**编码器**和**解码器**。编码器将数据压缩到低维空间，而解码器则试图从低维表示中重构原始数据。通过最小化重构误差，自编码器能够学习到数据的低维表示。

#### 结构
1. **编码器**：编码器部分是一个神经网络，输入数据经过编码器后被压缩到一个低维的隐空间（瓶颈层），这个隐空间表示就是数据的低维表示。

2. **解码器**：解码器是另一个神经网络，它从隐空间的低维表示中尝试重构出原始的高维数据。

3. **训练**：通过最小化输入数据与解码器输出数据之间的重构误差（如均方误差），自编码器可以学习到数据的低维表示。

> #### 优点
> - **灵活性强**：自编码器可以处理复杂的非线性数据，并且可以适应各种数据类型（如图像、文本等）。
> - **扩展性好**：自编码器可以通过增加网络层数和神经元数量来处理复杂的高维数据，具有很强的扩展性。

> #### 缺点
> - **需要大量数据训练**：自编码器需要大量训练数据，并且训练过程较慢。
> - **可能过拟合**：如果模型设计不当，自编码器可能会过拟合训练数据，导致泛化能力下降。
> - **解释性差**：自编码器学到的低维表示不易解释，其隐层的表示可能难以理解和直观解释。

> #### 应用场景
> - 数据降维和特征提取，特别是在图像数据中的特征表示。
> - 图像去噪、异常检测，通过重构误差发现异常样本。
> - 用于生成模型（如变分自编码器，VAE）和数据生成任务。

### 对比总结


| **方法**         | **原理**                                  | **目标**                               | **优点**                                                     | **缺点**                                                  | **应用场景**                                    |
|------------------|-------------------------------------------|----------------------------------------|--------------------------------------------------------------|------------------------------------------------------------|-------------------------------------------------|
| **Kernel PCA**    | **将数据映射到高维空间**，通过核函数进行PCA来捕捉非线性特征 | **核函数，非线性降维**                 | 处理非线性数据，灵活性强                                      | 结果难解释，计算复杂度高                                    | 图像处理、模式识别、特征提取                   |
| **NDA**           | **核函数与线性判别分析结合**，在高维空间中进行判别分析   | **核函数，分类任务中的非线性降维**     | 适合非线性分类，结合核函数和判别分析的优点                    | 依赖标签信息，计算复杂度高                                  | 人脸识别、文本分类、降维和分类结合任务         |
| **t-SNE**        | **通过最小化KL散度**来保持高维空间中的局部相似性关系      | **数据可视化，非线性降维**             | **局部结构保留好**，适合复杂数据的可视化                          | 全局结构丢失，计算复杂度高，结果不稳定                      | 数据可视化，基因表达分析，图像和文本可视化     |
| **自编码器**     | **神经网络**通过压缩和解压数据进行自监督学习，学习数据的低维表示  | 非线性降维，**数据重构**                 | 灵活性强，适合处理多种数据类型，能够学习复杂的非线性关系      | 需要大量数据训练，可能过拟合，结果解释性差                  | 图像处理、去噪、异常检测，生成模型（VAE）      |


- **Kernel PCA** 和 **NDA** 通过核函数扩展经典的线性降维和分类方法，能够处理非线性数据。
- **t-SNE** 是一种专注于保持局部结构的降维方法，主要用于数据的可视化。
- **自编码器** 则通过神经网络学习数据的低维表示，能够处理复杂的非线性关系，适合各种类型的数据。

# 总结

- **线性降维方法**：适用于数据可以通过线性变换表示的情况，计算简单、可解释性强，但无法处理复杂的非线性结构。
- **非线性降维方法**：适用于数据结构复杂的情况，能够捕捉数据中的非线性关系，但计算复杂度高，结果难以解释。

选择合适的降维方法取决于数据的特点和应用场景。对于具有简单线性关系的数据，线性降维方法通常更有效；对于具有复杂非线性结构的数据，非线性降维方法能够更好地揭示数据的潜在结构。



# 历史文章
本系列其他相关笔记参考如下：
[机器学习笔记——损失函数、代价函数和KL散度](https://blog.csdn.net/haopinglianlian/article/details/143831958?)
[机器学习笔记——特征工程、正则化、强化学习](https://blog.csdn.net/haopinglianlian/article/details/143832118?)
[机器学习笔记——30种常见机器学习算法简要汇总](https://blog.csdn.net/haopinglianlian/article/details/143832321)
[机器学习笔记——感知机、多层感知机(MLP)、支持向量机(SVM)](https://blog.csdn.net/haopinglianlian/article/details/143832552)
[机器学习笔记——KNN（K-Nearest Neighbors，K 近邻算法）](https://blog.csdn.net/haopinglianlian/article/details/143832692)
[机器学习笔记——朴素贝叶斯算法](https://blog.csdn.net/haopinglianlian/article/details/143832781?)
[机器学习笔记——决策树](https://blog.csdn.net/haopinglianlian/article/details/143834363)
[机器学习笔记——集成学习、Bagging（随机森林）、Boosting（AdaBoost、GBDT、XGBoost、LightGBM）、Stacking](https://blog.csdn.net/haopinglianlian/article/details/143834494?)
[机器学习笔记——Boosting中常用算法（GBDT、XGBoost、LightGBM）迭代路径](https://blog.csdn.net/haopinglianlian/article/details/143834628)

