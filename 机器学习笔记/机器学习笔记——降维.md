> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本笔记介绍机器学习中常见的无监督学习方法——降维。

![在这里插入图片描述](https://github.com/GoodnoteX/Ai_Interview/blob/main/机器学习笔记/image/11.png)

---

> @[toc]
---
# 降维方法概述

**降维**目的是将**高维**数据`映射到`**低维**空间中，同时**尽量保留数据的主要信息**。降维可以**减少数据冗余**、**降低计算复杂度**、**减轻过拟合**风险，并帮助我们更好地理解和可视化数据。降维方法主要分为**线性降维**和**非线性降维**两类。

# 线性降维方法

## 定义
线性降维方法假设数据可以通过**线性变换从高维空间映射到低维空间**。这类方法适用于**数据具有线性结构/线性分布**的情况。

## 特点
- **线性变换**：通过线性变换（如矩阵乘法）来将高维数据映射到低维空间。
- **全局结构**：关注数据的全局结构，试图在降维过程中保留整体几何和统计特性。
- **计算简单**：计算复杂度低，结果具有良好的可解释性。
- **局限性**：对具有复杂非线性结构的数据表现不佳。

## 常见方法
### PCA（Principal Component Analysis，主成分分析）
#### 原理
PCA 是一种**无监督**的**线性降维方法**，目的是通过**寻找数据中方差最大的投影方向**，将数据从高维空间映射到一个低维空间。PCA 的基本思想是：**最大化数据在降维空间中的方差**，即使得降维后的数据尽可能地保留原始数据的信息。
#### 步骤
1. **数据中心化**：将数据矩阵的每一列减去其均值，使数据均值为零。
2. **计算协方差矩阵**：对于 $n \times p$ 的数据矩阵 $X$，计算协方差矩阵 $C = \frac{1}{n-1} X^T X$。
3. **特征值分解**：对协方差矩阵 $C$ 进行特征值分解，得到特征值和对应的特征向量。特征值表示主成分的方差，特征向量表示主成分的方向。
4. **选择主成分**：选择前 $k$ 个最大特征值对应的特征向量，作为新的低维空间的基向量。
5. **数据投影**：将原始数据投影到选定的主成分上，得到降维后的数据。
#### 降维后的数据
经过降维后：
$$
Y = X W
$$

- **$X$**：**原始数据矩阵**，包含 $n$ 个样本和 $p$ 个特征。
- **$W$**：包含**前 $k$ 个主要特征向量的矩阵**，这些特征向量对应于最大特征值，表示数据的主要方向。
- **$Y$**：**降维后的数据矩阵**，表示将原始数据从 $p$ 维空间投影到 $k$ 维空间后的结果。

通过将 $X$ 与 $W$ 相乘，得到的 $Y$ 可以保留数据的主要特征信息，同时减少维度，便于后续分析。

#### 优点
- **最大化方差**：PCA 能找到数据方差最大的方向，并将数据投影到这些方向上，尽可能保留数据的主要信息。
- **降维效果好，计算简单**：只涉及矩阵分解和简单的线性代数运算，计算效率高，同时保持了数据的主要结构。

#### 缺点
- **无法处理非线性关系**：PCA 假设数据是线性分布的，无法处理非线性关系的数据。
- **对离群点敏感**：PCA 使用的协方差矩阵会受到离群点的影响，因此对异常值敏感。
- **解释性差**：主成分是线性组合，不总是容易解释为原始特征的物理含义。

#### 应用场景
- **数据降维**：在高维数据中提取主要特征，例如图像压缩、降维后用于可视化。
- **噪声消除**：通过保留方差较大的主成分，消除方差较小的噪声成分。
- **特征提取**：在分类或回归问题中，通过 PCA 提取主要特征用于建模。


### LDA（Linear Discriminant Analysis，线性判别分析）
#### 原理
LDA 是一种**有监督**的**线性降维方法**，目的是通过一个线性变换，**寻找一个最佳的投影方向**，使得变换后的**数据集类内差异最小，类间差异最大**。
> 在对比学习（Contrastive Learning）中，模型尝试**最大化**同类样本的相似性，同时**最小化**不同类样本之间的相似性。
#### 步骤
- **计算类内散布矩阵 $S_W$**：**反映同一类数据的分散程度**，类内散布矩阵的定义为各类别协方差矩阵的加权和：




> 详细全文请移步公主号：Goodnote。  
参考：[欢迎来到好评笔记（Goodnote）！](https://mp.weixin.qq.com/s/lCcceUHTrM7wOjnxkfrFsQ)
