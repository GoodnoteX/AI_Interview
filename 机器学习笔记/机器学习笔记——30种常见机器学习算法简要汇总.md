> 大家好，这里是好评笔记，本文为试读，查看全文请移步公主号：Goodnote。本笔记介绍机器学习中常见的损失函数和代价函数，各函数的使用场景。
> 

![在这里插入图片描述](https://github.com/GoodnoteX/Ai_Interview/blob/main/机器学习笔记/image/3.png)

---
> @[toc]
---

## 监督学习算法（Supervised Learning）
### 回归算法（Regression Algorithms）
- 线性回归（Linear Regression）
- 岭回归（Ridge Regression）
- 套索回归（Lasso Regression）
- k近邻算法（K-Nearest Neighbors, KNN）
- 决策树（Decision Tree）
- 随机森林（Random Forest）
- 梯度提升树（Gradient Boosting, GB）
- XGBoost 和 LightGBM
### 分类算法（Classification Algorithms）
- 逻辑回归（Logistic Regression）
- k近邻算法（K-Nearest Neighbors, KNN）
- 支持向量机（Support Vector Machine, SVM）
- 朴素贝叶斯（Naive Bayes）
- 决策树（Decision Tree）
- 随机森林（Random Forest）
- 梯度提升树（Gradient Boosting, GB）
- XGBoost 和 LightGBM

**总结：**
| **算法**                | **解释**                                                                                              | **应用场景**                                  | **优点**                                                         | **缺点**                                                       |
|-------------------------|-------------------------------------------------------------------------------------------------------|-----------------------------------------------|------------------------------------------------------------------|----------------------------------------------------------------|
| **线性回归**             | 预测**连续数值型变量的算法**。它通过拟合一条直线，来表示自变量与因变量之间的线性关系，预测连续变量。                                                             | 房价预测、股票价格预测。                        | 简单易用，结果易解释，适合小规模数据。                                        | 只能处理线性关系，对异常值敏感。                                             |
| **岭回归**               | 在**损失函数中加入L2正则化**，防止过拟合。                                                                    | 多重共线性问题的数据集。                         | **防止过拟合，处理多重共线性问题**。                                              | 不能进行特征选择，所有特征系数减小。                                           |
| **套索回归**             | 在**损失函数中加入L1正则化**，自动进行特征选择。                                                                | 高维数据的特征选择和回归预测。                    | **自动特征选择，产生稀疏解**。                                                   | 特征高度相关时，模型不稳定。                                                 |
| **逻辑回归**             | 用于二分类任务的线性分类模型，是一种**分类算法**。通过**sigmoid函数**将线性组合转为概率，用于分类。                                                               | 垃圾邮件检测、疾病诊断等**二分类任务**。              | 简单高效，概率输出易理解，适合高维稀疏数据。                                      | 只能处理线性可分问题，对离群点敏感。                                             |
| **KNN**                  | 根据**样本的K个邻居**的类别决定待分类样本的类别。                                                              | 文本分类、图像识别等。                           | 实现简单、易于理解，**无需训练**，对异常值不敏感。                                           | **计算复杂度高**，容易受噪声和高维数据影响。                                           |
| **SVM**                  | 寻找**最大化类间间隔的超平面**进行分类。                                                                     | 图像分类、文本分类、生物信息学。                  | 在**高维空间表现良好**，泛化能力强，**可处理非线性问题**。                                 | 训练时间长，**对参数和核函数敏感**。                                                |
| **朴素贝叶斯**           | 基于贝叶斯定理，**假设特征间相互独立**，通过**先验**和**条件概率**进行**分类**。                                                  | 文本分类、垃圾邮件检测等。                      | 训练和预测速度快，对小规模数据表现良好。                                         | 特征独立性假设不常成立，类别分布不均衡时效果差。                                     |
| **决策树**               | 通过**递归分割特征空间**构建决策树模型**进行分类或回归**。                                                           | 客户分类、信用评分。                            | 直观易懂，能处理数值和类别型特征。                                              | **容易过拟合**，对数据变动敏感。                                                  |
| **随机森林**             | Bagging集成学习的一种，**对数据集进行有放回的随机采样**并且**随机选择特征**，通过**两个随机**，**组合多个决策树**提高预测性能（数据集随机和特征随机）。                                                                      | **分类和回归任务**，适合大数据场景。                  | **稳定性高**，能处理高维数据和噪声。                                              | 计算复杂度高，预测速度慢。              **用来解决过拟合（高方差（High Variance）），易欠拟合**                                      |
| **梯度提升树**           | 通过逐步**构建多个弱学习器，逐步降低误差**。                                                                  | 广泛应用于广告预测、信用评分等。                  | **捕捉复杂特征关系，精度高**。                                                   | 训练速度慢，**用来解决欠拟合（高偏差（High Bias）），易过拟合**。                                                       |
| **XGBoost/LightGBM**     | 梯度提升的优化版本，采用**高效的数据结构** 和  **算法优化策略** ，支持 **并行计算**，并能够处理大规模数据。                                                           | 大规模分类和回归任务，如广告预测、推荐系统等。     | 训练速度快，支持并行计算，防止过拟合。                                          | 参数多，调参复杂，解释性较差。                                                |



> 详细全文请移步公主号：Goodnote。  
参考：[欢迎来到好评笔记（Goodnote）！](https://mp.weixin.qq.com/s/lCcceUHTrM7wOjnxkfrFsQ)
