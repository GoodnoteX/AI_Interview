> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本笔记介绍机器学习中朴素贝叶斯算法。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/5a712bdb29104349ab33b20525e5b50e.jpeg#pic_center)

---

>@[toc]
---
## 贝叶斯定理（Bayes' Theorem）
贝叶斯定理**用于描述事件之间的条件概率关系，解决分类和间接解决回归问题**。它的
描述了事件 $A$ 在事件 $B$ 发生后的**条件概率**：

$$
P(A | B) = \frac{P(B | A) \cdot P(A)}{P(B)}
$$

在朴素贝叶斯分类中：
- $A$ 表示数据点属于某个类别（如“垃圾邮件”或“正常邮件”）。
- $B$ 表示数据点的特征（如邮件的词频）。
- P(A | B) ：表示在已知**特征 \( B \)** 的情况下，属于类别 \( A \) 的概率（**后验概率**）。
- P(B | A) ：表示在已知**类别 \( A \)** 的情况下，观察到特征 \( B \) 的概率（**条件概率**）。
- P(A) ：事件 A 发生的**先验概率**。
- P(B) ：事件 B 发生的**先验概率**。

贝叶斯定理的核心思想是**通过已知的先验概率和条件概率，计算某个事件的后验概率**。

> 如图像生成时候的CFG（Classifier-Free Guidance）就是使用贝叶斯实现的。
## 朴素贝叶斯分类器（Naive Bayes Classifier）

朴素贝叶斯分类器是基于贝叶斯定理的一种简单而有效的分类算法。它的**核心假设**是在给定目标变量的条件下，**所有特征之间是相互独立的**，即“**条件独立性假设**”。虽然这个假设在**现实中通常不成立**，**但在实际应用中表现得非常好**。

### 计算步骤

1. **计算先验概率**：计算每个类别的先验概率 $P(C_i)$，其中 $C_i$ 表示类别。

2. **计算条件概率/似然概率**：对于每个特征，计算在给定类别的条件下特征出现的概率 $P(x_j | C_i)$。

3. **应用贝叶斯定理**：计算给定样本属于每个类别的后验概率 $P(C_i | x)$，其中 $x$ 是特征向量。

4. **做出分类决策**：选择具有**最高后验概率**的类别作为**分类结果**。

数学表达式为：

$$
P(C_i | x_1, x_2, \dots, x_n) = \frac{P(C_i) \cdot P(x_1 | C_i) \cdot P(x_2 | C_i) \cdots P(x_n | C_i)}{P(x_1, x_2, \dots, x_n)}
$$

在实际应用中，由于分母 $P(x_1, x_2, \dots, x_n)$对所有类别是相同的，所以只需要比较分子部分：

$$
P(C_i) \cdot P(x_1 | C_i) \cdot P(x_2 | C_i) \cdots P(x_n | C_i)
$$

### 优势
1. **计算简单**：因为条件独立假设，计算复杂度低，速度快。
2. **数据需求少**：对小数据集也能表现良好。
3. **处理多类别问题**：适合处理多类别分类问题。

### 局限性
1. **条件独立性假设不现实**：在许多情况下，特征之间并不是独立的，假设不成立时分类器效果可能下降。
2. **对数据格式敏感**：在某些应用场景中，对特征的处理和分布的要求较高。

## 朴素贝叶斯的三种常见变体

根据数据的不同特性，朴素贝叶斯有三种常见的变体模型：高斯朴素贝叶斯、多项式朴素贝叶斯和伯努利朴素贝叶斯。它们分别适用于不同类型的数据和应用场景。

### 1. 高斯朴素贝叶斯（Gaussian Naive Bayes）

高斯朴素贝叶斯**连续特征数据，假设特征服从高斯分布（正态分布）**。如身高、体重。

**假设**：每个类别 $C_i$ 下的特征 $x_j$ 服从正态分布：
$$
P(x_j | C_i) = \frac{1}{\sqrt{2 \pi \sigma_{C_i}^2}} \exp \left( -\frac{(x_j - \mu_{C_i})^2}{2 \sigma_{C_i}^2} \right)
$$
其中，$\mu_{C_i}$ 和 $\sigma_{C_i}$ 分别是类别 $C_i$ 下特征 $x_j$ 的均值和标准差。


> - **适用场景**：
>   - 特征为连续值（如身高、体重等）。
>   - 特征值近似服从正态分布的场景。
>   - 不适合处理离散数据，如文本分类中的词频数据。
> - **示例应用**：
>   - 分类问题中，特征是连续变量的，如预测癌症的肿瘤大小。

### 2. 多项式朴素贝叶斯（Multinomial Naive Bayes）

多项式朴素贝叶斯适用于**离散型数据，假设特征（如词频）符合多项式分布**。如词频或 **TF-IDF** 值。

**假设**：每个类别 $C_i$ 下的特征 $x_j$ 服从多项式分布：
$$
P(x | C_i) = \frac{\left( \sum_{j=1}^d x_j \right)!}{x_1! x_2! \cdots x_d!} \prod_{j=1}^d P(x_j | C_i)^{x_j}
$$
其中，$d$ 是特征数量，$x_j$ 是特征 $j$ 的出现次数，$P(x_j | C_i)$ 是在类别 $C_i$ 下特征 $j$ 出现的概率。

>- **适用场景**：
>   - 特征值是非负整数（表示频数）。
>   - 文本分类，特征为词频或 **TF-IDF** 值。

>- **示例应用**：
>   - 垃圾邮件分类，根据邮件中不同词的出现频率进行分类。
>   - 文档主题分类。

### 3. 伯努利朴素贝叶斯（Bernoulli Naive Bayes）

伯努利朴素贝叶斯适用于**二元特征数据（如 0 和 1），假设特征服从伯努利分布。**，常用于特征值表示是否出现某个事件的场景。

假设：每个类别 $C_i$ 下的特征 $x_j$ 服从伯努利分布：
$$
P(x_j | C_i) = P(x_j = 1 | C_i)^{x_j} \cdot (1 - P(x_j = 1 | C_i))^{1 - x_j}
$$
其中，$x_j$ 为 0 或 1，表示特征 $j$ 是否在样本中出现。

> - **适用场景**：
>   - 特征为布尔值（0或1）表示的场景，如文本数据中的**词袋模型**（词是否出现）。
>   - 适用于稀疏数据，尤其是大量特征值为0的情况。
> - **示例应用**：
>   - 文本分类中，每个特征表示某个词是否出现在文档中（即只关心是否出现，不关心出现的次数）。
>   - 文本情感分析，特征表示是否出现某些情感词汇。

### 总结
- **贝叶斯定理** 提供了一种**计算条件概率**的方法。
- **朴素贝叶斯分类器** **假设特征之间相互独立**，尽管这一假设在实际中可能并不成立，但在很多应用中仍然表现良好。
- **高斯朴素贝叶斯**：适合**连续值特征**，**假设**特征**服从正态分布**。
- **多项式朴素贝叶斯**：适合**离散值特征**，**假设**特征**服从多项式分布**。特征表示频数，如词频数据。
- **伯努利朴素贝叶斯**：适合**布尔值特征**，**假设**特征**服从伯努利分布**。特征表示某事件是否发生，如词袋模型的文本分类。

选择合适的朴素贝叶斯模型有助于提高分类效果，应根据数据特征和应用场景进行选择。

## 零概率问题
没有平滑时，这个概率可以表示为：

$$
P(x_i | C) = \frac{\text{count}(x_i, C)}{\text{count}(C)}
$$

其中：
- $\text{count}(x_i, C)$ 表示类别 $C$ 下特征 $x_i$ 出现的次数。
- $\text{count}(C)$ 表示类别 $C$ 出现的总次数。

朴素贝叶斯中的零概率问题是指在计算后验概率时，如果**某个特征值在训练数据中没有出现**，则该**特征值的概率会被计算为0**。由于贝叶斯公式中包含了特征值的概率乘积，只要一个特征值的概率为0，那么**整体公式的结果也会为0，导致预测结果不准确**。
### 总结
- **拉普拉斯平滑**：一种简单的平滑方法，通过在每个事件的频数上**加1**来避免零概率问题。适合简单场景，但在数据量较大时可能过于平滑。
- **加权平滑**：**引入一个超参数控制特征的重要性或频率**分布，进行比例调整，适合在特征权重差异较大的情况下使用。
- **Dirichlet平滑**：一种基于Dirichlet分布的平滑方法，灵活度更高，通过给**每个特征引入超参数**对平滑程度进行调节，常用于复杂的文本模型、语言模型或多项式分布估计中。
### 拉普拉斯平滑（Laplace Smoothing）
拉普拉斯平滑（也称为加一平滑）是一种解决概率估计中零概率问题的简单方法。拉普拉斯平滑通过**在每个事件的频数上加一个小的正数(通常为1)** 来避免零概率的出现。

公式为：
$$P(x_i | C') = \frac{count(x_i, C) + 1}{count(C) + |V|} $$

其中：
- $\text{count}(x_i, C)$ 表示类别 $C$ 下特征 $x_i$ 出现的次数。
- $\text{count}(C)$ 表示类别 $C$ 出现的总次数。
- | $V$ |是特征空间的大小 (即可能出现的所有特征的数量）。
- 加上1是为了保证所有特征的概率不为零

拉普拉斯平滑适用于解决朴素贝叶斯分类器中的零概率问题，这可能导致对频率较高的事件也进行了不必要的平滑，使得估计结果过于平滑。
### 加权平滑（Weighted Smoothing）
可以**根据特征重要性**或频率分布**给予不同的权重**，从而在估计概率时更加准确。

公式为：
$$
P(x_i | C) = \frac{\text{count}(x_i, C) + \alpha}{\text{count}(C) + \alpha \cdot |V|}
$$
其中：
- $\text{count}(x_i, C)$ 表示类别 $C$ 下特征 $x_i$ 出现的次数。
- $\text{count}(C)$ 表示类别 $C$ 出现的总次数。
- | $V$ |是特征空间的大小 (即可能出现的所有特征的数量）。
- $\alpha$ 是**加权平滑的平滑参数**，用来控制平滑的强度。
  - 当 $\alpha = 1$ 时，公式退化为拉普拉斯平滑。
  - 如果 $\alpha > 1$，则加大对未见事件的平滑强度。
  - 如果 $\alpha < 1$，则对未见事件的平滑力度较小。

通过引入特征权重 α ，根据特征的重要性或频率分布进行比例调整。需要在平滑过程中考虑特征间差异的情况，调整 α 。


### 狄利克雷平滑（Dirichlet Smoothing）
Dirichlet平滑是一种更加灵活的平滑方法，它通过**引入超参数对每个特征的平滑程度进行调整**。相比拉普拉斯平滑，Dirichlet平滑能够根据数据特点选择不同的平滑强度。

公式为：
$$
P(x_i | C) = \frac{\text{count}(x_i, C) + \alpha_i}{\text{count}(C) + \sum_{i=1}^{|V|} \alpha_i}
$$
其中：
- $\text{count}(x_i, C)$ 表示类别 $C$ 下特征 $x_i$ 出现的次数。
- $\text{count}(C)$ 表示类别 $C$ 出现的总次数。
- | $V$ |是特征空间的大小 (即可能出现的所有特征的数量）。
- $\alpha_i$ 是每个特征 $x_i$ 的平滑参数，不同的特征可以有不同的平滑强度。
- 当 $\alpha_i$ 相等且为 1 时，Dirichlet 平滑退化为拉普拉斯平滑。

为每个类别分配不同的平滑参数，更加灵活。 计算较复杂，但在处理复杂的数据分布时更具优势。



# 历史文章
本系列其他相关笔记参考如下：
[机器学习笔记——损失函数、代价函数和KL散度](https://blog.csdn.net/haopinglianlian/article/details/143831958?)
[机器学习笔记——特征工程、正则化、强化学习](https://blog.csdn.net/haopinglianlian/article/details/143832118?)
[机器学习笔记——30种常见机器学习算法简要汇总](https://blog.csdn.net/haopinglianlian/article/details/143832321)
[机器学习笔记——感知机、多层感知机(MLP)、支持向量机(SVM)](https://blog.csdn.net/haopinglianlian/article/details/143832552)
[机器学习笔记——KNN（K-Nearest Neighbors，K 近邻算法）](https://blog.csdn.net/haopinglianlian/article/details/143832692)

